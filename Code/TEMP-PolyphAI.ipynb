{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e5d52bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb8741",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93594753",
   "metadata": {},
   "source": [
    "### Transfroming the data into more readable input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1202ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_song(song):\n",
    "    result = []\n",
    "    prev = {'note0': -1, 'note1': -1, 'note2': -1, 'note3': -1} # COME BACKKKK\n",
    "    for index, row in song.iterrows():\n",
    "        frame = []\n",
    "        for voice in ['note0', 'note1', 'note2', 'note3']:\n",
    "            pitch = row[voice]\n",
    "            previous_pitch = prev[voice]\n",
    "            \n",
    "            tied = 1 if pitch == previous_pitch else 0\n",
    "            frame.append((int(pitch), tied))\n",
    "            prev[voice] = pitch\n",
    "        result.append(frame)\n",
    "    tensor = torch.tensor(result, dtype=torch.int64)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "28b12a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Data/'\n",
    "test = []\n",
    "train = []\n",
    "validation = []\n",
    "for dirname in os.listdir(folder_path):\n",
    "    if dirname != '.DS_Store':\n",
    "        for filename in os.listdir(folder_path + dirname):\n",
    "            if filename != '.ipynb_checkpoints':\n",
    "                df = pd.read_csv(folder_path + dirname + '/' + filename)\n",
    "                song = encode_song(df)\n",
    "                if dirname == 'test':\n",
    "                    test.append(song)\n",
    "                if dirname == 'train':\n",
    "                    train.append(song)\n",
    "                if dirname == 'valid':\n",
    "                    validation.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b5ff2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=100, num_layers=2, dropout=0.3):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.input_proj = nn.Linear(6, hidden_dim)\n",
    "\n",
    "    def forward(self, melody, target=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = melody.size(0)\n",
    "        seq_length = melody.size(1)\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(melody)\n",
    "        decoder_input = torch.zeros(batch_size, 1, hidden.size(2))\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, seq_length, self.fc.out_features)\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            decoder_output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            \n",
    "            output = self.fc(decoder_output.squeeze(1))\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            use_teacher_forcing = True if torch.rand(1).item() < teacher_forcing_ratio else False\n",
    "            \n",
    "            if target is not None and use_teacher_forcing:   \n",
    "                flattened_target = target[:, t, :].view(batch_size, -1)\n",
    "                projected_target = self.input_proj(flattened_target).unsqueeze(1)\n",
    "                decoder_input = projected_target\n",
    "            else:\n",
    "                ecoder_input = output.unsqueeze(1)\n",
    "                \n",
    "        probabilities = F.softmax(output, dim=-1)\n",
    "        print(\"probabilities are: \", probabilities)\n",
    "        predicted_harmony = torch.argmax(probabilities, dim=-1)\n",
    "        print(\"predicted harmony: \", predicted_harmony)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3003dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for song_index, song in enumerate(train[:5]):\n",
    "        print(f\"Training on song {song_index + 1}\")\n",
    "        \n",
    "        melody = song[:, 0, :].unsqueeze(0).float()\n",
    "        print(\"Melody is: \", melody)\n",
    "        harmonies = song[:, 1:, :].permute(1, 0, 2).float()\n",
    "        print(\"Harmonies are: \", harmonies)\n",
    "\n",
    "        harmonies_for_loss = harmonies_to_class(harmonies[:, :, 0])\n",
    "        print(\"Harmonies for loss: \", harmonies_for_loss)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ratio = 0.9 - (0.9 - 0.1) * (epoch / num_epochs)\n",
    "            ratio = max(0.1, ratio)\n",
    "            \n",
    "            output = model(melody, target=harmonies, teacher_forcing_ratio=ratio)\n",
    "            print(\"Output of the model is: \", output)\n",
    "            \n",
    "            output = output.view(-1, 128)\n",
    "            harmonies_for_loss = harmonies_for_loss.view(-1) \n",
    "            \n",
    "            loss = criterion(output, harmonies_for_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Song {song_index + 1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_song in validation:\n",
    "                val_melody = val_song[:, 0, :].unsqueeze(0).float()\n",
    "                val_harmonies = val_song[:, 1:, :].permute(1, 0, 2).float() \n",
    "                val_harmonies_for_loss = harmonies_to_class(val_harmonies[:, :, 0])\n",
    "\n",
    "                val_output = model(val_melody)\n",
    "                val_output = val_output.view(-1, 128)\n",
    "                val_harmonies_for_loss = val_harmonies_for_loss.view(-1)\n",
    "                \n",
    "                val_loss = criterion(val_output, val_harmonies_for_loss)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / len(validation)\n",
    "        print(f\"Validation Loss after song {song_index + 1}: {average_val_loss}\")\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "df64d219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on song 1\n",
      "Melody is:  tensor([[[66.,  0.],\n",
      "         [66.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [66.,  0.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [66.,  0.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [73.,  0.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [71.,  0.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [71.,  0.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [73.,  0.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [71.,  0.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [66.,  0.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [73.,  0.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [71.,  0.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [73.,  0.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [71.,  0.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [69.,  1.],\n",
      "         [71.,  0.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [73.,  0.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [73.,  1.],\n",
      "         [71.,  0.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [71.,  1.],\n",
      "         [69.,  0.],\n",
      "         [69.,  1.],\n",
      "         [68.,  0.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [68.,  1.],\n",
      "         [66.,  0.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.],\n",
      "         [66.,  1.]]])\n",
      "Harmonies are:  tensor([[[61.,  0.],\n",
      "         [61.,  1.],\n",
      "         [61.,  1.],\n",
      "         ...,\n",
      "         [61.,  1.],\n",
      "         [61.,  1.],\n",
      "         [61.,  1.]],\n",
      "\n",
      "        [[57.,  0.],\n",
      "         [57.,  1.],\n",
      "         [59.,  0.],\n",
      "         ...,\n",
      "         [57.,  1.],\n",
      "         [57.,  1.],\n",
      "         [57.,  1.]],\n",
      "\n",
      "        [[54.,  0.],\n",
      "         [54.,  1.],\n",
      "         [54.,  1.],\n",
      "         ...,\n",
      "         [54.,  1.],\n",
      "         [54.,  1.],\n",
      "         [54.,  1.]]])\n",
      "Harmonies for loss:  tensor([[7747, 7747, 7747, 7747, 8382, 8382, 8382, 8382, 8255, 8255, 8255, 8255,\n",
      "         8382, 8382, 8382, 8382, 8382, 8382, 8382, 8382, 8382, 8382, 8382, 8382,\n",
      "         8255, 8255, 8255, 8255, 7747, 7747, 7747, 7747, 8382, 8382, 8382, 8382,\n",
      "         8382, 8382, 8255, 8255, 8382, 8382, 8382, 8382, 8255, 8255, 8255, 8255,\n",
      "         8255, 8255, 8255, 8255, 8382, 8382, 8382, 8382, 8382, 8382, 8382, 8382,\n",
      "         8382, 8382, 8636, 8763, 8636, 8636, 8636, 8636, 8636, 8636, 8382, 8382,\n",
      "         8255, 8255, 8382, 8382, 8636, 8636, 8636, 8636, 8636, 8636, 8382, 8382,\n",
      "         8382, 8382, 8382, 8382, 8255, 8255, 8255, 8255, 7747, 7747, 7747, 7747,\n",
      "         8382, 8382, 8382, 8382, 8382, 8382, 8255, 8255, 8382, 8382, 8382, 8382,\n",
      "         8255, 8255, 8255, 8255, 8382, 8382, 8128, 8128, 8001, 8001, 8128, 8128,\n",
      "         8382, 8382, 8001, 8001, 8128, 8128, 8128, 8128, 8636, 8636, 8255, 8255,\n",
      "         8382, 8382, 8382, 8382, 8763, 8763, 8382, 8382, 7874, 7874, 7874, 7874,\n",
      "         9017, 9017, 8636, 8636, 8763, 8763, 8509, 8509, 8382, 8382, 8382, 8382,\n",
      "         8382, 8382, 8255, 8255, 8382, 8382, 8382, 8382, 8382, 8382, 8255, 8001,\n",
      "         8255, 8255, 8255, 8255, 7747, 7747, 7747, 7747, 7747, 7747, 7747, 7747],\n",
      "        [7239, 7239, 7493, 7493, 7747, 7747, 7747, 7747, 7747, 7747, 7493, 7493,\n",
      "         7239, 7239, 7239, 7239, 7493, 7493, 7493, 7493, 7747, 7747, 7747, 7747,\n",
      "         7747, 7747, 7493, 7493, 7239, 7239, 7239, 7239, 7239, 7239, 7239, 7239,\n",
      "         7493, 7493, 7493, 7493, 7747, 7747, 7747, 7747, 7747, 7747, 7747, 7747,\n",
      "         7747, 7747, 7747, 7747, 7747, 7747, 7747, 7747, 7874, 7874, 7874, 7874,\n",
      "         7874, 7874, 8128, 8382, 8128, 8128, 7874, 7874, 7747, 7747, 7747, 7747,\n",
      "         7493, 7493, 7239, 7239, 7874, 7874, 7747, 7747, 7747, 7747, 7874, 7874,\n",
      "         7874, 7874, 7493, 7493, 7112, 7112, 7493, 7493, 7239, 7239, 7239, 7239,\n",
      "         7239, 7239, 7239, 7239, 7493, 7493, 7747, 7747, 8001, 8001, 8001, 8001,\n",
      "         7747, 7747, 7747, 7747, 7747, 7747, 7747, 7747, 6858, 6858, 6858, 6858,\n",
      "         7493, 7493, 7493, 7493, 7493, 7493, 7493, 7493, 7747, 7747, 7747, 7747,\n",
      "         7747, 7747, 7239, 7239, 7874, 7874, 7874, 7874, 7874, 7874, 7493, 7493,\n",
      "         8128, 8128, 8128, 8128, 8128, 8128, 7747, 7747, 8382, 8382, 8128, 8128,\n",
      "         7874, 7874, 7747, 7747, 8001, 8001, 8001, 8001, 7747, 7747, 7112, 7112,\n",
      "         7747, 7747, 7493, 7493, 7239, 7239, 7239, 7239, 7239, 7239, 7239, 7239],\n",
      "        [6858, 6858, 6858, 6858, 6858, 7112, 7239, 7493, 7747, 7747, 6223, 6223,\n",
      "         6350, 6350, 6350, 6350, 6350, 6350, 6350, 6350, 6223, 6223, 5969, 5969,\n",
      "         6223, 6223, 6223, 6223, 5334, 5334, 5334, 5334, 6858, 6858, 6858, 6858,\n",
      "         7112, 7112, 7112, 7112, 7239, 7239, 7493, 7493, 7747, 7747, 7747, 7747,\n",
      "         6223, 6223, 6223, 6223, 6858, 6858, 6604, 6604, 6350, 6350, 6223, 6223,\n",
      "         6350, 6350, 5969, 5969, 6604, 6604, 6604, 6604, 5715, 5715, 7239, 7239,\n",
      "         7112, 7112, 6858, 6858, 6858, 6858, 6731, 6731, 6858, 6858, 6350, 6350,\n",
      "         5969, 5969, 5588, 5588, 6223, 6223, 6223, 6223, 5334, 5334, 5334, 5334,\n",
      "         6858, 6858, 6604, 6604, 6350, 6350, 6223, 6223, 6096, 6096, 6096, 6096,\n",
      "         6223, 6223, 6223, 6223, 5715, 5715, 5715, 5715, 5969, 5969, 6223, 6223,\n",
      "         6477, 6477, 5969, 5969, 6604, 6604, 6604, 6604, 6731, 6731, 6223, 6223,\n",
      "         6858, 6858, 6858, 6858, 6858, 6858, 6350, 6350, 7112, 7112, 7112, 7112,\n",
      "         7112, 7112, 6604, 6604, 7239, 7239, 7239, 7239, 7366, 7366, 6858, 6858,\n",
      "         7493, 7493, 7493, 7493, 7620, 7620, 7112, 7112, 7747, 7747, 7747, 7747,\n",
      "         6223, 6223, 6223, 6223, 6858, 6858, 6858, 6858, 6858, 6858, 6858, 6858]])\n",
      "probabilities are:  tensor([[0.3683, 0.3289, 0.3029]], grad_fn=<SoftmaxBackward0>)\n",
      "predicted harmony:  tensor([0])\n",
      "Output of the model is:  tensor([[[ 2.0373e-02, -2.7077e-02, -5.3901e-02],\n",
      "         [ 3.0572e-03,  3.0925e-02, -7.3503e-02],\n",
      "         [ 1.1046e-02,  7.3425e-02, -1.0007e-01],\n",
      "         [ 1.4127e-02,  4.2390e-02, -9.3878e-02],\n",
      "         [-2.9610e-02, -6.5949e-03, -4.9490e-02],\n",
      "         [-2.2338e-02, -1.5781e-02, -7.0998e-02],\n",
      "         [-1.8266e-02, -3.2693e-02, -1.1921e-01],\n",
      "         [ 1.0731e-02, -5.4986e-02, -1.1315e-01],\n",
      "         [-3.3665e-03, -5.3093e-02, -1.2627e-01],\n",
      "         [-5.0806e-03, -4.0909e-02, -1.2793e-01],\n",
      "         [ 1.7146e-02, -7.1160e-02, -1.3691e-01],\n",
      "         [ 1.6579e-02, -5.3548e-02, -1.2617e-01],\n",
      "         [-1.8906e-02, -6.3671e-02, -1.5502e-01],\n",
      "         [ 3.2202e-02, -4.5803e-02, -1.2742e-01],\n",
      "         [ 1.9150e-02, -5.4590e-02, -1.2850e-01],\n",
      "         [ 8.1651e-03, -6.6607e-02, -1.3740e-01],\n",
      "         [ 5.4156e-03, -5.5331e-02, -1.3919e-01],\n",
      "         [ 1.2197e-02, -5.8404e-02, -1.4560e-01],\n",
      "         [-2.4399e-03, -8.8181e-02, -1.2388e-01],\n",
      "         [ 2.5330e-02, -5.8743e-02, -1.9938e-01],\n",
      "         [ 4.5816e-02, -4.3394e-02, -2.0667e-01],\n",
      "         [ 1.2245e-02, -5.5528e-02, -1.6944e-01],\n",
      "         [ 1.2488e-02, -6.9721e-02, -1.3224e-01],\n",
      "         [ 4.5186e-03, -7.8521e-02, -9.2383e-02],\n",
      "         [ 1.6556e-02, -4.8014e-02, -1.5042e-01],\n",
      "         [-9.7703e-05, -3.7015e-02, -1.6212e-01],\n",
      "         [-2.2349e-02, -5.7183e-02, -1.3148e-01],\n",
      "         [-3.8363e-02, -8.6856e-02, -8.9167e-02],\n",
      "         [-3.3284e-02, -8.8087e-02, -1.0767e-01],\n",
      "         [-1.7015e-02, -8.8875e-02, -1.3455e-01],\n",
      "         [-2.6450e-02, -6.9053e-02, -1.2140e-01],\n",
      "         [-1.5858e-02, -6.9159e-02, -1.0964e-01],\n",
      "         [ 1.6907e-02, -7.7895e-02, -1.1097e-01],\n",
      "         [-9.1308e-03, -8.2407e-02, -1.1427e-01],\n",
      "         [-1.1046e-02, -6.6783e-02, -1.1771e-01],\n",
      "         [-1.7201e-02, -6.8870e-02, -1.2329e-01],\n",
      "         [ 1.5077e-02, -6.9627e-02, -1.2323e-01],\n",
      "         [ 3.6697e-02, -8.9579e-02, -1.2264e-01],\n",
      "         [ 5.8241e-03, -8.4839e-02, -1.4944e-01],\n",
      "         [-9.5652e-03, -9.0292e-02, -1.5709e-01],\n",
      "         [-5.1258e-03, -4.7535e-02, -1.1905e-01],\n",
      "         [-4.6444e-04, -3.8608e-02, -1.2954e-01],\n",
      "         [ 8.1787e-03, -4.0557e-02, -1.4479e-01],\n",
      "         [-1.0693e-04, -1.2103e-01, -1.1637e-01],\n",
      "         [-1.2283e-02, -1.0529e-01, -8.7084e-02],\n",
      "         [ 2.8926e-03, -1.0324e-01, -1.3685e-01],\n",
      "         [ 2.3367e-02, -9.0234e-02, -1.2434e-01],\n",
      "         [ 4.6958e-02, -8.7459e-02, -1.2979e-01],\n",
      "         [ 5.3146e-02, -9.5600e-02, -1.0221e-01],\n",
      "         [ 1.8841e-02, -8.8611e-02, -8.8106e-02],\n",
      "         [ 1.1177e-03, -1.0849e-01, -1.2248e-01],\n",
      "         [-3.6730e-03, -1.1658e-01, -1.3582e-01],\n",
      "         [ 4.3877e-03, -1.1430e-01, -1.2697e-01],\n",
      "         [-1.8814e-02, -9.4591e-02, -1.3780e-01],\n",
      "         [-9.9902e-03, -1.4556e-01, -1.2013e-01],\n",
      "         [ 7.4869e-03, -9.0523e-02, -1.2554e-01],\n",
      "         [ 6.3028e-03, -8.7859e-02, -4.9057e-02],\n",
      "         [-9.5060e-03, -1.0356e-01, -1.0096e-01],\n",
      "         [-1.3605e-02, -1.1515e-01, -8.2120e-02],\n",
      "         [-5.2940e-02, -1.0533e-01, -7.5858e-02],\n",
      "         [-1.5985e-02, -1.0413e-01, -1.0527e-01],\n",
      "         [-3.9532e-02, -8.1814e-02, -1.1013e-01],\n",
      "         [-5.0639e-02, -9.9849e-02, -1.1880e-01],\n",
      "         [-4.2504e-02, -8.2609e-02, -1.0220e-01],\n",
      "         [-4.2851e-02, -6.2708e-02, -1.4504e-01],\n",
      "         [-1.6901e-02, -9.3946e-02, -1.3213e-01],\n",
      "         [-2.1172e-03, -1.0076e-01, -1.4006e-01],\n",
      "         [ 1.5177e-03, -1.1283e-01, -1.0840e-01],\n",
      "         [-1.5100e-03, -8.2642e-02, -1.6748e-01],\n",
      "         [-2.4000e-02, -9.6871e-02, -1.8708e-01],\n",
      "         [ 2.4624e-02, -7.1933e-02, -1.8937e-01],\n",
      "         [ 1.2134e-03, -6.4022e-02, -1.6631e-01],\n",
      "         [ 3.7955e-02, -2.8511e-02, -1.8090e-01],\n",
      "         [ 1.6009e-02, -4.0370e-02, -2.1124e-01],\n",
      "         [-1.6213e-03, -7.6351e-02, -1.5549e-01],\n",
      "         [ 3.9145e-02, -9.4649e-02, -1.4230e-01],\n",
      "         [ 2.1669e-02, -6.3470e-02, -7.3157e-02],\n",
      "         [-1.9099e-03, -1.1185e-01, -1.0177e-01],\n",
      "         [-1.5606e-02, -8.0388e-02, -1.0714e-01],\n",
      "         [ 8.8261e-03, -3.1730e-02, -1.0595e-01],\n",
      "         [-1.3225e-02, -1.2279e-02, -1.7524e-01],\n",
      "         [ 5.1230e-03, -5.6033e-02, -1.6484e-01],\n",
      "         [ 3.3324e-02, -1.8019e-02, -1.5035e-01],\n",
      "         [ 3.3787e-02, -7.8995e-02, -1.4285e-01],\n",
      "         [ 8.6107e-03, -8.3982e-02, -1.7176e-01],\n",
      "         [ 1.2649e-02, -1.0486e-01, -1.4713e-01],\n",
      "         [-4.9500e-03, -1.2282e-01, -1.2552e-01],\n",
      "         [-3.1836e-02, -1.0546e-01, -1.0622e-01],\n",
      "         [-3.4958e-02, -6.8520e-02, -8.6799e-02],\n",
      "         [-2.1263e-02, -9.7959e-02, -9.6280e-02],\n",
      "         [-7.5585e-03, -1.0302e-01, -1.1553e-01],\n",
      "         [-1.6217e-02, -1.0004e-01, -8.3053e-02],\n",
      "         [-4.7723e-02, -5.5313e-02, -1.0016e-01],\n",
      "         [-3.2036e-02, -9.8747e-02, -1.3140e-01],\n",
      "         [-3.4614e-02, -1.2998e-01, -1.3062e-01],\n",
      "         [-3.3077e-03, -1.0296e-01, -1.1044e-01],\n",
      "         [-2.5206e-03, -1.0408e-01, -1.0587e-01],\n",
      "         [-3.2778e-02, -1.0348e-01, -9.1583e-02],\n",
      "         [-1.9505e-02, -8.6435e-02, -8.5236e-02],\n",
      "         [-3.3610e-03, -1.1259e-01, -1.2245e-01],\n",
      "         [-1.8551e-02, -9.7144e-02, -1.4134e-01],\n",
      "         [-3.4347e-03, -8.7232e-02, -1.9220e-01],\n",
      "         [-1.6958e-03, -1.0382e-01, -1.3398e-01],\n",
      "         [ 1.4023e-02, -1.2295e-01, -9.8722e-02],\n",
      "         [-1.3285e-02, -1.4700e-01, -8.9437e-02],\n",
      "         [-7.8258e-03, -1.2037e-01, -9.2258e-02],\n",
      "         [-1.6204e-02, -9.3290e-02, -4.8227e-02],\n",
      "         [ 7.7190e-03, -1.2276e-01, -6.9195e-02],\n",
      "         [-2.1431e-02, -8.7196e-02, -8.9889e-02],\n",
      "         [-2.6466e-02, -7.3436e-02, -1.5137e-01],\n",
      "         [-2.5394e-02, -8.7398e-02, -1.1730e-01],\n",
      "         [-2.4429e-02, -9.6274e-02, -1.6585e-01],\n",
      "         [-9.6330e-03, -9.0996e-02, -1.4056e-01],\n",
      "         [ 1.4040e-03, -8.2623e-02, -9.1559e-02],\n",
      "         [ 1.8883e-02, -8.3250e-02, -1.6640e-01],\n",
      "         [ 1.4203e-02, -5.4757e-02, -1.6947e-01],\n",
      "         [ 2.3355e-02, -5.1518e-02, -1.9684e-01],\n",
      "         [-6.9370e-03, -5.3829e-02, -1.3577e-01],\n",
      "         [-6.2966e-03, -1.1776e-01, -1.1721e-01],\n",
      "         [-7.4573e-03, -1.3915e-01, -9.9179e-02],\n",
      "         [-1.5601e-02, -1.3778e-01, -1.1825e-01],\n",
      "         [-7.1378e-03, -1.1587e-01, -1.5876e-01],\n",
      "         [-7.0814e-03, -1.2425e-01, -1.4318e-01],\n",
      "         [-1.7758e-02, -8.4125e-02, -1.7615e-01],\n",
      "         [-3.6107e-02, -6.9860e-02, -1.5189e-01],\n",
      "         [ 3.4998e-02, -9.5913e-02, -1.5789e-01],\n",
      "         [ 7.7408e-03, -8.3838e-02, -1.4568e-01],\n",
      "         [ 1.7936e-02, -9.2031e-02, -1.3835e-01],\n",
      "         [-2.9540e-02, -6.2112e-02, -9.0002e-02],\n",
      "         [-9.1623e-03, -7.6504e-02, -1.2851e-01],\n",
      "         [ 1.2370e-02, -7.4604e-02, -1.1753e-01],\n",
      "         [ 1.1419e-02, -1.0159e-01, -1.1254e-01],\n",
      "         [-5.3611e-05, -1.3632e-01, -1.3201e-01],\n",
      "         [ 1.6954e-02, -1.3046e-01, -1.1964e-01],\n",
      "         [ 4.4750e-02, -8.3219e-02, -1.0452e-01],\n",
      "         [ 3.3270e-03, -1.1437e-01, -1.0563e-01],\n",
      "         [ 2.9994e-02, -1.0544e-01, -1.0981e-01],\n",
      "         [ 2.8450e-03, -6.8898e-02, -1.7187e-01],\n",
      "         [-6.5645e-03, -5.3679e-02, -1.3508e-01],\n",
      "         [-1.6294e-02, -7.4174e-02, -1.3742e-01],\n",
      "         [-5.9031e-03, -8.9142e-02, -1.5665e-01],\n",
      "         [-2.7574e-02, -7.0596e-02, -1.6697e-01],\n",
      "         [ 7.9961e-03, -3.0323e-02, -1.6753e-01],\n",
      "         [ 1.1435e-02, -3.3734e-02, -1.4201e-01],\n",
      "         [ 1.2236e-02, -2.8130e-02, -1.6760e-01],\n",
      "         [-7.3814e-03, -7.9581e-02, -1.3669e-01],\n",
      "         [ 2.2255e-02, -1.1253e-01, -1.5384e-01],\n",
      "         [ 6.9433e-03, -1.2255e-01, -9.5328e-02],\n",
      "         [-8.2810e-03, -1.1995e-01, -5.5455e-02],\n",
      "         [-6.5664e-03, -1.2293e-01, -8.8128e-02],\n",
      "         [-1.0432e-02, -1.0345e-01, -1.2609e-01],\n",
      "         [-1.1384e-02, -1.4463e-01, -1.3144e-01],\n",
      "         [-9.8567e-04, -1.1904e-01, -1.0727e-01],\n",
      "         [-3.9340e-02, -1.3404e-01, -7.5149e-02],\n",
      "         [ 6.0016e-03, -1.2728e-01, -8.5348e-02],\n",
      "         [-1.0882e-02, -8.8536e-02, -1.4320e-01],\n",
      "         [ 8.0913e-04, -8.3551e-02, -1.1535e-01],\n",
      "         [ 1.6814e-02, -6.9858e-02, -1.1247e-01],\n",
      "         [ 1.5643e-02, -9.4715e-02, -1.0120e-01],\n",
      "         [ 4.6878e-02, -9.5708e-02, -1.2557e-01],\n",
      "         [ 1.7062e-02, -7.8935e-02, -1.6155e-01],\n",
      "         [ 1.3177e-02, -7.0090e-02, -1.3915e-01],\n",
      "         [ 4.4593e-02, -8.1087e-02, -1.0890e-01],\n",
      "         [ 2.2645e-02, -1.0794e-01, -9.4884e-02],\n",
      "         [ 2.8117e-02, -8.7189e-02, -1.0267e-01],\n",
      "         [ 2.6076e-02, -8.0439e-02, -9.0246e-02],\n",
      "         [ 6.3412e-02, -7.1055e-02, -1.0151e-01],\n",
      "         [ 1.1031e-01, -6.1240e-02, -1.5003e-01],\n",
      "         [ 1.0928e-01, -5.2939e-02, -1.9721e-01],\n",
      "         [ 7.1219e-02, -3.9202e-02, -1.4397e-01],\n",
      "         [ 2.6935e-02, -3.0153e-02, -8.9622e-02],\n",
      "         [ 2.1310e-02, -8.7014e-02, -1.6325e-01],\n",
      "         [-1.7947e-02, -6.4303e-02, -1.5553e-01],\n",
      "         [ 2.6685e-02, -5.1488e-02, -1.5159e-01],\n",
      "         [ 1.7079e-02, -5.1062e-02, -1.0104e-01],\n",
      "         [ 1.2967e-02, -4.9549e-02, -1.0487e-01],\n",
      "         [-2.2079e-03, -6.3666e-02, -1.2669e-01],\n",
      "         [ 2.0503e-03, -5.0424e-02, -1.4100e-01],\n",
      "         [-1.9000e-03, -7.0567e-02, -1.2071e-01],\n",
      "         [ 3.6139e-02, -7.7077e-02, -1.5947e-01]]], grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 128]' is invalid for input of size 540",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m train_model(model, optimizer, criterion, num_epochs)\n",
      "Cell \u001b[0;32mIn[186], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m model(melody, target\u001b[38;5;241m=\u001b[39mharmonies, teacher_forcing_ratio\u001b[38;5;241m=\u001b[39mratio)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput of the model is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     25\u001b[0m harmonies_for_loss \u001b[38;5;241m=\u001b[39m harmonies_for_loss\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, harmonies_for_loss)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 128]' is invalid for input of size 540"
     ]
    }
   ],
   "source": [
    "def harmonies_to_class(harmonies):\n",
    "    harmonies_classes = torch.round(harmonies * 127).long()  \n",
    "    return harmonies_classes\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "\n",
    "model = Model(2, 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "train_model(model, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294723a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8c627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
