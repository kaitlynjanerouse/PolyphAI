{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d52bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from music21 import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb8741",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93594753",
   "metadata": {},
   "source": [
    "### Transfroming the data into more readable input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1202ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_song(song):\n",
    "    result = []\n",
    "    prev = {'note0': -1, 'note1': -1, 'note2': -1, 'note3': -1}\n",
    "    result.append('START')\n",
    "    \n",
    "    for index, row in song.iterrows():\n",
    "        for voice in ['note0', 'note1', 'note2', 'note3']:\n",
    "            pitch = row[voice]\n",
    "            previous_pitch = prev[voice]\n",
    "            \n",
    "            tied = 1 if pitch == previous_pitch else 0\n",
    "            result.append((pitch,tied))\n",
    "            prev[voice] = pitch\n",
    "        result.append('|||')\n",
    "    result.append('END')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b12a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['START', (66, 0), (61, 0), (57, 0), (54, 0), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', (68, 0), (61, 1), (59, 0), (54, 1), '|||', (68, 1), (61, 1), (59, 1), (54, 1), '|||', (69, 0), (66, 0), (61, 0), (54, 1), '|||', (69, 1), (66, 1), (61, 1), (56, 0), '|||', (69, 1), (66, 1), (61, 1), (57, 0), '|||', (69, 1), (66, 1), (61, 1), (59, 0), '|||', (68, 0), (65, 0), (61, 1), (61, 0), '|||', (68, 1), (65, 1), (61, 1), (61, 1), '|||', (68, 1), (65, 1), (59, 0), (49, 0), '|||', (68, 1), (65, 1), (59, 1), (49, 1), '|||', (66, 0), (66, 0), (57, 0), (50, 0), '|||', (66, 1), (66, 1), (57, 1), (50, 1), '|||', (66, 1), (66, 1), (57, 1), (50, 1), '|||', (66, 1), (66, 1), (57, 1), (50, 1), '|||', (66, 1), (66, 1), (59, 0), (50, 1), '|||', (66, 1), (66, 1), (59, 1), (50, 1), '|||', (68, 0), (66, 1), (59, 1), (50, 1), '|||', (68, 1), (66, 1), (59, 1), (50, 1), '|||', (69, 0), (66, 1), (61, 0), (49, 0), '|||', (69, 1), (66, 1), (61, 1), (49, 1), '|||', (69, 1), (66, 1), (61, 1), (47, 0), '|||', (69, 1), (66, 1), (61, 1), (47, 1), '|||', (68, 0), (65, 0), (61, 1), (49, 0), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (68, 1), (65, 1), (59, 0), (49, 1), '|||', (68, 1), (65, 1), (59, 1), (49, 1), '|||', (66, 0), (61, 0), (57, 0), (42, 0), '|||', (66, 1), (61, 1), (57, 1), (42, 1), '|||', (66, 1), (61, 1), (57, 1), (42, 1), '|||', (66, 1), (61, 1), (57, 1), (42, 1), '|||', (73, 0), (66, 0), (57, 1), (54, 0), '|||', (73, 1), (66, 1), (57, 1), (54, 1), '|||', (73, 1), (66, 1), (57, 1), (54, 1), '|||', (73, 1), (66, 1), (57, 1), (54, 1), '|||', (71, 0), (66, 1), (59, 0), (56, 0), '|||', (71, 1), (66, 1), (59, 1), (56, 1), '|||', (71, 1), (65, 0), (59, 1), (56, 1), '|||', (71, 1), (65, 1), (59, 1), (56, 1), '|||', (69, 0), (66, 0), (61, 0), (57, 0), '|||', (69, 1), (66, 1), (61, 1), (57, 1), '|||', (69, 1), (66, 1), (61, 1), (59, 0), '|||', (69, 1), (66, 1), (61, 1), (59, 1), '|||', (68, 0), (65, 0), (61, 1), (61, 0), '|||', (68, 1), (65, 1), (61, 1), (61, 1), '|||', (68, 1), (65, 1), (61, 1), (61, 1), '|||', (68, 1), (65, 1), (61, 1), (61, 1), '|||', (68, 1), (65, 1), (61, 1), (49, 0), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (69, 0), (66, 0), (61, 1), (54, 0), '|||', (69, 1), (66, 1), (61, 1), (54, 1), '|||', (69, 1), (66, 1), (61, 1), (52, 0), '|||', (69, 1), (66, 1), (61, 1), (52, 1), '|||', (69, 1), (66, 1), (62, 0), (50, 0), '|||', (69, 1), (66, 1), (62, 1), (50, 1), '|||', (69, 1), (66, 1), (62, 1), (49, 0), '|||', (69, 1), (66, 1), (62, 1), (49, 1), '|||', (71, 0), (66, 1), (62, 1), (50, 0), '|||', (71, 1), (66, 1), (62, 1), (50, 1), '|||', (71, 1), (68, 0), (64, 0), (47, 0), '|||', (71, 1), (69, 0), (66, 0), (47, 1), '|||', (71, 1), (68, 0), (64, 0), (52, 0), '|||', (71, 1), (68, 1), (64, 1), (52, 1), '|||', (71, 1), (68, 1), (62, 0), (52, 1), '|||', (71, 1), (68, 1), (62, 1), (52, 1), '|||', (73, 0), (68, 1), (61, 0), (45, 0), '|||', (73, 1), (68, 1), (61, 1), (45, 1), '|||', (73, 1), (66, 0), (61, 1), (57, 0), '|||', (73, 1), (66, 1), (61, 1), (57, 1), '|||', (73, 1), (65, 0), (59, 0), (56, 0), '|||', (73, 1), (65, 1), (59, 1), (56, 1), '|||', (73, 1), (66, 0), (57, 0), (54, 0), '|||', (73, 1), (66, 1), (57, 1), (54, 1), '|||', (71, 0), (68, 0), (62, 0), (54, 1), '|||', (71, 1), (68, 1), (62, 1), (54, 1), '|||', (71, 1), (68, 1), (61, 0), (53, 0), '|||', (71, 1), (68, 1), (61, 1), (53, 1), '|||', (69, 0), (68, 1), (61, 1), (54, 0), '|||', (69, 1), (68, 1), (61, 1), (54, 1), '|||', (69, 1), (66, 0), (62, 0), (50, 0), '|||', (69, 1), (66, 1), (62, 1), (50, 1), '|||', (68, 0), (66, 1), (62, 1), (47, 0), '|||', (68, 1), (66, 1), (62, 1), (47, 1), '|||', (68, 1), (66, 1), (59, 0), (44, 0), '|||', (68, 1), (66, 1), (59, 1), (44, 1), '|||', (68, 1), (65, 0), (56, 0), (49, 0), '|||', (68, 1), (65, 1), (56, 1), (49, 1), '|||', (68, 1), (65, 1), (59, 0), (49, 1), '|||', (68, 1), (65, 1), (59, 1), (49, 1), '|||', (66, 0), (61, 0), (57, 0), (42, 0), '|||', (66, 1), (61, 1), (57, 1), (42, 1), '|||', (66, 1), (61, 1), (57, 1), (42, 1), '|||', (66, 1), (61, 1), (57, 1), (42, 1), '|||', (73, 0), (66, 0), (57, 1), (54, 0), '|||', (73, 1), (66, 1), (57, 1), (54, 1), '|||', (73, 1), (66, 1), (57, 1), (52, 0), '|||', (73, 1), (66, 1), (57, 1), (52, 1), '|||', (71, 0), (66, 1), (59, 0), (50, 0), '|||', (71, 1), (66, 1), (59, 1), (50, 1), '|||', (71, 1), (65, 0), (61, 0), (49, 0), '|||', (71, 1), (65, 1), (61, 1), (49, 1), '|||', (69, 0), (66, 0), (63, 0), (48, 0), '|||', (69, 1), (66, 1), (63, 1), (48, 1), '|||', (68, 0), (66, 1), (63, 1), (48, 1), '|||', (68, 1), (66, 1), (63, 1), (48, 1), '|||', (68, 1), (65, 0), (61, 0), (49, 0), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (73, 0), (66, 0), (61, 1), (45, 0), '|||', (73, 1), (66, 1), (61, 1), (45, 1), '|||', (73, 1), (64, 0), (61, 1), (45, 1), '|||', (73, 1), (64, 1), (61, 1), (45, 1), '|||', (71, 0), (63, 0), (54, 0), (47, 0), '|||', (71, 1), (63, 1), (54, 1), (47, 1), '|||', (71, 1), (64, 0), (54, 1), (49, 0), '|||', (71, 1), (64, 1), (54, 1), (49, 1), '|||', (69, 0), (66, 0), (59, 0), (51, 0), '|||', (69, 1), (66, 1), (59, 1), (51, 1), '|||', (69, 1), (63, 0), (59, 1), (47, 0), '|||', (69, 1), (63, 1), (59, 1), (47, 1), '|||', (68, 0), (64, 0), (59, 1), (52, 0), '|||', (68, 1), (64, 1), (59, 1), (52, 1), '|||', (68, 1), (64, 1), (59, 1), (52, 1), '|||', (68, 1), (64, 1), (59, 1), (52, 1), '|||', (68, 1), (68, 0), (61, 0), (53, 0), '|||', (68, 1), (68, 1), (61, 1), (53, 1), '|||', (68, 1), (65, 0), (61, 1), (49, 0), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (69, 0), (66, 0), (61, 1), (54, 0), '|||', (69, 1), (66, 1), (61, 1), (54, 1), '|||', (69, 1), (66, 1), (57, 0), (54, 1), '|||', (69, 1), (66, 1), (57, 1), (54, 1), '|||', (69, 1), (69, 0), (62, 0), (54, 1), '|||', (69, 1), (69, 1), (62, 1), (54, 1), '|||', (69, 1), (66, 0), (62, 1), (50, 0), '|||', (69, 1), (66, 1), (62, 1), (50, 1), '|||', (71, 0), (62, 0), (62, 1), (56, 0), '|||', (71, 1), (62, 1), (62, 1), (56, 1), '|||', (71, 1), (62, 1), (59, 0), (56, 1), '|||', (71, 1), (62, 1), (59, 1), (56, 1), '|||', (71, 1), (71, 0), (64, 0), (56, 1), '|||', (71, 1), (71, 1), (64, 1), (56, 1), '|||', (71, 1), (68, 0), (64, 1), (52, 0), '|||', (71, 1), (68, 1), (64, 1), (52, 1), '|||', (73, 0), (69, 0), (64, 1), (57, 0), '|||', (73, 1), (69, 1), (64, 1), (57, 1), '|||', (73, 1), (67, 0), (61, 0), (57, 1), '|||', (73, 1), (67, 1), (61, 1), (57, 1), '|||', (73, 1), (66, 0), (66, 0), (58, 0), '|||', (73, 1), (66, 1), (66, 1), (58, 1), '|||', (73, 1), (66, 1), (64, 0), (54, 0), '|||', (73, 1), (66, 1), (64, 1), (54, 1), '|||', (71, 0), (66, 1), (62, 0), (59, 0), '|||', (71, 1), (66, 1), (62, 1), (59, 1), '|||', (71, 1), (65, 0), (61, 0), (59, 1), '|||', (71, 1), (65, 1), (61, 1), (59, 1), '|||', (69, 0), (66, 0), (63, 0), (60, 0), '|||', (69, 1), (66, 1), (63, 1), (60, 1), '|||', (68, 0), (66, 1), (63, 1), (56, 0), '|||', (68, 1), (66, 1), (63, 1), (56, 1), '|||', (68, 1), (66, 1), (61, 0), (61, 0), '|||', (68, 1), (66, 1), (61, 1), (61, 1), '|||', (68, 1), (65, 0), (56, 0), (61, 1), '|||', (68, 1), (63, 0), (56, 1), (61, 1), '|||', (68, 1), (65, 0), (61, 0), (49, 0), '|||', (68, 1), (65, 1), (61, 1), (49, 1), '|||', (68, 1), (65, 1), (59, 0), (49, 1), '|||', (68, 1), (65, 1), (59, 1), (49, 1), '|||', (66, 0), (61, 0), (57, 0), (54, 0), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', (66, 1), (61, 1), (57, 1), (54, 1), '|||', 'END']\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'Data/'\n",
    "test = []\n",
    "train = []\n",
    "validation = []\n",
    "for dirname in os.listdir(folder_path):\n",
    "    if dirname != '.DS_Store':\n",
    "        for filename in os.listdir(folder_path + dirname):\n",
    "            if filename != '.ipynb_checkpoints':\n",
    "                df = pd.read_csv(folder_path + dirname + '/' + filename)\n",
    "                song = encode_song(df)\n",
    "                if dirname == 'test':\n",
    "                    test.append(song)\n",
    "                if dirname == 'train':\n",
    "                    train.append(song)\n",
    "                if dirname == 'valid':\n",
    "                    validation.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ff2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=128, output_size=127, num_harmony_parts=3, num_layers=1, dropout_rate=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.fc_alto = nn.Linear(hidden_size, output_size)\n",
    "        self.fc_tenor = nn.Linear(hidden_size, output_size)\n",
    "        self.fc_bass = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        output_alto = self.fc_alto(lstm_out)\n",
    "        output_tenor = self.fc_tenor(lstm_out)\n",
    "        output_bass = self.fc_bass(lstm_out)\n",
    "        \n",
    "        output = torch.stack([output_alto, output_tenor, output_bass], dim=2)\n",
    "        return output\n",
    "    \n",
    "class HarmonyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(HarmonyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.alto_fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tenor_fc = nn.Linear(hidden_size, output_size)\n",
    "        self.bass_fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, melody, hidden_state=None):\n",
    "        lstm_out, hidden_state = self.lstm(melody, hidden_state)\n",
    "        alto = self.alto_fc(lstm_out)\n",
    "        tenor = self.tenor_fc(lstm_out)\n",
    "        bass = self.bass_fc(lstm_out)\n",
    "        return alto, tenor, bass, hidden_state\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim=64, hidden_size=128, output_size=127, num_harmony_parts=3, num_layers=1, dropout_rate=0.5):\n",
    "#         super(Model, self).__init__()\n",
    "        \n",
    "#         # Embedding layer for tokenized chords\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "#         # LSTM with input size set to embedding_dim\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "#         # Fully connected layers for each harmony part\n",
    "#         self.fc_alto = nn.Linear(hidden_size, output_size)\n",
    "#         self.fc_tenor = nn.Linear(hidden_size, output_size)\n",
    "#         self.fc_bass = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Convert token indices to embeddings\n",
    "#         x = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "#         # LSTM layer\n",
    "#         lstm_out, _ = self.lstm(x)  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "#         # Linear layers for each harmony part\n",
    "#         output_alto = self.fc_alto(lstm_out)\n",
    "#         output_tenor = self.fc_tenor(lstm_out)\n",
    "#         output_bass = self.fc_bass(lstm_out)\n",
    "        \n",
    "#         # Stack outputs for alto, tenor, and bass\n",
    "#         output = torch.stack([output_alto, output_tenor, output_bass], dim=2)  # Shape: (batch_size, sequence_length, 3, output_size)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_model(model, optimizer, criterion, num_epochs, teacher_forcing_ratio=0.7):\n",
    "    model.train()\n",
    "    for song_index, song in enumerate(train[:25]):\n",
    "        print(f\"Training on song {song_index + 1}\")\n",
    "        \n",
    "        melody = song[:, 0, :].unsqueeze(0).float()\n",
    "        harmonies = song[:, 1:, :].permute(1, 0, 2).float()\n",
    "        harmonies = harmonies[:, :, 0]\n",
    "        harmonies = harmonies.permute(1, 0).unsqueeze(0)\n",
    "        hidden = None\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(melody)\n",
    "            \n",
    "            loss = criterion(output.view(-1, 127), harmonies.reshape(-1).long())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Song {song_index + 1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "                print(f\"Targets    : {harmonies}\")\n",
    "                print(f\"Predictions: {output.argmax(dim=-1)}\")\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_song in validation:\n",
    "                val_melody = val_song[:, 0, :].unsqueeze(0).float()\n",
    "                val_harmonies = val_song[:, 1:, :].permute(1, 0, 2).float() \n",
    "                val_harmonies = val_harmonies[:, :, 0]\n",
    "                val_harmonies = val_harmonies.permute(1, 0).unsqueeze(0)\n",
    "\n",
    "                val_output = model(val_melody)\n",
    "                val_loss = criterion(val_output.view(-1, 127), val_harmonies.reshape(-1).long())\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / len(validation)\n",
    "        print(f\"Validation Loss after song {song_index + 1}: {average_val_loss}\")\n",
    "\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64d219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "\n",
    "model = Model(2, 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "train_model(model, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8c627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c737c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from music21 import stream, note\n",
    "\n",
    "def midi_to_note(part):\n",
    "    result = stream.Part()\n",
    "    count = 1\n",
    "    prev = round(part[0])\n",
    "    for i in range(1, len(part)):\n",
    "        curr = round(part[i])\n",
    "        if curr == prev:\n",
    "            count += 1\n",
    "        else:\n",
    "            result.append(note.Note(prev, quarterLength=count / 4))\n",
    "            count = 1\n",
    "        prev = curr\n",
    "    result.append(note.Note(prev, quarterLength=count / 4))\n",
    "    return result\n",
    "\n",
    "def output_to_sheet_music(melody, result):\n",
    "    melody = melody[:, :, 0].squeeze()\n",
    "    melody_part = midi_to_note(melody.numpy())\n",
    "    print(\"melody part is: \", melody)\n",
    "\n",
    "    result = torch.argmax(result, dim=-1)\n",
    "    result = result.squeeze(0)\n",
    "\n",
    "    alto_notes = result[:, 0].numpy()\n",
    "    tenor_notes = result[:, 1].numpy()\n",
    "    bass_notes = result[:, 2].numpy()\n",
    "\n",
    "    alto_part = midi_to_note(alto_notes)\n",
    "    print(\"alto part is: \", alto_notes)\n",
    "    tenor_part = midi_to_note(tenor_notes)\n",
    "    print(\"tenor part is: \", tenor_notes)\n",
    "    bass_part = midi_to_note(bass_notes)\n",
    "    print(\"bass part is: \", bass_notes)\n",
    "\n",
    "    score = stream.Score()\n",
    "    score.append(melody_part)\n",
    "    score.append(alto_part)\n",
    "    score.append(tenor_part)\n",
    "    score.append(bass_part)\n",
    "\n",
    "    score.show('midi')\n",
    "    score.write('musicxml', 'output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_song = test[2]\n",
    "\n",
    "melody = test_song[:, 0, :].unsqueeze(0).float()\n",
    "result = model(melody)\n",
    "output_to_sheet_music(melody, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294723a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model output size as 127 (for each note class)\n",
    "# Assuming model output shape: (batch_size, seq_len, 3, 127)\n",
    "output = torch.randn(5, 10, 3, 127)  # Random output for demonstration\n",
    "print(output)\n",
    "\n",
    "# Assume target is structured as note indices for alto, tenor, and bass\n",
    "# Shape of target: (batch_size, seq_len, 3) - with each entry as an index in [0, 126]\n",
    "target = torch.randint(0, 127, (5, 10, 3))  # Random target for demonstration\n",
    "print(target)\n",
    "\n",
    "# CrossEntropyLoss expects shape (N, C) for input and (N) for target, so reshape\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reshape output to (batch_size * seq_len * 3, 127) and target to (batch_size * seq_len * 3)\n",
    "loss = criterion(output.view(-1, 127), target.view(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17941e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609cfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee17a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e78a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37625629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7803aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da90c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8da2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
