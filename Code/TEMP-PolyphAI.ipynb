{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5d52bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from music21 import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb8741",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93594753",
   "metadata": {},
   "source": [
    "### Transfroming the data into more readable input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "1202ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_class(note):\n",
    "    return note % 12\n",
    "\n",
    "def find_matching_octave_note(df):\n",
    "    bass_line = df['note3'].values\n",
    "    last_bass_note = bass_line[-1]\n",
    "    return last_bass_note\n",
    "\n",
    "def explore_for_lowest_tonic(df, pitch_class):\n",
    "    bass_notes = df['note3'].values  \n",
    "    matching_notes = [note for note in bass_notes if get_pitch_class(note) == pitch_class]\n",
    "    if matching_notes:\n",
    "        return min(matching_notes) \n",
    "    else:\n",
    "        return bass_notes[0]\n",
    "\n",
    "def detect_tonic(df):\n",
    "    candidate_note = find_matching_octave_note(df)\n",
    "    pitch_class = get_pitch_class(candidate_note)\n",
    "    true_tonic_note = explore_for_lowest_tonic(df, pitch_class)\n",
    "    return true_tonic_note\n",
    "\n",
    "def is_major(df):\n",
    "    chord_notes = df.iloc[0, :4].values\n",
    "    unique_pitch_classes = set(note % 12 for note in chord_notes)\n",
    "    \n",
    "    for root in unique_pitch_classes:\n",
    "        intervals = sorted((note - root) % 12 for note in unique_pitch_classes if note != root)\n",
    "        if 4 in intervals and 7 in intervals:\n",
    "            return True\n",
    "        \n",
    "        if 3 in intervals and 7 in intervals:\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "def key_transposition(df):\n",
    "    tonic_note = detect_tonic(df)\n",
    "    transpose_val = 48 if is_major(df) else 45\n",
    "    transpose_val -= tonic_note \n",
    "    df = (df + transpose_val).clip(lower=0, upper=127)\n",
    "    return df\n",
    "\n",
    "def encode_song(song):\n",
    "    result = []\n",
    "    prev = {'note0': -1, 'note1': -1, 'note2': -1, 'note3': -1}\n",
    "    result.append('START')\n",
    "    \n",
    "    for index, row in song.iterrows():\n",
    "        for voice in ['note0', 'note1', 'note2', 'note3']:\n",
    "            pitch = row[voice]\n",
    "            previous_pitch = prev[voice]\n",
    "            \n",
    "            tied = 1 if pitch == previous_pitch else 0\n",
    "            result.append((pitch,tied))\n",
    "            prev[voice] = pitch\n",
    "        result.append('|||')\n",
    "    result.append('END')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "28b12a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Data/'\n",
    "test_set = []\n",
    "train_set = []\n",
    "validation_set = []\n",
    "for dirname in os.listdir(folder_path):\n",
    "    if dirname != '.DS_Store':\n",
    "        for filename in os.listdir(folder_path + dirname):\n",
    "            if filename != '.ipynb_checkpoints':\n",
    "                df = pd.read_csv(folder_path + dirname + '/' + filename)\n",
    "                song = encode_song(df)\n",
    "                if dirname == 'test':\n",
    "                    test_set.append(song)\n",
    "                if dirname == 'train':\n",
    "                    train_set.append(song)\n",
    "                if dirname == 'valid':\n",
    "                    validation_set.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "b5ff2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=256, vocab_size=259, num_layers=2, droput_rate=.3):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(droput_rate)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = 1\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, song, mask, hidden=None, teacher_forcing_ratio=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden_state()\n",
    "            \n",
    "        output = []\n",
    "        x = song[0]\n",
    "        \n",
    "        for i in range(len(song)):\n",
    "            x = self.embedding(torch.tensor(x, dtype=torch.long))\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "            curr, hidden = self.lstm(x, hidden)\n",
    "            curr = self.dropout(curr)\n",
    "            curr = self.fc(curr)\n",
    "            \n",
    "            if mask[i]:\n",
    "                clamped_output = torch.zeros_like(curr)\n",
    "                clamped_output[0, 0, song[i]] = 1.0\n",
    "                output.append(clamped_output)\n",
    "                curr = song[i]\n",
    "            else: \n",
    "                if teacher_forcing_ratio and random.random() < teacher_forcing_ratio:\n",
    "                    output.append(curr)\n",
    "                    curr = song[i]\n",
    "                else:\n",
    "                    output.append(curr)\n",
    "                    curr = curr.argmax(dim=2).item()\n",
    "            x = curr\n",
    "        output = torch.cat(output, dim=1).view(-1, self.fc.out_features)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "3003dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask_training(song):\n",
    "    result = []\n",
    "    result.append(True) # start\n",
    "    for i in range(1, len(song) - 1):\n",
    "        if i % 5 == 1 or i % 5 == 0:\n",
    "            result.append(True)\n",
    "        else:\n",
    "            result.append(False)\n",
    "    result.append(True) # end\n",
    "    return result\n",
    "\n",
    "def compute_mask_testing(song):\n",
    "    result = []\n",
    "    for i in range(len(song)):\n",
    "        if i % 5 == 1 and i != len(song) - 1:\n",
    "            result.append(True)\n",
    "        else:\n",
    "            result.append(False)\n",
    "    return result\n",
    "\n",
    "def embedding_dictionary():\n",
    "    token_to_index = {(\"START\"): 256, (\"END\"): 257, (\"|||\"): 258}\n",
    "    for note in range(128):\n",
    "        token_to_index[(note, 0)] = note\n",
    "        token_to_index[(note, 1)] = note + 128\n",
    "    return token_to_index\n",
    "\n",
    "def harmonies_to_zero(song):\n",
    "    result = []\n",
    "    for i in range(len(song)):\n",
    "        if i % 5 == 2 or i % 5 == 3 or i % 5 == 4:\n",
    "            result.append(-1)\n",
    "        else:\n",
    "            result.append(song[i])\n",
    "    return result\n",
    "        \n",
    "token_dictionary = embedding_dictionary()\n",
    "\n",
    "def train_model(model, optimizer, criterion, num_epochs):\n",
    "    for index, song in enumerate(train_set[:10]):\n",
    "        model.train()\n",
    "        melody_mask = compute_mask_training(song)\n",
    "        embeded_song = [token_dictionary[token] for token in song]\n",
    "        hidden = None\n",
    "        for epoch in range(num_epochs):\n",
    "            teacher_forcing_rate = max(0.5 * (1 - epoch / num_epochs), 0)\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(embeded_song, melody_mask, hidden, teacher_forcing_rate)\n",
    "            loss = criterion(output, torch.tensor(embeded_song))\n",
    "            loss.backward()\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "            optimizer.step()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Song {index + 1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            for val_song in validation_set:\n",
    "                val_embeded_song = [token_dictionary[token] for token in val_song]\n",
    "                val_melody_mask = compute_mask_testing(val_song)\n",
    "                val_input_song = harmonies_to_zero(val_embeded_song)\n",
    "                val_output, _ = model(val_input_song, val_melody_mask)\n",
    "                val_loss = criterion(val_output, torch.tensor(val_embeded_song))\n",
    "                total_val_loss += val_loss.item()\n",
    "            print(f'Validation loss: {total_val_loss / len(validation_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df64d219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 1, Epoch 10/100, Loss: 5.127753257751465\n",
      "Song 1, Epoch 20/100, Loss: 5.079412460327148\n",
      "Song 1, Epoch 30/100, Loss: 4.916849136352539\n",
      "Song 1, Epoch 40/100, Loss: 4.4575371742248535\n",
      "Song 1, Epoch 50/100, Loss: 4.132639408111572\n",
      "Song 1, Epoch 60/100, Loss: 3.9968533515930176\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "train_model(model, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9c7444bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from music21 import stream, note\n",
    "\n",
    "def midi_to_note(part):\n",
    "    result = stream.Part()\n",
    "    count = 1\n",
    "    prev = round(part[0])\n",
    "    for i in range(1, len(part)):\n",
    "        curr = round(part[i])\n",
    "        if curr == prev:\n",
    "            count += 1\n",
    "        else:\n",
    "            result.append(note.Note(prev, quarterLength=count / 4))\n",
    "            count = 1\n",
    "        prev = curr\n",
    "    result.append(note.Note(prev, quarterLength=count / 4))\n",
    "    return result\n",
    "\n",
    "def process_sequence(sequence, delimiter_token=\"|||\"):\n",
    "    index_to_token = {v: k for k, v in token_dictionary.items()}\n",
    "    original_sequence = [index_to_token[embedded_value] for embedded_value in sequence]\n",
    "    original_sequence = original_sequence[1:-1]\n",
    "    melody, alto, tenor, bass = [], [], [], []\n",
    "    for i in range(0, len(original_sequence), 5):\n",
    "        melody.append(original_sequence[i][0])\n",
    "        alto.append(original_sequence[i+1][0])\n",
    "        tenor.append(original_sequence[i+2][0])\n",
    "        bass.append(original_sequence[i+3][0])\n",
    "    \n",
    "    return melody, alto, tenor, bass\n",
    "\n",
    "def output_to_sheet_music(result):\n",
    "    result = torch.argmax(result, dim=-1)\n",
    "    result = result.squeeze(0)\n",
    "    melody_notes, alto_notes, tenor_notes, bass_notes = process_sequence(result.numpy())\n",
    "\n",
    "    melody_part = midi_to_note(melody_notes)\n",
    "    alto_part = midi_to_note(alto_notes)\n",
    "    tenor_part = midi_to_note(tenor_notes)\n",
    "    bass_part = midi_to_note(bass_notes)\n",
    "\n",
    "    score = stream.Score()\n",
    "    score.append(melody_part)\n",
    "    score.append(alto_part)\n",
    "    score.append(tenor_part)\n",
    "    score.append(bass_part)\n",
    "\n",
    "    score.show('midi')\n",
    "    score.write('musicxml', 'output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9aa3c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 52, 52, 45, 45, 45, 45, 45, 45, 61, 45, 45, 64, 64, 61, 61, 45, 64, 68, 68, 68, 68, 45, 64, 64, 61, 61, 61, 45, 52, 52, 52, 61, 45, 45, 59, 59, 45, 64, 61, 61, 61, 61, 61, 45, 45, 45, 45, 45, 57, 57, 57, 62, 59, 59, 59, 59, 61, 61, 61, 45, 45, 52, 52, 52, 64, 52, 45, 45, 59, 59, 59, 59, 59, 52, 45, 64, 64, 61, 61, 45, 64, 68, 68, 68, 68, 45, 64, 61, 61, 61, 61, 45, 52, 52, 52, 52, 59, 52, 45, 64, 64, 61, 61, 61, 61, 61, 61, 45, 45, 45, 45, 45, 57, 57, 57, 62, 59, 59, 59, 59, 61, 61, 61, 45, 45, 52, 52, 52, 64, 52, 45, 45, 64, 64, 64, 68, 68, 68, 68, 45, 45, 45, 56, 68, 68, 68, 64, 45, 64, 68, 68, 68, 45, 45, 45, 45, 45, 68, 68, 68, 68, 57, 57, 57, 66, 66, 66, 66, 66, 66, 68, 68, 52, 52, 52, 45, 61, 61, 61, 57, 68, 68, 68, 68, 64, 61, 61, 45, 45, 59, 59, 59, 59, 52, 45, 45, 45, 45, 45, 45, 45, 45, 59, 59, 59, 59, 59, 52, 45, 45, 45, 45, 45, 45, 52, 52, 68, 64, 45, 45, 45, 45, 52, 52, 52, 52, 52, 45, 61, 45, 45, 59, 59, 59, 59, 59, 45, 61, 61, 45, 64, 68, 68, 68, 68, 45, 64, 61, 61, 61, 61, 45, 52, 52, 52, 52, 59, 52, 45, 64, 64, 61, 61, 61, 61, 61, 61, 45, 45, 45, 45, 45, 57, 57, 57, 62, 59, 59, 52, 52, 52, 45, 45, 45, 45, 45, 45, 68, 68, 59, 59, 52, 45, 45, 45, 45, 45, 45, 45, 52, 59, 52, 64, 45, 45, 45, 45, 59, 59, 59, 59, 59, 61, 61, 61, 45, 45, 52, 52, 52, 52, 64, 52]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv41580\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv41580_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv41580\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,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\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv41580_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv41580_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_song = test_set[3]\n",
    "embedded_test = [token_dictionary[token] for token in test_song]\n",
    "test_mask = compute_mask_testing(test_song)\n",
    "input_embedded_test = harmonies_to_zero(embedded_test)\n",
    "\n",
    "output, _ = model(input_embedded_test, test_mask)\n",
    "output_to_sheet_music(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294723a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model output size as 127 (for each note class)\n",
    "# Assuming model output shape: (batch_size, seq_len, 3, 127)\n",
    "output = torch.randn(5, 10, 3, 127)  # Random output for demonstration\n",
    "print(output)\n",
    "\n",
    "# Assume target is structured as note indices for alto, tenor, and bass\n",
    "# Shape of target: (batch_size, seq_len, 3) - with each entry as an index in [0, 126]\n",
    "target = torch.randint(0, 127, (5, 10, 3))  # Random target for demonstration\n",
    "print(target)\n",
    "\n",
    "# CrossEntropyLoss expects shape (N, C) for input and (N) for target, so reshape\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reshape output to (batch_size * seq_len * 3, 127) and target to (batch_size * seq_len * 3)\n",
    "loss = criterion(output.view(-1, 127), target.view(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f292697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
