{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "7d361ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include necessary imports\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3c67e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "def detect_tonic(df):\n",
    "    bass_notes = df.iloc[3].values\n",
    "    unique, counts = np.unique(bass_notes, return_counts=True)\n",
    "    \n",
    "    most_frequent_note = unique[np.argmax(counts)]\n",
    "    return most_frequent_note\n",
    "\n",
    "def key_transposition(df):\n",
    "    tonic_note = detect_tonic(df)\n",
    "    \n",
    "    transpose_val = 48 - tonic_note\n",
    "    df += transpose_val\n",
    "    \n",
    "    df = df.clip(lower=0, upper=127)\n",
    "    return df\n",
    "\n",
    "# Min-Max normalization technique\n",
    "def normalize_df(df):\n",
    "    X_std = df / 127\n",
    "    return X_std\n",
    "\n",
    "folder_path = 'Data/'\n",
    "test = []\n",
    "train = []\n",
    "validation = []\n",
    "for dirname in os.listdir(folder_path):\n",
    "    if dirname != '.DS_Store':\n",
    "        for filename in os.listdir(folder_path + dirname):\n",
    "            if filename != '.ipynb_checkpoints':\n",
    "                df = pd.read_csv(folder_path + dirname + '/' + filename)\n",
    "                transposed_df = key_transposition(df.transpose())\n",
    "                normalized_df = normalize_df(transposed_df)\n",
    "                if dirname == 'test':\n",
    "                    test.append(normalized_df)\n",
    "                if dirname == 'train':\n",
    "                    train.append(normalized_df)\n",
    "                if dirname == 'valid':\n",
    "                    validation.append(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8606521",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "551e0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim=40, n_layers=2, dropout_rate=.3):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size * 128)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(next(self.parameters()).device))\n",
    "\n",
    "    def forward(self, x, target=None, hidden=None, teacher_forcing=.7):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "        lstm_input = x[:, 0:1, :]\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            lstm_output, hidden = self.lstm(lstm_input, hidden)\n",
    "            lstm_output_step = lstm_output[:, -1, :] \n",
    "            model_output_step = self.fc(lstm_output_step)\n",
    "            model_output_step = model_output_step.view(batch_size, 3, 128)\n",
    "\n",
    "            outputs.append(model_output_step.unsqueeze(1))\n",
    "            \n",
    "            if target is not None and random.random() < teacher_forcing:\n",
    "                next_harmony = target[:, t:t+1, :]\n",
    "            else:\n",
    "                next_harmony = model_output_step.argmax(dim=-1).unsqueeze(1)\n",
    "\n",
    "            next_harmony = next_harmony.view(batch_size, 1, 3)\n",
    "\n",
    "            if t + 1 < seq_len:\n",
    "                melody_at_next_step = x[:, t + 1:t + 2, :1]  \n",
    "                lstm_input = torch.cat((melody_at_next_step, next_harmony), dim=2) \n",
    "\n",
    "        output = torch.cat(outputs, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09e503",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "511051f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for song_index, song in enumerate(train[:10]):\n",
    "        print(f\"Training on song {song_index + 1}\")\n",
    "        \n",
    "        melody = torch.tensor(song.iloc[0].values.reshape(-1, 1), dtype=torch.float32).unsqueeze(0).reshape(1, song.shape[1], 1)\n",
    "        harmonies = torch.tensor(song.iloc[1:].values.T, dtype=torch.float32).unsqueeze(0)\n",
    "        harmonies_with_zero = torch.zeros(1, song.shape[1], 3)\n",
    "        melody_with_empty_harmonies = torch.cat((melody, harmonies_with_zero), dim = -1)\n",
    "        harmonies_for_loss = harmonies_to_class(harmonies)\n",
    "       \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(melody_with_empty_harmonies, harmonies)\n",
    "            output = output.reshape(-1, 128)\n",
    "            harmonies_for_loss = harmonies_for_loss.reshape(-1)\n",
    "            loss = criterion(output, harmonies_for_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Song {song_index + 1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"harmonies: \", harmonies)\n",
    "                    print(\"melody:\" , melody)\n",
    "                    print(\"output: \", output)\n",
    "                    \n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        # validation songs\n",
    "        with torch.no_grad():\n",
    "            for val_song in validation:\n",
    "                val_melody = torch.tensor(val_song.iloc[0].values.reshape(-1, 1), dtype=torch.float32).unsqueeze(0).reshape(1, val_song.shape[1], 1)\n",
    "                val_harmonies = torch.tensor(val_song.iloc[1:].values.T, dtype=torch.float32).unsqueeze(0)\n",
    "                val_harmonies = harmonies_to_class(val_harmonies)\n",
    "                val_harmonies_with_zero = torch.zeros(1, val_song.shape[1], 3)\n",
    "                val_melody_with_empty_harmonies = torch.cat((val_melody, val_harmonies_with_zero), dim=-1)\n",
    "                \n",
    "                val_output = model(val_melody_with_empty_harmonies)\n",
    "                val_output = val_output.reshape(-1, 128)\n",
    "                val_harmonies = val_harmonies.reshape(-1)\n",
    "                \n",
    "                val_loss = criterion(val_output, val_harmonies)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / len(validation)\n",
    "        print(f\"Validation Loss after song {song_index + 1}: {average_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "6c6e6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on song 1\n",
      "Song 1, Epoch 10/100, Loss: 4.768758296966553\n",
      "Song 1, Epoch 20/100, Loss: 4.4057745933532715\n",
      "Song 1, Epoch 30/100, Loss: 3.623469829559326\n",
      "Song 1, Epoch 40/100, Loss: 2.835541009902954\n",
      "Song 1, Epoch 50/100, Loss: 2.3465893268585205\n",
      "Song 1, Epoch 60/100, Loss: 2.1647231578826904\n",
      "Song 1, Epoch 70/100, Loss: 2.1007566452026367\n",
      "Song 1, Epoch 80/100, Loss: 2.075883388519287\n",
      "Song 1, Epoch 90/100, Loss: 2.0496203899383545\n",
      "Song 1, Epoch 100/100, Loss: 2.0485832691192627\n",
      "Validation Loss after song 1: 4.21611886146741\n",
      "Training on song 2\n",
      "Song 2, Epoch 10/100, Loss: 3.9056408405303955\n",
      "Song 2, Epoch 20/100, Loss: 3.2147433757781982\n",
      "Song 2, Epoch 30/100, Loss: 2.757871389389038\n",
      "Song 2, Epoch 40/100, Loss: 2.4775021076202393\n",
      "Song 2, Epoch 50/100, Loss: 2.287827253341675\n",
      "Song 2, Epoch 60/100, Loss: 2.1536097526550293\n",
      "Song 2, Epoch 70/100, Loss: 2.097165584564209\n",
      "Song 2, Epoch 80/100, Loss: 2.0674352645874023\n",
      "Song 2, Epoch 90/100, Loss: 2.051413059234619\n",
      "Song 2, Epoch 100/100, Loss: 2.041001081466675\n",
      "Validation Loss after song 2: 3.953938147960565\n",
      "Training on song 3\n",
      "Song 3, Epoch 10/100, Loss: 3.308140277862549\n",
      "Song 3, Epoch 20/100, Loss: 2.867309808731079\n",
      "Song 3, Epoch 30/100, Loss: 2.5839908123016357\n",
      "Song 3, Epoch 40/100, Loss: 2.4349968433380127\n",
      "Song 3, Epoch 50/100, Loss: 2.3574864864349365\n",
      "Song 3, Epoch 60/100, Loss: 2.3132946491241455\n",
      "Song 3, Epoch 70/100, Loss: 2.291372299194336\n",
      "Song 3, Epoch 80/100, Loss: 2.27767276763916\n",
      "Song 3, Epoch 90/100, Loss: 2.2738003730773926\n",
      "Song 3, Epoch 100/100, Loss: 2.2682275772094727\n",
      "Validation Loss after song 3: 3.564790713481414\n",
      "Training on song 4\n",
      "Song 4, Epoch 10/100, Loss: 2.4690258502960205\n",
      "Song 4, Epoch 20/100, Loss: 2.355973958969116\n",
      "Song 4, Epoch 30/100, Loss: 2.3100414276123047\n",
      "Song 4, Epoch 40/100, Loss: 2.2852649688720703\n",
      "Song 4, Epoch 50/100, Loss: 2.2721312046051025\n",
      "Song 4, Epoch 60/100, Loss: 2.265190601348877\n",
      "Song 4, Epoch 70/100, Loss: 2.2613613605499268\n",
      "Song 4, Epoch 80/100, Loss: 2.2578999996185303\n",
      "Song 4, Epoch 90/100, Loss: 2.2557458877563477\n",
      "Song 4, Epoch 100/100, Loss: 2.2535641193389893\n",
      "Validation Loss after song 4: 3.719353266251393\n",
      "Training on song 5\n",
      "Song 5, Epoch 10/100, Loss: 2.7205467224121094\n",
      "Song 5, Epoch 20/100, Loss: 2.2990753650665283\n",
      "Song 5, Epoch 30/100, Loss: 2.107243776321411\n",
      "Song 5, Epoch 40/100, Loss: 2.0149011611938477\n",
      "Song 5, Epoch 50/100, Loss: 1.9765698909759521\n",
      "Song 5, Epoch 60/100, Loss: 1.959797739982605\n",
      "Song 5, Epoch 70/100, Loss: 1.9480327367782593\n",
      "Song 5, Epoch 80/100, Loss: 1.9410383701324463\n",
      "Song 5, Epoch 90/100, Loss: 1.9359747171401978\n",
      "Song 5, Epoch 100/100, Loss: 1.9321904182434082\n",
      "Validation Loss after song 5: 3.609140756802681\n",
      "Training on song 6\n",
      "Song 6, Epoch 10/100, Loss: 3.0131001472473145\n",
      "Song 6, Epoch 20/100, Loss: 2.4412364959716797\n",
      "Song 6, Epoch 30/100, Loss: 2.1321330070495605\n",
      "Song 6, Epoch 40/100, Loss: 2.0741965770721436\n",
      "Song 6, Epoch 50/100, Loss: 2.056730270385742\n",
      "Song 6, Epoch 60/100, Loss: 2.0416817665100098\n",
      "Song 6, Epoch 70/100, Loss: 2.0330936908721924\n",
      "Song 6, Epoch 80/100, Loss: 2.027277708053589\n",
      "Song 6, Epoch 90/100, Loss: 2.023014783859253\n",
      "Song 6, Epoch 100/100, Loss: 2.019670248031616\n",
      "Validation Loss after song 6: 3.915644896336091\n",
      "Training on song 7\n",
      "Song 7, Epoch 10/100, Loss: 3.4721717834472656\n",
      "Song 7, Epoch 20/100, Loss: 2.7645747661590576\n",
      "Song 7, Epoch 30/100, Loss: 2.3936686515808105\n",
      "Song 7, Epoch 40/100, Loss: 2.2470955848693848\n",
      "Song 7, Epoch 50/100, Loss: 2.1976001262664795\n",
      "Song 7, Epoch 60/100, Loss: 2.1717963218688965\n",
      "Song 7, Epoch 70/100, Loss: 2.160055637359619\n",
      "Song 7, Epoch 80/100, Loss: 2.1524507999420166\n",
      "Song 7, Epoch 90/100, Loss: 2.1470565795898438\n",
      "Song 7, Epoch 100/100, Loss: 2.14292573928833\n",
      "Validation Loss after song 7: 3.6968659926683474\n",
      "Training on song 8\n",
      "Song 8, Epoch 10/100, Loss: 2.862293243408203\n",
      "Song 8, Epoch 20/100, Loss: 2.598830461502075\n",
      "Song 8, Epoch 30/100, Loss: 2.4312093257904053\n",
      "Song 8, Epoch 40/100, Loss: 2.355153799057007\n",
      "Song 8, Epoch 50/100, Loss: 2.3273589611053467\n",
      "Song 8, Epoch 60/100, Loss: 2.3148629665374756\n",
      "Song 8, Epoch 70/100, Loss: 2.3073880672454834\n",
      "Song 8, Epoch 80/100, Loss: 2.302077293395996\n",
      "Song 8, Epoch 90/100, Loss: 2.298438549041748\n",
      "Song 8, Epoch 100/100, Loss: 2.2957589626312256\n",
      "Validation Loss after song 8: 3.8666993349026413\n",
      "Training on song 9\n",
      "Song 9, Epoch 10/100, Loss: 2.5809149742126465\n",
      "Song 9, Epoch 20/100, Loss: 2.286638021469116\n",
      "Song 9, Epoch 30/100, Loss: 2.1260175704956055\n",
      "Song 9, Epoch 40/100, Loss: 2.0619537830352783\n",
      "Song 9, Epoch 50/100, Loss: 2.037992000579834\n",
      "Song 9, Epoch 60/100, Loss: 2.0258429050445557\n",
      "Song 9, Epoch 70/100, Loss: 2.019345283508301\n",
      "Song 9, Epoch 80/100, Loss: 2.014453887939453\n",
      "Song 9, Epoch 90/100, Loss: 2.0107908248901367\n",
      "Song 9, Epoch 100/100, Loss: 2.008187770843506\n",
      "Validation Loss after song 9: 3.707201144634149\n",
      "Training on song 10\n",
      "Song 10, Epoch 10/100, Loss: 2.1510541439056396\n",
      "Song 10, Epoch 20/100, Loss: 2.0427603721618652\n",
      "Song 10, Epoch 30/100, Loss: 1.9959535598754883\n",
      "Song 10, Epoch 40/100, Loss: 1.9684040546417236\n",
      "Song 10, Epoch 50/100, Loss: 1.9574705362319946\n",
      "Song 10, Epoch 60/100, Loss: 1.9515410661697388\n",
      "Song 10, Epoch 70/100, Loss: 1.9463560581207275\n",
      "Song 10, Epoch 80/100, Loss: 1.9426621198654175\n",
      "Song 10, Epoch 90/100, Loss: 1.9395745992660522\n",
      "Song 10, Epoch 100/100, Loss: 1.9372403621673584\n",
      "Validation Loss after song 10: 3.7797987583356027\n"
     ]
    }
   ],
   "source": [
    "def harmonies_to_class(harmonies):\n",
    "    harmonies_classes = torch.round(harmonies * 127).long()  \n",
    "    return harmonies_classes\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "\n",
    "model = Model(4, 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234bbe1-0257-4c91-a95f-263342853d92",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1d90b-1d91-4fcc-a156-6edcb721d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = [0.01, .001]\n",
    "n_layers= [1,2,3]\n",
    "hidden_dim = [20, 40, 50]\n",
    "epochs= [5000, 10000]\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for LR in learning:\n",
    "    for n_layer in n_layers:\n",
    "        for epoch in epochs:\n",
    "            for dims in hidden_dim:\n",
    "                print(f\"Training with LR={LR} and n_layers={n_layer} and epochs={epoch} and hidden_dims={dims}\")\n",
    "                model = Model(input_size=1, output_size=harmonies.shape[2], n_layers=n_layer, hidden_dim=dims)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "                train_model(model, melody, harmonies, optimizer, criterion, epoch)\n",
    "                with torch.no_grad():\n",
    "                    output = model(melody)\n",
    "                    loss = criterion(output, harmonies)\n",
    "                    print(f\"Final Loss: {loss.item()}\")        \n",
    "                # Keep track of the best model (with lowest loss)\n",
    "                if loss.item() < best_loss:\n",
    "                    best_loss = loss.item()\n",
    "                    best_params = {'learning': LR, 'n_layers': n_layer, 'epochs': epoch, 'hidden_dim': dims}\n",
    "print(\"BEST: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "9f9e5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_df(df):\n",
    "    X_scaled = df * 127\n",
    "    return X_scaled\n",
    "\n",
    "def midi_to_note_melody(part):\n",
    "    result = stream.Part()\n",
    "    count = 1\n",
    "    prev = round(part[0])\n",
    "    for i in range(1, len(part)):\n",
    "        pitch = part[i]\n",
    "        curr = round(pitch)\n",
    "        if curr == prev:\n",
    "            count += 1\n",
    "        else:\n",
    "            this_note = note.Note(prev, quarterLength=count/4)\n",
    "            result.append(this_note)\n",
    "            count = 1\n",
    "        prev = curr\n",
    "    this_note = note.Note(prev, quarterLength=count/4)\n",
    "    result.append(this_note)\n",
    "    return result\n",
    "\n",
    "def midi_to_note_harmonies(part):\n",
    "    probabilities = torch.softmax(torch.tensor(part), dim=-1).numpy()\n",
    "    print(probabilities)\n",
    "    result = stream.Part()\n",
    "    count = 1\n",
    "    predicted_notes = np.argmax(part, axis=1)\n",
    "    print(predicted_notes)\n",
    "    prev = predicted_notes[0]\n",
    "    for i in range(1, len(part)):\n",
    "        curr = predicted_notes[i]\n",
    "        if curr == prev:\n",
    "            count += 1\n",
    "        else:\n",
    "            this_note = note.Note(prev, quarterLength=count/4)\n",
    "            result.append(this_note)\n",
    "            count = 1\n",
    "        prev = curr\n",
    "    this_note = note.Note(prev, quarterLength=count/4)\n",
    "    result.append(this_note)\n",
    "    return result\n",
    "\n",
    "def output_to_sheet_music(melody, result):\n",
    "    result_numpy = result.squeeze(0).detach().numpy()\n",
    "    melody = inverse_df(melody).squeeze()\n",
    "    inversed = result_numpy.transpose(1, 0, 2) \n",
    "    \n",
    "    score = stream.Score()\n",
    "    melody_part = midi_to_note_melody(melody.numpy())\n",
    "    \n",
    "    alto_notes = inversed[0]\n",
    "    tenor_notes = inversed[1]\n",
    "    bass_notes = inversed[2]  \n",
    "    \n",
    "    alto_part = midi_to_note_harmonies(alto_notes)\n",
    "    tenor_part = midi_to_note_harmonies(tenor_notes)\n",
    "    bass_part = midi_to_note_harmonies(bass_notes)\n",
    "\n",
    "    score.append(melody_part)\n",
    "    score.append(alto_part)\n",
    "    score.append(tenor_part)\n",
    "    score.append(bass_part)\n",
    "    score.show('midi')\n",
    "    score.write('musicxml', 'output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "64ca5e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1552622e-03 1.1503743e-03 9.8342227e-04 ... 1.1710112e-03\n",
      "  1.3715901e-03 1.1881663e-03]\n",
      " [3.4769218e-05 2.6771555e-05 2.1490961e-05 ... 3.8753926e-05\n",
      "  4.4865825e-05 3.5381920e-05]\n",
      " [1.6395101e-05 1.1469007e-05 8.9925252e-06 ... 1.8158784e-05\n",
      "  2.0702908e-05 1.5664140e-05]\n",
      " ...\n",
      " [1.3433211e-05 9.0677722e-06 6.8864083e-06 ... 1.5123687e-05\n",
      "  1.6168968e-05 1.2281536e-05]\n",
      " [1.3432962e-05 9.0674830e-06 6.8862405e-06 ... 1.5123321e-05\n",
      "  1.6168606e-05 1.2281203e-05]\n",
      " [1.3432710e-05 9.0672102e-06 6.8860595e-06 ... 1.5122966e-05\n",
      "  1.6168273e-05 1.2280880e-05]]\n",
      "[64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n",
      " 64 64 64 64 64 64 64 64 64 64 64 64]\n",
      "[[1.17763819e-03 8.24519899e-04 1.02040870e-03 ... 7.69256498e-04\n",
      "  1.32683711e-03 1.21666305e-03]\n",
      " [3.16921250e-05 2.32521761e-05 2.84634716e-05 ... 1.65872752e-05\n",
      "  3.73033326e-05 3.22574924e-05]\n",
      " [1.42198105e-05 1.04841902e-05 1.28425954e-05 ... 7.01140470e-06\n",
      "  1.65612819e-05 1.45236454e-05]\n",
      " ...\n",
      " [1.16504070e-05 8.54516384e-06 1.03582242e-05 ... 5.61144634e-06\n",
      "  1.30191374e-05 1.17217451e-05]\n",
      " [1.16500532e-05 8.54494465e-06 1.03579187e-05 ... 5.61124943e-06\n",
      "  1.30187791e-05 1.17213986e-05]\n",
      " [1.16497031e-05 8.54472910e-06 1.03576267e-05 ... 5.61105344e-06\n",
      "  1.30184126e-05 1.17210584e-05]]\n",
      "[57 57 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      " 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      " 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      " 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      " 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      " 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      " 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55 55\n",
      " 55 55 55 55 55 55 55 55 55 55 55 55]\n",
      "[[8.66232789e-04 9.24096617e-04 9.36133903e-04 ... 8.23462789e-04\n",
      "  9.13130702e-04 8.07226112e-04]\n",
      " [2.43882896e-05 2.42586666e-05 2.59496883e-05 ... 1.67919206e-05\n",
      "  2.23612205e-05 1.79923682e-05]\n",
      " [1.11856325e-05 1.08275890e-05 1.16736119e-05 ... 6.97854693e-06\n",
      "  9.84848339e-06 7.87910903e-06]\n",
      " ...\n",
      " [9.13370059e-06 8.52277299e-06 9.10331619e-06 ... 5.35084928e-06\n",
      "  8.03506555e-06 6.34770095e-06]\n",
      " [9.13342592e-06 8.52255016e-06 9.10310428e-06 ... 5.35066329e-06\n",
      "  8.03486273e-06 6.34749222e-06]\n",
      " [9.13315853e-06 8.52233279e-06 9.10290692e-06 ... 5.35046593e-06\n",
      "  8.03465082e-06 6.34727621e-06]]\n",
      "[48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      " 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      " 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      " 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      " 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      " 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      " 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      " 48 48 48 48 48 48 48 48 48 48 48 48]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv46272\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv46272_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv46272\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,TVRoZAAAAAYAAQAFJ2BNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCM5g/y8ATVRyawAAAVwA/wMAAOAAQM5gkEFagewggEEAAJBAWqcwgEAAAJA+WqcwgD4AAJA8Ws5ggDwAAJBBWs5ggEEAAJBDWs5ggEMAAJBEWoGdQIBEAACQRlrOYIBGAACQRFrOYIBEAACQQ1qBnUCAQwAAkERazmCARAAAkEZazmCARgAAkEhapzCASAAAkEZapzCARgAAkERazmCARAAAkElagZ1AgEkAAJBIWqcwgEgAAJBGWqcwgEYAAJBIWs5ggEgAAJBGWoGdQIBGAACQRFqBnUCARAAAkENazmCAQwAAkEFazmCAQQAAkD9azmCAPwAAkDxapzCAPAAAkD1apzCAPQAAkD9agZ1AgD8AAJBBWs5ggEEAAJA/Ws5ggD8AAJA9WoGdQIA9AACQPFrOYIA8AACQSFrOYIBIAACQRlrOYIBGAACQRFrOYIBEAACQQ1qBnUCAQwAAkEFagrsAgEEAzmD/LwBNVHJrAAAAGAD/AwAA4ABAzmCQQFqb12CAQADOYP8vAE1UcmsAAAAhAP8DAADgAEDOYJA5WqcwgDkAAJA3WpuwMIA3AM5g/y8ATVRyawAAABgA/wMAAOAAQM5gkDBam9dggDAAzmD/LwA=\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv46272_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv46272_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_song = test[0]\n",
    "melody = test_song.iloc[0]\n",
    "\n",
    "melody = torch.tensor(test_song.iloc[0].values.reshape(-1,1), dtype=torch.float32).unsqueeze(0).reshape(1, test_song.shape[1], 1)\n",
    "harmonies = torch.tensor(test_song.iloc[1:].values.T, dtype=torch.float32).unsqueeze(0)\n",
    "harmonies_with_zero = torch.zeros(1, test_song.shape[1], 3)\n",
    "\n",
    "model_input = torch.cat((melody, harmonies_with_zero), dim = -1)\n",
    "result = model(model_input)\n",
    "output_to_sheet_music(melody, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c75011af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune (hyperparameters, move around test data (refer to notes), etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new data + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make any other changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheet music + audio (musicAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new models if time permits (follow steps 3 - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front end ** if time permits\n",
    "# - Interactive sheet music\n",
    "# - musescore front end??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121de9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real one and generated compare\n",
    "# train on all songs + test on a different song\n",
    "# measure the test loss not just the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "[60, 0, 0, 0] -> [60, 70, 70, 70] -> [61, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ba97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
