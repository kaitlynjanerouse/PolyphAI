{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d361ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include necessary imports\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c67e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# Transpose all valies to the same key\n",
    "def key_transposition(df):\n",
    "    transpose_val = 48 - df.iloc[3][0]\n",
    "    df += transpose_val\n",
    "    return df\n",
    "\n",
    "# Min-Max normalization technique\n",
    "def normalize_df(df):\n",
    "    X_std = (df - 1) / (88 - 1)\n",
    "    return X_std\n",
    "\n",
    "folder_path = 'Data/'\n",
    "test = []\n",
    "train = []\n",
    "validation = []\n",
    "for dirname in os.listdir(folder_path):\n",
    "    if dirname != '.DS_Store':\n",
    "        for filename in os.listdir(folder_path + dirname):\n",
    "            if filename != '.ipynb_checkpoints':\n",
    "                df = pd.read_csv(folder_path + dirname + '/' + filename)\n",
    "                transposed_df = key_transposition(df.transpose())\n",
    "                normalized_df = normalize_df(transposed_df)\n",
    "                if dirname == 'test':\n",
    "                    test.append(normalized_df)\n",
    "                if dirname == 'train':\n",
    "                    train.append(normalized_df)\n",
    "                if dirname == 'valid':\n",
    "                    validation.append(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8606521",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551e0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim=40, n_layers=2, dropout_rate=.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_output, (h,c) = self.lstm(x, hidden)\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        model_output = self.fc(lstm_output)\n",
    "        model_output = self.sigmoid(model_output)\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09e503",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511051f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for song_index, song in enumerate(train[:10]):\n",
    "        print(f\"Training on song {song_index + 1}\")\n",
    "        \n",
    "        melody = torch.tensor(song.iloc[0].values.reshape(-1, 1), dtype=torch.float32).unsqueeze(0).reshape(1, song.shape[1], 1)\n",
    "        harmonies = torch.tensor(song.iloc[1:].values.T, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(melody)\n",
    "            loss = criterion(output, harmonies)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Song {song_index + 1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"harmonies: \", harmonies)\n",
    "                    print(\"melody:\" , melody)\n",
    "                    print(\"output: \", output)\n",
    "                    \n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        # validation songs\n",
    "        with torch.no_grad():\n",
    "            for val_song in validation:\n",
    "                val_melody = torch.tensor(val_song.iloc[0].values.reshape(-1, 1), dtype=torch.float32).unsqueeze(0).reshape(1, val_song.shape[1], 1)\n",
    "                val_harmonies = torch.tensor(val_song.iloc[1:].values.T, dtype=torch.float32).unsqueeze(0)\n",
    "                \n",
    "                val_output = model(val_melody)\n",
    "                val_loss = criterion(val_output, val_harmonies)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / len(validation)\n",
    "        print(f\"Validation Loss after song {song_index + 1}: {average_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on song 1\n",
      "torch.Size([1, 180, 1])\n",
      "torch.Size([1, 180, 3])\n",
      "Song 1, Epoch 100/5000, Loss: 0.0014270960818976164\n",
      "Song 1, Epoch 200/5000, Loss: 0.0013960956130176783\n",
      "Song 1, Epoch 300/5000, Loss: 0.0014213745016604662\n",
      "Song 1, Epoch 400/5000, Loss: 0.0013838415034115314\n",
      "Song 1, Epoch 500/5000, Loss: 0.0013468981487676501\n",
      "Song 1, Epoch 600/5000, Loss: 0.001387500436976552\n",
      "Song 1, Epoch 700/5000, Loss: 0.0013233496574684978\n",
      "Song 1, Epoch 800/5000, Loss: 0.0013890728587284684\n",
      "Song 1, Epoch 900/5000, Loss: 0.0013641389086842537\n",
      "Song 1, Epoch 1000/5000, Loss: 0.0013794498518109322\n",
      "Song 1, Epoch 1100/5000, Loss: 0.001373840612359345\n",
      "Song 1, Epoch 1200/5000, Loss: 0.0013554851757362485\n",
      "Song 1, Epoch 1300/5000, Loss: 0.0013431255938485265\n",
      "Song 1, Epoch 1400/5000, Loss: 0.0013310563517734408\n",
      "Song 1, Epoch 1500/5000, Loss: 0.0012865348253399134\n",
      "Song 1, Epoch 1600/5000, Loss: 0.0013917596079409122\n",
      "Song 1, Epoch 1700/5000, Loss: 0.001375482534058392\n",
      "Song 1, Epoch 1800/5000, Loss: 0.0013895599404349923\n",
      "Song 1, Epoch 1900/5000, Loss: 0.0013771078083664179\n",
      "Song 1, Epoch 2000/5000, Loss: 0.0013445840450003743\n",
      "Song 1, Epoch 2100/5000, Loss: 0.001306034391745925\n",
      "Song 1, Epoch 2200/5000, Loss: 0.0013097366318106651\n",
      "Song 1, Epoch 2300/5000, Loss: 0.0013032491551712155\n",
      "Song 1, Epoch 2400/5000, Loss: 0.0012545866193249822\n",
      "Song 1, Epoch 2500/5000, Loss: 0.0011673825792968273\n",
      "Song 1, Epoch 2600/5000, Loss: 0.0012983059277758002\n",
      "Song 1, Epoch 2700/5000, Loss: 0.0011915357317775488\n",
      "Song 1, Epoch 2800/5000, Loss: 0.0008362782537005842\n",
      "Song 1, Epoch 2900/5000, Loss: 0.002042131032794714\n",
      "Song 1, Epoch 3000/5000, Loss: 0.0013141143135726452\n",
      "Song 1, Epoch 3100/5000, Loss: 0.0013014237629249692\n",
      "Song 1, Epoch 3200/5000, Loss: 0.0012765941210091114\n",
      "Song 1, Epoch 3300/5000, Loss: 0.0013278042897582054\n",
      "Song 1, Epoch 3400/5000, Loss: 0.001293916953727603\n",
      "Song 1, Epoch 3500/5000, Loss: 0.0012990348041057587\n",
      "Song 1, Epoch 3600/5000, Loss: 0.0013197825755923986\n",
      "Song 1, Epoch 3700/5000, Loss: 0.0012948558432981372\n",
      "Song 1, Epoch 3800/5000, Loss: 0.0012247771956026554\n",
      "Song 1, Epoch 3900/5000, Loss: 0.0012341754045337439\n",
      "Song 1, Epoch 4000/5000, Loss: 0.0012044276809319854\n",
      "Song 1, Epoch 4100/5000, Loss: 0.0011689652455970645\n",
      "Song 1, Epoch 4200/5000, Loss: 0.0011515041114762425\n",
      "Song 1, Epoch 4300/5000, Loss: 0.0011956999078392982\n",
      "Song 1, Epoch 4400/5000, Loss: 0.0012195382732897997\n",
      "Song 1, Epoch 4500/5000, Loss: 0.0010482085635885596\n",
      "Song 1, Epoch 4600/5000, Loss: 0.00109093205537647\n",
      "Song 1, Epoch 4700/5000, Loss: 0.0011858477955684066\n",
      "Song 1, Epoch 4800/5000, Loss: 0.0009255356853827834\n",
      "Song 1, Epoch 4900/5000, Loss: 0.0008954714285209775\n",
      "Song 1, Epoch 5000/5000, Loss: 0.0009751145262271166\n",
      "Validation Loss after song 1: 0.01665819603472184\n",
      "Training on song 2\n",
      "torch.Size([1, 264, 1])\n",
      "torch.Size([1, 264, 3])\n",
      "Song 2, Epoch 100/5000, Loss: 0.0007205659057945013\n",
      "Song 2, Epoch 200/5000, Loss: 0.0005679579335264862\n",
      "Song 2, Epoch 300/5000, Loss: 0.0005304260994307697\n",
      "Song 2, Epoch 400/5000, Loss: 0.0004919443745166063\n",
      "Song 2, Epoch 500/5000, Loss: 0.00045512907672673464\n",
      "Song 2, Epoch 600/5000, Loss: 0.00043089711107313633\n",
      "Song 2, Epoch 700/5000, Loss: 0.00037668264121748507\n",
      "Song 2, Epoch 800/5000, Loss: 0.00040638245991431177\n",
      "Song 2, Epoch 900/5000, Loss: 0.00036235235165804625\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "num_epochs = 5000\n",
    "\n",
    "model = Model(1, 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_model(model, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234bbe1-0257-4c91-a95f-263342853d92",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1d90b-1d91-4fcc-a156-6edcb721d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = [0.01, .001]\n",
    "n_layers= [1,2,3]\n",
    "hidden_dim = [20, 40, 50]\n",
    "epochs= [5000, 10000]\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for LR in learning:\n",
    "    for n_layer in n_layers:\n",
    "        for epoch in epochs:\n",
    "            for dims in hidden_dim:\n",
    "                print(f\"Training with LR={LR} and n_layers={n_layer} and epochs={epoch} and hidden_dims={dims}\")\n",
    "                model = Model(input_size=1, output_size=harmonies.shape[2], n_layers=n_layer, hidden_dim=dims)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "                train_model(model, melody, harmonies, optimizer, criterion, epoch)\n",
    "                with torch.no_grad():\n",
    "                    output = model(melody)\n",
    "                    loss = criterion(output, harmonies)\n",
    "                    print(f\"Final Loss: {loss.item()}\")        \n",
    "                # Keep track of the best model (with lowest loss)\n",
    "                if loss.item() < best_loss:\n",
    "                    best_loss = loss.item()\n",
    "                    best_params = {'learning': LR, 'n_layers': n_layer, 'epochs': epoch, 'hidden_dim': dims}\n",
    "print(\"BEST: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f9e5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_df(df):\n",
    "    X_scaled = df * (88 - 1) + 1\n",
    "    return X_scaled\n",
    "\n",
    "def midi_to_note(part):\n",
    "    result = stream.Part()\n",
    "    count = 1\n",
    "    prev = round(part[0])\n",
    "    for i in range(1, len(part)):\n",
    "        pitch = part[i]\n",
    "        curr = round(pitch)\n",
    "        if curr == prev:\n",
    "            count += 1\n",
    "        else:\n",
    "            this_note = note.Note(prev, quarterLength=count/4)\n",
    "            result.append(this_note)\n",
    "            count = 1\n",
    "        prev = curr\n",
    "    this_note = note.Note(prev, quarterLength=count/4)\n",
    "    result.append(this_note)\n",
    "    return result\n",
    "\n",
    "def output_to_sheet_music(melody, result):\n",
    "    result_numpy = result.detach().numpy()\n",
    "    melody = inverse_df(melody)\n",
    "    inversed = inverse_df(np.squeeze(result_numpy)).T\n",
    "    \n",
    "    score = stream.Score()\n",
    "    melody_part = midi_to_note(melody)\n",
    "    \n",
    "    alto_notes = inversed[0]\n",
    "    tenor_notes = inversed[1]\n",
    "    bass_notes = inversed[2]  \n",
    "    \n",
    "    alto_part = midi_to_note(alto_notes)\n",
    "    tenor_part = midi_to_note(tenor_notes)\n",
    "    bass_part = midi_to_note(bass_notes)\n",
    "\n",
    "    score.append(melody_part)\n",
    "    score.append(alto_part)\n",
    "    score.append(tenor_part)\n",
    "    score.append(bass_part)\n",
    "    score.show('midi')\n",
    "    score.write('musicxml', 'output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "64ca5e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv98497\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv98497_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv98497\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,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\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv98497_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv98497_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " test_song = test[0]\n",
    "melody = test_song.iloc[0]\n",
    "harmonies = torch.tensor(test_song.iloc[1:].values.T, dtype=torch.float32).unsqueeze(0)\n",
    "result = model(torch.tensor(test_song.iloc[0].values.reshape(-1,1), dtype=torch.float32).unsqueeze(0).reshape(1,test_song.shape[1],1))\n",
    "output_to_sheet_music(melody, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75011af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune (hyperparameters, move around test data (refer to notes), etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new data + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make any other changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheet music + audio (musicAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new models if time permits (follow steps 3 - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front end ** if time permits\n",
    "# - Interactive sheet music\n",
    "# - musescore front end??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121de9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real one and generated compare\n",
    "# train on all songs + test on a different song\n",
    "# measure the test loss not just the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb6748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ba97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
