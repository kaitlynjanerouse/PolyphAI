{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d361ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include necessary imports\n",
    "import os\n",
    "import torch \n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c67e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "folder_path = 'Data/'\n",
    "test = []\n",
    "train = []\n",
    "validation = []\n",
    "for dirname in os.listdir(folder_path):\n",
    "    if dirname != '.DS_Store':\n",
    "        for filename in os.listdir(folder_path + dirname):\n",
    "            df = pd.read_csv(folder_path + dirname + '/' + filename)\n",
    "            transposed_df = df.transpose()\n",
    "            if dirname == 'test':\n",
    "                test.append(transposed_df)\n",
    "            if dirname == 'train':\n",
    "                train.append(transposed_df)\n",
    "            if dirname == 'valid':\n",
    "                validation.append(transposed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8606521",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "551e0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim=50, n_layers=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_output, (h,c) = self.lstm(x, hidden)\n",
    "        model_output = self.fc(lstm_output)\n",
    "        return model_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09e503",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511051f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, melody, harmonies, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(melody)\n",
    "        loss = criterion(output, harmonies)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c6e6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  99 Loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0225, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.6786e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.5082e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.6739e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.9554e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.5603e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.3791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.5477e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.8596e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.3273e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "epoch = 5000\n",
    "index = 0\n",
    "#for song in train:\n",
    "#index = index + 1\n",
    "#print(\"training song \", index)\n",
    "song = train[0]\n",
    "scaler_melody = MinMaxScaler()\n",
    "scaler_harmonies = MinMaxScaler()\n",
    "melody = torch.tensor(scaler_melody.fit_transform(song.iloc[0].values.reshape(-1,1)), dtype=torch.float32).unsqueeze(0).reshape(1,song.shape[1],1)\n",
    "harmonies = torch.tensor(scaler_harmonies.fit_transform(song.iloc[1:].values.T), dtype=torch.float32).unsqueeze(0)\n",
    "model = Model(1, harmonies.shape[2])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "train_model(model, melody, harmonies, optimizer, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234bbe1-0257-4c91-a95f-263342853d92",
   "metadata": {},
   "source": [
    "# Hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1d90b-1d91-4fcc-a156-6edcb721d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = [0.01,0.001]\n",
    "n_layers= [1,2,3,4,5]\n",
    "epochs= [100,1000,5000]\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "\n",
    "for LR in learning:\n",
    "    for n_layer in n_layers:\n",
    "        for epoch in epochs:\n",
    "            print(f\"Training with LR={LR} and n_layers={n_layer} and epochs={epoch}\")\n",
    "            model = Model(input_size=1, output_size=harmonies.shape[2], n_layers=n_layer)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "            train_model(model, melody, harmonies, optimizer, criterion, epoch)\n",
    "            with torch.no_grad():\n",
    "                output = model(melody)\n",
    "                loss = criterion(output, harmonies)\n",
    "                print(f\"Final Loss: {loss.item()}\")        \n",
    "            # Keep track of the best model (with lowest loss)\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_params = {'learning': LR, 'n_layers': n_layer, 'epochs': epoch}\n",
    "print(\"BEST: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny = pd.DataFrame([[67,62,59,43], [68,62,59,43]]).transpose()\n",
    "# melody = torch.tensor(tiny.iloc[0], dtype=torch.float32).unsqueeze(0).reshape(1,2,1)\n",
    "# harmonies = torch.transpose(torch.tensor(tiny.iloc[1:].values, dtype=torch.float32),0,1).unsqueeze(0)\n",
    "# model = Model(1, harmonies.shape[2])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# train_model(model, melody, harmonies, optimizer, criterion, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f9e5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69.86145  71.8787   72.12315  72.13968  72.12614  72.12902  72.1289\n",
      "  72.128944 72.128944 72.128944 72.128944 72.128944 72.14373  72.14087\n",
      "  72.14098  72.140945 72.16598  72.161804 72.161995 72.16194  72.18089\n",
      "  72.178185 72.17832  72.178276 72.15929  72.16204  72.144745 72.14761\n",
      "  72.13953  72.14099  72.14093  72.14095  72.12612  72.12903  72.12892\n",
      "  72.12896  72.14374  72.140884 72.14099  72.14096  72.14096  72.14096\n",
      "  72.14096  72.14096  72.14096  72.14096  72.14096  72.14096  72.11255\n",
      "  72.11847  72.11192  72.11351  72.10158  72.10464  72.10453  72.104576\n",
      "  72.09363  72.096695 72.0966   72.09665  72.107544 72.104515 72.10462\n",
      "  72.104576 72.12269  72.11824  72.11839  72.11833  72.14655  72.140816\n",
      "  72.141045 72.14097  72.166016 72.16184  72.14487  72.14763  72.13954\n",
      "  72.141014 72.14095  72.14097  72.12614  72.12904  72.12894  72.12897\n",
      "  72.14376  72.1409   72.141014 72.140976 72.140976 72.140976 72.140976\n",
      "  72.140976 72.11256  72.118484 72.118256 72.11835  72.11835  72.11835\n",
      "  72.11835  72.11835  72.11835  72.11835  72.11835  72.11835  72.13183\n",
      "  72.12891  72.12902  72.128975 72.11545  72.11841  72.1183   72.11835\n",
      "  72.11835  72.11835  72.11835  72.11835  72.146576 72.14084  72.14106\n",
      "  72.14098  72.14098  72.14098  72.14098  72.14098  72.14098  72.14098\n",
      "  72.14098  72.14098  72.14098  72.14098  72.14099  72.14099  72.12615\n",
      "  72.12907  72.128944 72.12898  72.12898  72.12898  72.12898  72.12898\n",
      "  72.11546  72.11842  72.11831  72.118355 72.118355 72.118355 72.118355\n",
      "  72.118355 72.14658  72.14085  72.14108  72.14099  72.14101  72.14101\n",
      "  72.14101  72.14101  72.12615  72.129074 72.12895  72.129    72.129\n",
      "  72.129    72.129    72.129    72.11546  72.11843  72.131805 72.12893\n",
      "  72.14384  72.14092  72.149    72.147545 72.164734 72.161934 72.16206\n",
      "  72.16203  72.12195  72.1292   72.1289   72.129005 72.129005 72.129005\n",
      "  72.129005 72.129005 72.1438   72.14093  72.14901  72.14755  72.16474\n",
      "  72.16194  72.16207  72.16204  72.16204  72.16204  72.16204  72.16204\n",
      "  72.181015 72.17831  72.178444 72.178406 72.178406 72.178406 72.178406\n",
      "  72.178406 72.159386 72.162155 72.16202  72.162056 72.14487  72.1477\n",
      "  72.14757  72.14761  72.13964  72.14107  72.141014 72.14103  72.12618\n",
      "  72.12909  72.128975 72.12902  72.143814 72.14096  72.14107  72.14103\n",
      "  72.14104  72.14104  72.14104  72.14104 ]\n",
      " [69.41919  72.64081  72.97549  72.99881  73.01218  73.01361  73.0137\n",
      "  73.01371  73.01371  73.01372  73.01372  73.01372  73.00093  72.99927\n",
      "  72.99913  72.99911  72.98097  72.978424 72.97824  72.97821  72.96696\n",
      "  72.96527  72.965164 72.96514  72.976425 72.97809  72.99019  72.99187\n",
      "  72.99821  72.999054 72.99913  72.999146 73.01195  73.013596 73.01373\n",
      "  73.01375  73.000984 72.99931  72.999176 72.99916  72.99916  72.99916\n",
      "  72.99916  72.99917  72.99917  72.99917  72.999176 72.999176 73.025215\n",
      "  73.028465 73.03549  73.03635  73.05011  73.05174  73.051895 73.05192\n",
      "  73.06582  73.06742  73.06757  73.0676   73.05372  73.0521   73.05195\n",
      "  73.051926 73.03154  73.02907  73.02885  73.02881  73.002884 72.99952\n",
      "  72.99924  72.9992   72.981094 72.978546 72.990326 72.99194  72.998276\n",
      "  72.99913  72.99919  72.999214 73.012    73.01365  73.01379  73.01381\n",
      "  73.00105  72.99938  72.999245 72.99922  72.99922  72.99922  72.99923\n",
      "  72.99923  73.02525  73.0285   73.02879  73.02884  73.02884  73.02884\n",
      "  73.02884  73.02884  73.02884  73.02884  73.02885  73.02885  73.01566\n",
      "  73.01401  73.01386  73.01384  73.02705  73.02868  73.028824 73.02885\n",
      "  73.028854 73.028854 73.028854 73.028854 73.002945 72.99959  72.99931\n",
      "  72.99927  72.99927  72.99927  72.99927  72.99927  72.999275 72.999275\n",
      "  72.999275 72.99928  72.99928  72.99928  72.99928  72.99928  73.01206\n",
      "  73.0137   73.01384  73.01387  73.01387  73.01387  73.01387  73.01387\n",
      "  73.02707  73.02871  73.028854 73.02888  73.028885 73.028885 73.028885\n",
      "  73.028885 73.00299  72.99963  72.99936  72.99931  72.99931  72.99931\n",
      "  72.99931  72.99931  73.012085 73.01373  73.01387  73.01389  73.01389\n",
      "  73.0139   73.0139   73.0139   73.0271   73.02873  73.01571  73.01407\n",
      "  73.00119  72.9995   72.99318  72.992325 72.98037  72.97867  72.978546\n",
      "  72.97853  73.00946  73.01352  73.01386  73.013916 73.013916 73.01392\n",
      "  73.01392  73.01392  73.00119  72.99952  72.99321  72.992355 72.98041\n",
      "  72.978714 72.978584 72.97857  72.97857  72.97858  72.97858  72.97858\n",
      "  72.967415 72.96573  72.965614 72.9656   72.9656   72.96561  72.965614\n",
      "  72.965614 72.976814 72.97847  72.97859  72.978615 72.990524 72.99218\n",
      "  72.99231  72.99233  72.99851  72.999344 72.99941  72.99942  73.01218\n",
      "  73.013824 73.01396  73.01398  73.00127  72.999596 72.99946  72.99944\n",
      "  72.99944  72.99944  72.99944  72.99944 ]\n",
      " [46.174034 51.161095 52.459316 52.655853 52.58245  52.59601  52.595226\n",
      "  52.59546  52.59545  52.595455 52.59546  52.595455 52.70193  52.693127\n",
      "  52.694626 52.6945   52.855644 52.842274 52.844486 52.844303 52.952774\n",
      "  52.943718 52.94518  52.945057 52.83659  52.845715 52.73657  52.745712\n",
      "  52.690662 52.695255 52.69448  52.69455  52.588028 52.596886 52.59537\n",
      "  52.5955   52.70196  52.693157 52.694653 52.694527 52.69455  52.69455\n",
      "  52.69455  52.69455  52.69455  52.694553 52.694553 52.694553 52.48223\n",
      "  52.49989  52.44422  52.448814 52.34332  52.351997 52.350456 52.350582\n",
      "  52.246605 52.25512  52.25359  52.253716 52.35763  52.349148 52.35067\n",
      "  52.350544 52.507835 52.494946 52.497215 52.497025 52.70927  52.69179\n",
      "  52.69478  52.69453  52.855724 52.84234  52.736877 52.745705 52.690704\n",
      "  52.695293 52.694523 52.694588 52.588055 52.596916 52.595398 52.595528\n",
      "  52.702    52.69319  52.694687 52.694565 52.694588 52.694588 52.694588\n",
      "  52.694588 52.48225  52.499912 52.496857 52.497116 52.49707  52.497074\n",
      "  52.497074 52.497074 52.497074 52.497078 52.497078 52.497078 52.602837\n",
      "  52.594124 52.59563  52.595505 52.48974  52.498497 52.496975 52.497105\n",
      "  52.497078 52.49708  52.49708  52.49708  52.709312 52.69183  52.694817\n",
      "  52.69457  52.69461  52.694607 52.694607 52.69461  52.69461  52.69461\n",
      "  52.694614 52.694614 52.694614 52.694614 52.694614 52.69462  52.58809\n",
      "  52.596947 52.595436 52.595562 52.595543 52.595547 52.595547 52.595547\n",
      "  52.48975  52.498512 52.49699  52.49712  52.4971   52.4971   52.4971\n",
      "  52.4971   52.70934  52.691856 52.694843 52.69459  52.694637 52.694633\n",
      "  52.694637 52.694637 52.5881   52.596966 52.595448 52.595577 52.59556\n",
      "  52.59556  52.59556  52.59556  52.48976  52.498528 52.60277  52.594185\n",
      "  52.702175 52.693233 52.7483   52.74372  52.852154 52.843124 52.844612\n",
      "  52.844486 52.57664  52.599094 52.595295 52.59562  52.59557  52.595573\n",
      "  52.595573 52.595573 52.702087 52.69327  52.74831  52.743736 52.852177\n",
      "  52.843143 52.844635 52.84451  52.844532 52.844532 52.844532 52.844532\n",
      "  52.95304  52.94397  52.945435 52.94531  52.945335 52.945335 52.94534\n",
      "  52.94534  52.836784 52.845932 52.844448 52.844578 52.736835 52.74587\n",
      "  52.744373 52.744503 52.69092  52.695393 52.69464  52.69471  52.588146\n",
      "  52.59701  52.595497 52.595627 52.702126 52.69331  52.69481  52.694683\n",
      "  52.694706 52.694706 52.694706 52.694706]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv71947\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv71947_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv71947\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,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\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv71947_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv71947_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/foodrunner/CS370/PolyphAI/Code/output.xml')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melody = train[1].iloc[0]\n",
    "result = model(torch.tensor(melody, dtype=torch.float32).unsqueeze(0).reshape(1, train[1].shape[1], 1))\n",
    "result_numpy = result.detach().numpy()\n",
    "inversed = scaler_harmonies.inverse_transform(np.squeeze(result_numpy)).T\n",
    "print(inversed)\n",
    "\n",
    "score = stream.Score()\n",
    "melody_part = stream.Part()\n",
    "alto_part = stream.Part()\n",
    "tenor_part = stream.Part()\n",
    "bass_part = stream.Part()\n",
    "\n",
    "for pitch in melody:\n",
    "    melody_note = note.Note(int(pitch))\n",
    "    melody_part.append(melody_note)\n",
    "\n",
    "alto_notes = inversed[0]\n",
    "tenor_notes = inversed[1]\n",
    "bass_notes = inversed[2]  \n",
    "\n",
    "for pitch in alto_notes:\n",
    "    alto_note = note.Note(int(pitch.item()))\n",
    "    alto_part.append(alto_note)\n",
    "for pitch in tenor_notes:\n",
    "     tenor_note = note.Note(int(pitch.item()))\n",
    "     tenor_part.append(tenor_note)\n",
    "for pitch in bass_notes:\n",
    "    bass_note = note.Note(int(pitch.item()))\n",
    "    bass_part.append(bass_note)\n",
    "\n",
    "score.append(melody_part)\n",
    "score.append(alto_part)\n",
    "score.append(tenor_part)\n",
    "score.append(bass_part)\n",
    "score.show('midi')\n",
    "score.write('musicxml', 'output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75011af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune (hyperparameters, move around test data (refer to notes), etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new data + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make any other changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheet music + audio (musicAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new models if time permits (follow steps 3 - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front end ** if time permits\n",
    "# - Interactive sheet music\n",
    "# - musescore front end??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121de9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
