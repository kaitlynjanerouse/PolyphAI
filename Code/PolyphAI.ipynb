{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d361ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include necessary imports\n",
    "import os\n",
    "import torch \n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c67e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "folder_path = 'Data/'\n",
    "test = []\n",
    "train = []\n",
    "validation = []\n",
    "for dirname in os.listdir(folder_path):\n",
    "    if dirname != '.DS_Store':\n",
    "        for filename in os.listdir(folder_path + dirname):\n",
    "            df = pd.read_csv(folder_path + dirname + '/' + filename)\n",
    "            transposed_df = df.transpose()\n",
    "            if dirname == 'test':\n",
    "                test.append(transposed_df)\n",
    "            if dirname == 'train':\n",
    "                train.append(transposed_df)\n",
    "            if dirname == 'valid':\n",
    "                validation.append(transposed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8606521",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "551e0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim=50, n_layers=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_output, (h,c) = self.lstm(x, hidden)\n",
    "        model_output = self.fc(lstm_output)\n",
    "        return model_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09e503",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "511051f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, melody, harmonies, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(melody)\n",
    "        loss = criterion(output, harmonies)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c6e6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training song  1\n",
      "Epoch:  99 Loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.8175e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.9838e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.8709e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.5369e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.0841e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.1333e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4869e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.0491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.6502e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.7477e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.2511e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.0134e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.1687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.5413e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.7171e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.7408e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.0898e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.8000e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.2666e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0531e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.2228e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.0317e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.0599e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1702e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.2121e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.0982e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0935e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0107e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  2\n",
      "Epoch:  99 Loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.8453e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.9959e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.4211e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.4303e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.7514e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.2464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.8464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.5181e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.2460e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.6546e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.2817e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.8007e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.3907e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.3896e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.5179e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.5582e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.2649e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.3452e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.0456e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.7068e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.1917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7873e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.9893e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.7456e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.4831e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.8817e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.9878e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.9343e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.3235e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.5200e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.1641e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.6119e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.5478e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  3\n",
      "Epoch:  99 Loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0443, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.4433e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.0722e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.8755e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.8760e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.8127e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.5822e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.1337e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.7913e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.1425e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.2328e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.2073e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.1487e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.5691e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.5635e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.2630e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.5054e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.8286e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.0720e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.1198e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.3658e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.5607e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.2063e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.6473e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  4\n",
      "Epoch:  99 Loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.5143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.7526e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.7733e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.2307e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.3378e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.2594e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.6290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.2031e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.9300e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.5987e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.8682e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.6351e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.7741e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.1626e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.9773e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.4304e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.2177e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.2001e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.1245e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.3305e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.1567e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.6543e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  5\n",
      "Epoch:  99 Loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.4156e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.8477e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.9663e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.1765e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.0197e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.1905e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.5812e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.9858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.1285e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.4959e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.3375e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.7591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.6188e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0648e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(7.6036e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(5.6998e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.3881e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.4284e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.7020e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.8618e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.0297e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.4443e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.7763e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.3633e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0754e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.6277e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  6\n",
      "Epoch:  99 Loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0167, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.2902e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.0615e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.0115e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(6.0591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.1687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.3921e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.6242e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.0202e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.8876e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.1974e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.0508e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.5760e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.9208e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2199e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.6860e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0382, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.8269e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  7\n",
      "Epoch:  99 Loss:  tensor(0.0570, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.5300e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.9817e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.1484e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.8009e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.6858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.2765e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.9102e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.1182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.6107e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.3993e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.2504e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.1230e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.1009e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.1486e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.1309e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7845e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.7460e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.9403e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.9047e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.9812e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.5378e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.0075e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.3931e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.0530e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.8209e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.5687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.9874e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.0332e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  8\n",
      "Epoch:  99 Loss:  tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0181, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(6.4849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.4638e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.1813e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.5012e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0279, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.9244e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.0393e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.7945e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  9\n",
      "Epoch:  99 Loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.3157e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.4533e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.7794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.9428e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.6236e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.9318e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.4445e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.4921e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.3269e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.5584e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.7673e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.9974e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.6203e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.8757e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.8494e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3068e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.7281e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.6194e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.5032e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.7130e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.0728e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7863e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.9465e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.4719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.9444e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.1181e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.2349e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.2315e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.6042e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  10\n",
      "Epoch:  99 Loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0274, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(8.8672e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(5.5507e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(3.8956e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(2.1703e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(1.5124e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(1.2297e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.3764e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(1.0402e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.9930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.5404e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.7308e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.5069e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.2989e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.9520e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3916e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.3465e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.9775e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.5828e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.6403e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.9603e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.4490e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.0516e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.3414e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.5491e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.3040e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.6350e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.4059e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.6508e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.3078e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.2360e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.6272e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.2420e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.8234e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  11\n",
      "Epoch:  99 Loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0262, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.7593e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.0804e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.5006e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "training song  12\n",
      "Epoch:  99 Loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.7510e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.6595e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.9290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.9911e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.5185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.0708e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.7529e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0169, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.7227e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.3450e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.5530e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.6092e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.0556e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.1220e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.9524e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.5661e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.6580e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.0412e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.6069e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.2921e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0590e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.8353e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.1242e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.6092e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.0208e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.7497e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.4905e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.8216e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4004e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  13\n",
      "Epoch:  99 Loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0274, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(9.5647e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(6.1253e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.1350e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(4.2099e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.1510e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.7742e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.2824e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(6.1212e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.1427e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.1015e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.8599e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.2269e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.5593e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.5358e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.0781e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.9872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.5166e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.6687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.8600e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.7977e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.8108e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  14\n",
      "Epoch:  99 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0429, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.6458e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.0558e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.1583e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.2991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.3111e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.7851e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.1211e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.5819e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.5107e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.0679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.1560e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.5372e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.2559e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.0781e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.0375e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1734e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.5857e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(6.2416e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.1471e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.5436e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.3907e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.2623e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.3701e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.5869e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.7030e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2050e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.9958e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.9941e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  15\n",
      "Epoch:  99 Loss:  tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.9149e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.4115e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.5347e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.3253e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.1758e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.4962e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.2616e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.9805e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.1578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.1482e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.4251e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.8548e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.4060e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.0614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.4028e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.9515e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.6086e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.8143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.6650e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.9298e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.6172e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.8116e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.2549e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  16\n",
      "Epoch:  99 Loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0204, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.6009e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.0960e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.5203e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.1427e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.3920e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  17\n",
      "Epoch:  99 Loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0344, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.3756e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.5969e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.6287e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.2820e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.6651e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.7857e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.3463e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7903e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.8232e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.3610e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1189e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.2923e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.8059e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.4079e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.2217e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.2961e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.7384e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.0243e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.9794e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  18\n",
      "Epoch:  99 Loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0192, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.2609e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.6585e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.7339e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.0778e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.4169e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.8072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.5569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.3617e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.9675e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.8182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.5850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.3404e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.4004e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.5970e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "training song  19\n",
      "Epoch:  99 Loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0280, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.4990e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.7555e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.5501e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.5732e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.7278e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.0823e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.0834e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.5801e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.0417e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.9859e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.1444e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.8388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.6764e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.2879e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  20\n",
      "Epoch:  99 Loss:  tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.3203e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(3.5519e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(1.9876e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.2004e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0470, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0288, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0239, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "training song  21\n",
      "Epoch:  99 Loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0219, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.5704e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.3319e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.5688e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.5985e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.9291e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.4135e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.8291e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.1597e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.6674e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.9581e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  22\n",
      "Epoch:  99 Loss:  tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0211, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(8.9901e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.1120e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.0140e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.7867e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.1658e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.8294e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.4393e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.1403e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.4476e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.8212e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.9876e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.1794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.0378e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.1654e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.5061e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.6273e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.0302e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.6278e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.1129e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2120e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.2963e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  23\n",
      "Epoch:  99 Loss:  tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.0151e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.2928e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.3906e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.8829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.4372e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.9044e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.7631e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.1732e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.5512e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.3693e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.8472e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.4751e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.5547e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.0247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4285e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.1522e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.7154e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.0991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.9701e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.1466e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.8922e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2009e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7685e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4242e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  24\n",
      "Epoch:  99 Loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.7372e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.6439e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.2551e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.9680e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.0281e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.6535e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.1554e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.8422e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.6856e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.1066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.4021e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0262, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0285, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  25\n",
      "Epoch:  99 Loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.0880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.0052e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.7412e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.0834e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.0027e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.2649e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.7771e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.1067e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.1921e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.7280e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.3956e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.0230e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.6252e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.5985e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.4225e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.6409e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4905e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.0404e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.6553e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.9696e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.5012e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.8910e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.6982e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.7746e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4707e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.3459e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.9318e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.8973e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.0604e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2875e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.2208e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.8471e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  26\n",
      "Epoch:  99 Loss:  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.1749e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.9667e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.7670e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.0958e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.9261e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.3008e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.6895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.3096e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.0354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.6575e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.0615e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.0717e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.6976e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.6490e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.8116e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.1242e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.5520e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.0757e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.6838e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.3675e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.8869e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.9407e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  27\n",
      "Epoch:  99 Loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.4978e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.1497e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.1407e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.5204e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.1891e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0307, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.3011e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.8305e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.3779e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.8298e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.6751e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3468e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.8704e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8870e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.2983e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.1407e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0537e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.2046e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.1719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.9135e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  28\n",
      "Epoch:  99 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(8.7405e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(6.6370e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(6.9845e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(4.2578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.1173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.7964e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.0162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.3227e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.8462e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.8106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.8940e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.7208e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.6880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0327e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.1195e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.6519e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.5461e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.2239e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.9634e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.9919e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.6548e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0946e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.4160e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7787e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.0018e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.0940e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.2766e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.7305e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.6613e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.5782e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.3246e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.5569e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.7739e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.5946e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.8791e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.3093e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  29\n",
      "Epoch:  99 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.1641e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.3391e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.9627e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.6541e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.7639e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.1045e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.6071e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.2311e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.4712e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.3312e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.7258e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.4890e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.6970e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.6097e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.5426e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.9379e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8661e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.3154e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.9508e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.2396e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.8336e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.2111e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  30\n",
      "Epoch:  99 Loss:  tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0478, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.2331e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.1399e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.7560e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.6730e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.8656e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.3316e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.0535e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.9160e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.8430e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.8002e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.6956e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.5945e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.2604e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.0781e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.6445e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.0328e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.7048e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.7636e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.2254e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.3599e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.0852e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.7330e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.9022e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.7532e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.7769e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.5586e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0672e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  31\n",
      "Epoch:  99 Loss:  tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.5829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.3717e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.8938e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.2420e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.9140e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.3759e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.5030e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.6149e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.6297e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.4104e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.7201e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.3996e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.0800e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.7075e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.1724e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.5922e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2658e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  32\n",
      "Epoch:  99 Loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.4492e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(6.1596e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.1461e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.8690e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.5338e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.2999e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.9405e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.2320e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.7579e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.1427e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.5749e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.3100e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.3729e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1745e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.5841e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.1264e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.1655e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.3797e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0243, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.5048e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.5841e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.6936e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.7509e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  33\n",
      "Epoch:  99 Loss:  tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(7.7302e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(8.0188e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(4.2990e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(2.8743e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(1.2984e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.6013e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.1980e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.5815e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.7547e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.5311e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.0080e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.0324e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.1437e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.4762e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.9280e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  34\n",
      "Epoch:  99 Loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0138, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.4993e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.1404e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.7134e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.6051e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.9756e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.2619e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.8819e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.4160e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.5104e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.2600e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.0943e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.9702e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.0791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.4206e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.7023e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.6838e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.2836e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.5307e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.0291e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.2209e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.8398e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.3894e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.9732e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.9875e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.2301e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.6209e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  35\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0470, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.4518e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.5788e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.4571e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.7424e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.7386e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  36\n",
      "Epoch:  99 Loss:  tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(9.1651e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.3401e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.5792e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.4100e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.0182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.0320e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.8143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0375, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.2754e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.3884e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.6816e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.5211e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.0566e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.6798e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8394e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.3090e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.4947e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.9531e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.1208e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.7899e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "training song  37\n",
      "Epoch:  99 Loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.9595e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.9927e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.2018e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.9283e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.5314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.2320e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.2757e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.9607e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.3758e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.4105e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.3923e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.0928e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.2262e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.2305e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.6293e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.2889e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3258e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.8251e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.3680e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.0626e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.1446e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.3695e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.4845e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.5098e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4724e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  38\n",
      "Epoch:  99 Loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.6328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.4569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.0198e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.0280e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.2849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.6150e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.9766e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.5977e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.5871e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.4753e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.1061e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.3894e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.3394e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.8801e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.1306e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.0771e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.2824e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.6146e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.5176e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.5117e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.8701e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.9744e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.3354e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.8402e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.4496e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1402e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.9511e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.0189e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.5696e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.4458e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.4787e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.1308e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.7065e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.0179e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  39\n",
      "Epoch:  99 Loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(6.2076e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.1634e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.8034e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.7980e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.2687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.8114e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.1585e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.1627e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.9489e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.0707e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.7348e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.2715e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.2134e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.2689e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.0451e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.7337e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.8538e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.2308e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.1712e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.9946e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.8288e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2812e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.3501e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.6741e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.8626e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.0949e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.9745e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7512e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  40\n",
      "Epoch:  99 Loss:  tensor(0.0527, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(8.4517e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0279, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.6938e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.5067e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.6503e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.5972e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.8956e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.4259e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.1166e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.3420e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.4719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.8996e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.8780e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.9073e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.9307e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.8000e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.9387e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.6027e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7338e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.2862e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.0936e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0764e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.3337e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2110e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.9783e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.7586e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.4244e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.2970e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.4236e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.2755e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.0320e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  41\n",
      "Epoch:  99 Loss:  tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0324, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.4593e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.8789e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.0264e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.4095e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.5113e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.9588e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.5897e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.3192e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.1025e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.1562e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.4617e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.9003e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.6938e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.3175e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.3776e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.5214e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.6415e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4978e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.7149e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.2737e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.9522e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.3206e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.5447e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.0046e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.9063e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.5928e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.7632e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.4637e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.9786e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.4770e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "training song  42\n",
      "Epoch:  99 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.1954e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.1826e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.5693e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.5658e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.1319e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.4931e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.4467e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.2445e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.0462e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.4046e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.3922e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1558e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.3789e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.0580e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.2196e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.4115e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.8868e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.2811e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.0783e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  43\n",
      "Epoch:  99 Loss:  tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.9818e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.0821e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.6283e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.5167e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(6.7055e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.5193e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.4099e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.8138e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4201e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.1358e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.9109e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.8110e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(8.4687e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.7051e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.8525e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.9667e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.5880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.1738e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.4545e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.0873e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1430e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.3283e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.2263e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  44\n",
      "Epoch:  99 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0216, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.7548e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.3499e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.5305e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.2390e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5885e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3167e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.2173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.3701e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.8127e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0692e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.6747e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.6179e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.9869e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "training song  45\n",
      "Epoch:  99 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0190, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.2938e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.9890e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.8228e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.7056e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.3810e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.2109e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.6182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.8221e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.6437e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.1823e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "training song  46\n",
      "Epoch:  99 Loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(6.4031e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(6.5167e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(3.4428e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(2.1149e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(1.3845e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.8911e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.9334e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.7224e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.1629e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.1926e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.1023e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.3608e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.0313e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.2407e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(6.7372e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.8487e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.4541e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.6866e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.9033e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.7244e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.2896e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.1665e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.3023e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.7114e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.1173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.3491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.1389e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.7734e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.2313e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.0571e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.8282e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.7894e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.3586e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3498e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0170e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  47\n",
      "Epoch:  99 Loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.1997e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.3621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.6200e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.4861e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.7642e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.2521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.3218e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.8075e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.8121e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.8985e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.6735e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.8203e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.1881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.6434e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.8634e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2886e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4857e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.0146e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.4967e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.0210e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.0381e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.3315e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  48\n",
      "Epoch:  99 Loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.8719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.6689e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.4421e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.3649e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.2224e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.0309e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0248, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.8072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.2311e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.6790e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.8167e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.2762e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.3803e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.9836e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.8486e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.8132e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.9720e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.3915e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.0051e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.9345e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.9722e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  49\n",
      "Epoch:  99 Loss:  tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.4867e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.5314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.8818e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.2078e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.9384e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.0016e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.9209e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.0479e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.3004e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.7309e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.4882e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.2900e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.0087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.7193e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(8.3095e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.5915e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.3312e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  50\n",
      "Epoch:  99 Loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.8872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.2519e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.0176e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.7302e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.3930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.0892e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.2565e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(5.4042e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.5574e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.6206e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.0362e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.6186e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.2979e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.0435e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.3970e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.7697e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.4828e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.4727e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  51\n",
      "Epoch:  99 Loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0235, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  52\n",
      "Epoch:  99 Loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.9402e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.6013e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.4491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.1553e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.5200e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.8180e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.3911e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.0851e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.6327e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.0259e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.5764e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.5579e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.9596e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.6181e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.4263e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.8739e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.1734e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.9640e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.0690e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.0345e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.7169e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.0823e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8349e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.5295e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1426e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.9735e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.9335e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.4118e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.3745e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  53\n",
      "Epoch:  99 Loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.3379e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.0987e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.6988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.5590e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.2537e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.9953e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.6847e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.6976e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.9234e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.1933e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.4457e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.9480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.9639e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1928e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.9936e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.0395e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.6434e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.2959e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  54\n",
      "Epoch:  99 Loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.6182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.1652e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.5379e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.7618e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.1621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.3569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.7371e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.0770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.1909e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.4877e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.1001e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.5428e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.8771e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.6698e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.7380e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.9840e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.3544e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.8187e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.8772e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.0319e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.9774e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.4903e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.1552e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.9101e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  55\n",
      "Epoch:  99 Loss:  tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0159, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(8.4399e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(2.8926e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(1.9041e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(1.3555e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(1.0899e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(2.3985e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.0579e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.2401e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.3125e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.7127e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.1161e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.1224e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.5927e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.4587e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.3986e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2175e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.2169e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.5936e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.0833e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.5473e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.5835e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.5262e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.5546e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.6515e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.6404e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.9720e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0275e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.1051e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.8608e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.6219e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.4128e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.6946e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.3413e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  56\n",
      "Epoch:  99 Loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.8329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.5084e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.2342e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.4347e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.8529e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.3881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.2588e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.2093e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.2231e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.8427e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.3559e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.5778e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.6857e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.1455e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.0852e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.7047e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.9512e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4859e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.3472e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.9730e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.0929e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7519e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.5609e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.6300e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.3421e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.0812e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.5874e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.6541e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.0529e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.1305e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.7982e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.2696e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.1583e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  57\n",
      "Epoch:  99 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.7011e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.9788e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.1369e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.7088e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.7834e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.9449e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.7152e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.5442e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.5373e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.1304e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.1185e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.1815e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.5915e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.9132e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.0005e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4691e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.3401e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.8421e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.0943e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.8113e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.5237e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.9583e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.0568e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0572e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.9224e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.8062e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.2664e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.3505e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.6354e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.0086e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.7198e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.9891e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  58\n",
      "Epoch:  99 Loss:  tensor(0.0502, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0282, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.2309, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.9906e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.9933e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.6801e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "training song  59\n",
      "Epoch:  99 Loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0271, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.8279e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.7854e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.6535e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.7663e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.2492e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.3136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.7094e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.7383e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.1064e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.5408e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.6493e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.7871e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.1781e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.3488e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  60\n",
      "Epoch:  99 Loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0242, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(7.6603e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(4.7254e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(2.7471e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.9327e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.9832e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.4075e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.1979e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.9522e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.6065e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.4581e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.0398e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.1101e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0187, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.4649e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.8775e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.4847e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.7367e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.0022e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7220e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.4138e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.4778e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.5213e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.2722e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.4852e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.4551e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.8342e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.5348e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.0975e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.6246e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2157e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  61\n",
      "Epoch:  99 Loss:  tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.7393e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.9383e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.9700e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.3680e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.5349e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.0298e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.7602e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.4790e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.9961e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.2703e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.2845e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.7308e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.8114e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  62\n",
      "Epoch:  99 Loss:  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.6495e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.0034e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.8181e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.9500e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.0820e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.4640e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.9690e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5565e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.2149e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.5869e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4836e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.4930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.0514e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.5261e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.2331e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.4371e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.2292e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.5117e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.8589e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.4705e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1992e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.9607e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.3857e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.4987e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.0994e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.3037e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.7718e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.8423e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.5064e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.5038e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  63\n",
      "Epoch:  99 Loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.1401e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.0222e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.0062e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.1229e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.9963e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.4981e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.4724e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.6480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.0643e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.7197e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4853e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.3162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.1963e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.1219e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.0321e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.3429e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.2865e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.3605e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.6142e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.3048e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.3752e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.7662e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.0323e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.2629e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.2836e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.6168e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.9976e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.6730e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.9884e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.9040e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.2496e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.4212e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.4565e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.7717e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  64\n",
      "Epoch:  99 Loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.2080e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.4639e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.4978e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.8328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.8226e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.2176e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.1736e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(6.5339e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.2013e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.8710e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.3502e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.5052e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.5588e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.7374e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.2137e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.3262e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1936e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(9.0026e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.5166e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.7612e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.9698e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.7351e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.2532e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.9242e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.1954e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.9177e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.9158e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.1023e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.4271e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.8589e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.4643e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.8752e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  65\n",
      "Epoch:  99 Loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0440, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.0254e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.5388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.4642e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.7419e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.5645e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.7212e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.4042e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.2160e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.7973e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.7200e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.9212e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.3491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.5163e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.6876e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.2122e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.9221e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.9129e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.7227e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.5466e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.7578e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1304e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.5633e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.4112e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1169e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.3124e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.9876e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.7204e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.9798e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  66\n",
      "Epoch:  99 Loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.9851e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.2601e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.1385e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.3781e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.6541e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.1621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.7545e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.7102e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.2340e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(6.7461e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.0851e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.6386e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.9763e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.3132e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.5397e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.4499e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.4742e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.9214e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.7220e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.5541e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.1631e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0256, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  67\n",
      "Epoch:  99 Loss:  tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.4222e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0444, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.9712e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.8052e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.8333e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.5840e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.2775e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.1983e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.4971e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.1178e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.9405e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.2625e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.9278e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.8481e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.9584e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.3306e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.6462e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.1160e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8002e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.1514e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.3155e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0720e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1692e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.2412e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.3626e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  68\n",
      "Epoch:  99 Loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.4533e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.6673e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.7816e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.6087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.5930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.0235e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.0304e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.5152e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.0511e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.3175e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(9.5681e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4571e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.2720e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.1271e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.9469e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.0861e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.5060e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.2850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.6903e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.8733e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4535e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.4945e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.2937e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.4105e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.4882e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.5255e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2875e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.3317e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.1659e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  69\n",
      "Epoch:  99 Loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0229, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.0687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.5219e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.1509e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.8077e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.0136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.5535e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.7761e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.6126e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.4381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3530e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.6668e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.3155e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.0565e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0523e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.2961e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.2171e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.2565e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2897e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.0389e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.4438e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  70\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.2156e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.9386e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(6.6578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.1263e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.9146e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.0532e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4742e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0390e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.2987e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.2842e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.1490e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(7.0818e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.8613e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.4577e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.3591e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.4808e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.6328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.1520e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0216, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "training song  71\n",
      "Epoch:  99 Loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.4878e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.1114e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.0809e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.7917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.6814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.2058e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.1575e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.2271e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.9492e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.3820e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.1622e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.6553e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.6450e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.5605e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0621, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.6443e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.4792e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.4518e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.3371e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  72\n",
      "Epoch:  99 Loss:  tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.8610e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.9806e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.6314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.6232e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.7227e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.5053e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.5961e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.9153e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.3273e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.8060e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.3467e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.5735e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.6588e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.1554e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.0263e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.3477e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4714e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.5232e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.0615e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.3652e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.1696e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.7206e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.7930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.6784e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.6101e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  73\n",
      "Epoch:  99 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0283, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.4229e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.0253e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.1791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.2211e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.0575e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.8730e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.4182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.1017e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.6484e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.1833e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4797e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.6287e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.1540e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.9616e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.8876e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.9499e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.1053e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.3707e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.3735e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.7290e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.2812e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.5388e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.7696e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.4520e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.8932e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.2898e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.8982e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.4786e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.6610e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.1193e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.1289e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.8790e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.0317e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.9307e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7254e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  74\n",
      "Epoch:  99 Loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.9988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.6550e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.4420e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.7070e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.3580e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.7624e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.5126e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.1329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.2770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.5377e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.5035e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(8.7515e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.8485e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.1074e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  75\n",
      "Epoch:  99 Loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.7881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.6376e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.7408e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.7820e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.2641e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.9834e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.3285e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.4625e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.3055e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.4556e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.2711e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.9947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.9088e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5404e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.1727e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.5174e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2061e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.8379e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.7487e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.2511e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.2766e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.0569e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0290, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "training song  76\n",
      "Epoch:  99 Loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(8.7063e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.3988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.5238e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.5761e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.5667e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.7889e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.3516e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.3793e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.1536e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.7503e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.4345e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.3144e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.8122e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.7675e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.3765e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.3388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.5267e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7773e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.9996e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.8374e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.9081e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.1889e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.7935e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.5724e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  77\n",
      "Epoch:  99 Loss:  tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0442, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0283, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.6278e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.4805e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.8216e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.6681e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.0273e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.9363e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.1325e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.8101e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.8821e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.8560e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5067e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.6219e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.2717e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.6453e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.4651e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.9907e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.1138e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.4887e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.0702e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.7958e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.3410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.3306e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.8997e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.2626e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.0376e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.0414e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.3374e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  78\n",
      "Epoch:  99 Loss:  tensor(0.0498, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.4337e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.4964e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.2802e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.8014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.7906e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.7202e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2344e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4886e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.3903e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.8726e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.7069e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0607e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(9.0037e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.8719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.6605e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0254, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0171, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "training song  79\n",
      "Epoch:  99 Loss:  tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.7808e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.9751e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.1251e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.1963e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.4405e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.4483e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.1767e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.6677e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.6948e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.2016e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.7837e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1340e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.3804e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(8.7527e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.5378e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.0387e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  80\n",
      "Epoch:  99 Loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.0208e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.2810e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.2251e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.5945e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.9185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.5153e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.7788e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.2557e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.4254e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.3723e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.0747e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.6498e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.5287e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.3193e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.5752e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.1293e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0713e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0413e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.5046e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.6004e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.4578e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.3509e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.0009e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.4882e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.5307e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.1896e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  81\n",
      "Epoch:  99 Loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0424, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(6.3441e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(5.3324e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(2.7011e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0239, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.3191e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.1222e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(8.2818e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.5799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(5.0057e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.3924e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.8567e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.8682e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.7954e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.0883e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.5618e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.1387e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7848e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  82\n",
      "Epoch:  99 Loss:  tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0254, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.4710e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.5447e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.6091e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.3806e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.1894e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.3750e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.7629e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.2750e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0278, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  83\n",
      "Epoch:  99 Loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.8903e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.8479e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.1873e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.8286e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.4530e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.7255e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0927, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.9922e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.6014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.4537e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.2477e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.3570e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.7862e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.9727e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.2177e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.3728e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.1396e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.0137e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.1877e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.0798e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.9715e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.0223e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.0600e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  84\n",
      "Epoch:  99 Loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.7066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.2048e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.5858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.6671e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0295e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.5809e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.2559e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.0088e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.1365e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.4184e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.4734e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.5294e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.1770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.0711e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.2031e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.9205e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.8217e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.8776e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.6067e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.2087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.4104e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0044e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.9506e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  85\n",
      "Epoch:  99 Loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0136, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(8.7489e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(5.4521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.6527e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(2.6661e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.0688e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.6552e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.9825e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.2407e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.0117e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.5580e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.1997e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.8077e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.8425e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.4177e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.8888e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.0933e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.8794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.5295e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.7437e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.2014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.7991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.7804e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.1310e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  86\n",
      "Epoch:  99 Loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.9854e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.3567e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.1971e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.8844e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.4576e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.3246e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.1292e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.0238e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.1926e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.5189e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.5489e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.4947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.3291e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  87\n",
      "Epoch:  99 Loss:  tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0317, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0211, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.8830e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.7038e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.2039e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0396, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.2459e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.6563e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.5560e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.1564e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  88\n",
      "Epoch:  99 Loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.2340e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.4207e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.1072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.0850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.8489e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.3683e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.2110e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.6176e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.1501e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.6123e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.6311e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.4631e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.7290e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.7514e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.0467e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.1251e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.3031e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(6.3864e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.8595e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.8931e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.3254e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.7859e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.9778e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.4945e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.1763e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.5156e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.8486e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.5745e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  89\n",
      "Epoch:  99 Loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(9.1486e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(6.3045e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.2947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.5992e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.0309e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.6257e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.0093e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.6432e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.2917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.3678e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.2948e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.8711e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.1929e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.2543e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.1610e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4375e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.9447e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.5976e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.6342e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.0286e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.5622e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.6690e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.6726e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1164e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.4688e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.3073e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.2680e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.3584e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.2235e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.9601e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.9552e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.2594e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.2731e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.9556e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0757e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  90\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.0247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.4051e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.5455e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.2769e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.4630e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.9221e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.2034e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.7877e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.7513e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.4081e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.2647e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0351, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.2945e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.1008e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.6892e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.7955e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.1362e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.6080e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.6486e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4540e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.4418e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.6552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.7468e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.7383e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.0710e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  91\n",
      "Epoch:  99 Loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.3520e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.3028e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.4877e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.6303e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.0626e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.6428e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.3175e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.1030e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.5629e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.0968e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.1183e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.6255e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.4808e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.6986e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.7675e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.7384e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.3738e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.7755e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.6711e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.6218e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  92\n",
      "Epoch:  99 Loss:  tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.4139e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.3377e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.7301e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.0046e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.3612e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.1144e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.1838e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.1268e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.8290e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.3428e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.5956e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.7916e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.5173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.5926e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.5313e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0549e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.0039e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.2540e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.4342e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.9508e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.2044e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.6442e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.9812e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.2455e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.6162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.7491e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2087e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  93\n",
      "Epoch:  99 Loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.5273e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.8437e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.3316e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(6.4441e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.2581e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.5709e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.4396e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.2124e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.4269e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.5814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.2112e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.2863e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.9811e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.4794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.1402e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.3992e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.4983e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.5274e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.7018e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.5821e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.7105e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.8517e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.9858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.0939e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.8411e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "training song  94\n",
      "Epoch:  99 Loss:  tensor(0.0443, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0251, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.7082e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.7840e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.1867e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.6464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.6721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.3495e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.3097e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.7230e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.3456e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.7116e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.9039e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.8393e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.4709e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.9263e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.0548e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.4005e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.9572e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.5556e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2709e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.5809e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.9724e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.2382e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.7693e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1245e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0197, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "training song  95\n",
      "Epoch:  99 Loss:  tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.2726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.5052e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.2780e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.7494e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.4874e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.5721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0684e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.6814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1460e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.4059e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.1139e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4762e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.1075e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.6198e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.7507e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.2128e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1552e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.8156e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.3587e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.3940e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.7267e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2859e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0029e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.1768e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.0888e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.9120e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.8097e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.0226e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.1094e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  96\n",
      "Epoch:  99 Loss:  tensor(0.0550, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.6323e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.6000e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.7245e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.4134e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.9709e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.7157e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.6418e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  97\n",
      "Epoch:  99 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0443, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.9014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.7584e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(8.3410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.2100e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.8792e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.0751e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.1614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.2561e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.8445e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.8025e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.2967e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.6100e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.8410e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.5025e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.1337e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  98\n",
      "Epoch:  99 Loss:  tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.4832e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.7947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.2247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.4424e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.1718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.9387e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.0759e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.0486e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.2697e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.5862e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.3298e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.6206e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.2973e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.2285e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.9643e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.6533e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.8297e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.9892e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.2042e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.0303e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.4843e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.5235e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.5693e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.0973e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.2149e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.4224e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  99\n",
      "Epoch:  99 Loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0317, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0225, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.3072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.5346e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.7986e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.9647e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.3870e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.8481e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.1833e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.5779e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.0963e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.5178e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.3731e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.9495e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.0591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(6.4719e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.3168e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.0906e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.3861e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8393e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4941e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.8203e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.7641e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.0948e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.0686e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.3379e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  100\n",
      "Epoch:  99 Loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0169, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.1707e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.8286e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.3044e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.7709e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.1798e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.4280e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.0434e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.9213e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.7639e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.0596e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.7382e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.8895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.6125e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.0582e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.8505e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.5973e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.8140e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.1378e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.1470e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2985e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0964e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.3495e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  101\n",
      "Epoch:  99 Loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0239, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.9665e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.5926e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.8146e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.7449e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.5674e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.7046e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.6684e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.5368e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.5801e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.9733e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.9028e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.0696e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.1812e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.4185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.4714e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.7120e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.5191e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.8380e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.5548e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.4931e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  102\n",
      "Epoch:  99 Loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0379, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0204, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.9848e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.0521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.7050e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.2136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.7929e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.4432e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.2887e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.6002e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1374e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.2144e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.0184e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.4183e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.2471e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.4064e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.8061e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.6797e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.1920e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.2625e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.7097e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.0787e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.1614e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.0170e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.8378e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.2405e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.3128e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(8.6392e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.8916e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.0313e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  103\n",
      "Epoch:  99 Loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.4294e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.2306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.0804e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.5770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.5957e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.9849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.5223e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.3224e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.0832e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.1992e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.9410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5848e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.8098e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.0963e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.1997e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.4725e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.1865e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.4570e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0413e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.9018e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.9423e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.4776e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0019e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.2582e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.5439e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7095e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  104\n",
      "Epoch:  99 Loss:  tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(9.6233e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.4486e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(8.6461e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.7397e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.5495e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(2.7202e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.0901e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.1370e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.7482e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.7124e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.6095e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.6025e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.3695e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.4203e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.2825e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.5576e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.9060e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.9022e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4091e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.0147e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.1609e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.6657e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0113e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.8315e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.2603e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.9812e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.2937e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  105\n",
      "Epoch:  99 Loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.7397e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.7846e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.5985e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.4201e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.3725e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.9774e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.8632e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.2794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.2062e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0181, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.5436e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.5327e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.4985e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.2802e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.0055e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.1008e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.9633e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.2435e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.8537e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.4446e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.0015e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3172e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1754e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.7485e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.7594e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.8972e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.1595e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0332e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.1923e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.4996e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.7932e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  106\n",
      "Epoch:  99 Loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.8865e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.6460e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.7781e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.8177e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.2608e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "training song  107\n",
      "Epoch:  99 Loss:  tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0321, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(9.7765e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.7501e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.2838e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.1170e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.1437e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.6397e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.0917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.8843e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.8199e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.5248e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.0801e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.4799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.1047e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.3552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.1679e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.8791e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(9.3917e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.4512e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.2452e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.3434e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.0322e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0373, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "training song  108\n",
      "Epoch:  99 Loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.4825e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.4870e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(6.7189e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.5072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.5289e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.9002e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.5919e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.0445e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.1003e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.8283e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.7562e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.2221e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.4265e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(9.1727e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.1155e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.2911e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.2360e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.6157e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.1775e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.7135e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.4039e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.5056e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.8128e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.2404e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.7475e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.3193e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.8025e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.9005e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  109\n",
      "Epoch:  99 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(6.0510e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(3.8259e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(2.5319e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(1.8581e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(1.2358e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.7449e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.8944e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.0784e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.3810e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.6916e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.6591e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0174, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.0488e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.9748e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.8272e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.8944e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.9141e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.6329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.0455e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.2621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.6172e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.3261e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7132e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  110\n",
      "Epoch:  99 Loss:  tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.3816e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.6598e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.5947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.0884e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.8083e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.6539e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.9136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.8940e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.5070e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.1890e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.7538e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.5904e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.8350e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.5245e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.6633e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.6361e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.0123e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.0502e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.6334e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.1385e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.9833e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  111\n",
      "Epoch:  99 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0377, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.3991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.7764e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.8257e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.8110e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.3027e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.6119e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.7037e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.4177e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.4404e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.1308e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.7875e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3061e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.7550e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.5678e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.4607e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.5454e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.0173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.4124e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.7558e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.9895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.1686e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.5363e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  112\n",
      "Epoch:  99 Loss:  tensor(0.0445, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0159, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(7.3753e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.8185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.8043e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(6.7599e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.2220e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.9569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.3925e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.1131e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.2061e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.6546e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.8066e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.3243e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.1148e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.1228e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.3107e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.7318e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.1973e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.0979e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.0902e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.7649e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.1921e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.6128e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.4444e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.5596e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.3248e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.4795e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0398e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.7230e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  113\n",
      "Epoch:  99 Loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.7980e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(6.6076e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.1613e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.2579e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.0198e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.0035e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.0137e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.2824e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.6799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.1561e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.8102e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.5216e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0364e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(7.6553e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(5.7577e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.5850e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.3921e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.5988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.5842e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.8723e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.5586e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.8096e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3357e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0144e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  114\n",
      "Epoch:  99 Loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.9117e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.4139e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.4903e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.3049e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.0849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.5307e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.6694e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.0794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.6568e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.4190e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9827e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.8740e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.7908e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.4416e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.5010e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.3726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.5422e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.0711e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.3060e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  115\n",
      "Epoch:  99 Loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0316, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.1113e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.4778e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.4601e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.6726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.9325e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.1945e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.7656e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.9213e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.6203e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  116\n",
      "Epoch:  99 Loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.0193e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(6.2121e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(5.3574e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.5201e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.0479e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.5624e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.6937e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.0883e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.6568e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.5753e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.1394e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.1318e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.0877e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.0006e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.1483e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.4355e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.5061e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.9030e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.4785e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1665e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.3184e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.1020e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.4848e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.5061e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.9974e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.4127e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.3098e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.5619e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.0105e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.6237e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.0690e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.0875e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.8964e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
      "training song  117\n",
      "Epoch:  99 Loss:  tensor(0.0470, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0171, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.0995e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(7.8963e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.2806e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.2137e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.0815e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.0135e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.5587e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.1148e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.9847e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.1741e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.6106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.3482e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.0822e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.8314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.0143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.4211e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  118\n",
      "Epoch:  99 Loss:  tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(9.0896e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.5184e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.3589e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.9365e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.9983e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.2866e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.7547e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.3764e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.1068e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.1549e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.0375e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.9874e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.9226e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.4250e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.5890e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.3313e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.5561e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.0679e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.7040e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.0424e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.0121e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.4893e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.1795e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.6542e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.9248e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.9894e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.2953e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.3503e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  119\n",
      "Epoch:  99 Loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.4654e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.5190e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.6988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.1293e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.7822e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.2597e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.0823e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.0374e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.1387e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4325e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.0593e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.2113e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.5170e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.2322e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.2223e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.4133e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.8429e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2754e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.2311e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.9222e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.4098e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2567e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.1827e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.2681e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.3312e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9314e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.7970e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.4767e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.6995e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0324, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0195, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "training song  120\n",
      "Epoch:  99 Loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.8881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.4242e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.1948e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.0666e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.0412e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.2723e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.5388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.1034e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.4583e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.1754e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.8934e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.2039e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.3121e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.3733e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.4931e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.3703e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.5647e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.7343e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.7692e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.6838e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.5497e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0153, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.7440e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.4118e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.2418e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.1716e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.6346e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.4802e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.6250e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  121\n",
      "Epoch:  99 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.2630e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.7715e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.1290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.1544e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.5153e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.1308e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5987e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.3295e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.2164e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.7646e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.3621e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(6.4540e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.1943e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.6107e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.2190e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.3753e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.0786e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.7102e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.8963e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.3033e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.3585e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(5.1850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.0153e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.5701e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.1372e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.2556e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.6378e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1960e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "training song  122\n",
      "Epoch:  99 Loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.4086e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.3829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.1873e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.4186e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.5234e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0317e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.6979e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.4594e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.1003e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.5941e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.3226e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.1633e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.0995e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.0614e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.1691e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.3073e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.0364e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.2897e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "training song  123\n",
      "Epoch:  99 Loss:  tensor(0.0536, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.7312e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.9306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.0388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.5013e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.5993e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.9682e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.5395e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.1231e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.1661e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  124\n",
      "Epoch:  99 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.7774e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.7489e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.5473e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.1605e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.6742e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.6357e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.7522e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.7590e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.8546e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.2829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.8380e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.4751e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.5474e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.0261e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.7130e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.6602e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.7787e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  125\n",
      "Epoch:  99 Loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0324, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.9877e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.3950e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.6569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.4185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.3292e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.9939e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.5165e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.4689e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.8630e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.5258e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.8615e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.5197e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.9947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.0740e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.2491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.0850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.8900e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0126e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.7171e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.8753e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  126\n",
      "Epoch:  99 Loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0265, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(6.1056e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.9815e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(5.5088e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.4150e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.5680e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.5012e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.5693e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.4900e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.6913e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.5569e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.7868e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.2312e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.1360e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.5070e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.3679e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4815e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.7843e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3897e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.1192e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.1709e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.5940e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.3373e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.3255e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.5826e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.1927e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.2130e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.9860e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.5133e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.6191e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.6499e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.5431e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.2116e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.5449e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.4491e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7859e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.6282e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  127\n",
      "Epoch:  99 Loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.2688e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.5631e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.3150e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.0937e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.7495e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.8854e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.7455e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0175e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4928e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.1876e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.7941e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.9847e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.6296e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.8881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.9802e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.2253e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.2857e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.1802e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.0735e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2167e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.2367e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.7497e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0275e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.6606e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.0577e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.1032e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.4439e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.9926e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.8210e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.4174e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.3042e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.2011e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  128\n",
      "Epoch:  99 Loss:  tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.3292e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.6559e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.5573e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.8815e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.9910e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.2380e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.1889e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.1463e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.4236e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.1465e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.6222e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.5660e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.1787e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  129\n",
      "Epoch:  99 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0248, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.5670e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.5631e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.0511e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.9470e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0283, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  130\n",
      "Epoch:  99 Loss:  tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(8.1289e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(5.8170e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.5482e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.4272e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.4282e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.6656e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.4309e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.4557e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.4642e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.4382e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.9683e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.6879e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5115e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.2956e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.5210e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.1002e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1113e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.2108e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.5279e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.9698e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0207, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.9329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  131\n",
      "Epoch:  99 Loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.6404e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.1035e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.0027e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.7491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.6170e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.4373e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.3656e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.0251e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.9729e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.7616e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.2096e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.1731e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.7244e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.9721e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4920e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.1549e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(9.0640e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.1898e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.7622e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.6256e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.8204e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.6250e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9364e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.4526e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.5355e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.0113e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4470e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.9651e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.1771e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.8558e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.1363e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7151e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  132\n",
      "Epoch:  99 Loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0261, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.7198e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.4067e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.7064e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.3999e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.0605e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.1686e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.8980e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.5829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.3199e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.8552e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.1747e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1226e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.1646e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.6302e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  133\n",
      "Epoch:  99 Loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0265, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.5113e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.3587e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.7299e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(6.9136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.6168e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.5738e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.8172e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.1991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.5964e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.2925e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.8326e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.5353e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.6403e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.1057e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.6303e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.5596e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.6938e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.1480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.2216e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.4416e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.6740e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.0291e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.7055e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.6612e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.7410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.9633e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.9409e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  134\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.7137e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.1314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.4227e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.5505e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.0919e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.9680e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.0869e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.9000e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.4947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.6128e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.0272e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.3251e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.4512e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.1897e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.3073e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.6614e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.1668e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.1635e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.5590e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2797e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.2888e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.8103e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  135\n",
      "Epoch:  99 Loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0174, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.1226e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.1184e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.6542e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.4547e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.3005e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.3844e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.6408e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.2394e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.3575e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.1707e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.9022e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.6177e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  136\n",
      "Epoch:  99 Loss:  tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(7.4162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(5.4737e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.1458e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.2718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.2083e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.5035e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(1.1189e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.7683e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.3354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.3247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.3564e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.8566e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.6073e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.1203e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.8111e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.8211e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.8540e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3030e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.6537e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.4624e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.9551e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.8628e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.2173e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.7994e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.1562e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.6819e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.8613e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.0487e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3340e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.9309e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.8871e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.7672e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.8454e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0235, grad_fn=<MseLossBackward0>)\n",
      "training song  137\n",
      "Epoch:  99 Loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.3349e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.6463e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.9272e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.7813e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.2693e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.5343e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.2075e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.0248e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.7238e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(6.4397e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.3495e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.3403e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.7390e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(9.8255e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.3751e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.0266e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.6680e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.4814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.6644e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.8625e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.0809e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.9378e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3782e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.8070e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2570e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0311e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.3270e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.7438e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.6832e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.1066e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.9856e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.4933e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  138\n",
      "Epoch:  99 Loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.3584e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.5993e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.2707e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.5644e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.8527e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.3182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.6746e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.6732e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.3460e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.3963e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2121e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.2579e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.4010e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.4435e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "training song  139\n",
      "Epoch:  99 Loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0265, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0327, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.9769e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.0635e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.0877e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  140\n",
      "Epoch:  99 Loss:  tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(6.1988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(4.1290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(4.3087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.9310e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.1976e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.4917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.3976e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.6354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.7551e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.5014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.8597e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.8057e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.1779e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.7180e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.1715e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.2027e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.0549e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.0113e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.4038e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.1631e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.9592e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0863e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.5367e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.0283e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.4523e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.7818e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.3123e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.3021e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
      "training song  141\n",
      "Epoch:  99 Loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.8831e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.6808e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.9591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.8305e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.7586e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.1247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.6941e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.4567e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.2163e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.1424e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.9712e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.5318e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.9128e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.0760e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.7944e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.0601e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.7705e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.5082e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.7512e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0420e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.6891e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.6769e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.6167e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.9655e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.5257e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.2104e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.7513e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.9414e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.1011e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.2700e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.3925e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.0061e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.3150e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.6263e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  142\n",
      "Epoch:  99 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.2555e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.5698e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.3224e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.3225e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.7150e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.0809e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.7281e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.0200e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.6663e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.1905e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.6221e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.2228e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.8412e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.5726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.1021e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.7501e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.2799e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.0749e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.2402e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.4812e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1530e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.9214e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.9425e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "training song  143\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0389, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0182, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.1951e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.6942e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.1799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.5156e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.6287e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.8015e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.5084e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.6022e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(8.1792e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.1331e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.2704e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.6391e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.1387e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.1421e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.4131e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.7912e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.2799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.0621e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.1879e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.8021e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.6752e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  144\n",
      "Epoch:  99 Loss:  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(6.0894e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.1935e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.0297e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.2945e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.7195e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.8259e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.4503e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.6478e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.6613e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.6885e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.2332e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.6478e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0886e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.0008e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.4334e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.0719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.9392e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.1598e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.9919e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.0972e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  145\n",
      "Epoch:  99 Loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(8.2119e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(7.3181e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(4.5578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.2456e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(2.3928e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.0382e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.4526e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.1306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.7721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.9257e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.3277e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.7811e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.8264e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.2079e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.7894e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4461e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0159e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.4091e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.0921e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.9693e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.8267e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1997e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.6217e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.4521e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.0135e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.0392e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.3492e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.8304e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.4173e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.0734e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "training song  146\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.9746e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.2299e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.6064e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.6890e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.4533e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.5530e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.9564e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.6311e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.4101e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.2420e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.1034e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.6106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.6398e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.0726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.9853e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.7758e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.7755e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.8990e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.3096e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.6407e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.1808e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.3288e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.6089e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.1185e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.3240e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.7723e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3458e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.0007e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.7141e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0308, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "training song  147\n",
      "Epoch:  99 Loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0308, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.9783e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.4997e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.9469e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.5328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.8421e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.3977e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.7726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.7153e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.7879e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.3906e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.3230e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.7716e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.7972e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.2105e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7784e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.4442e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.1801e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.6916e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.3951e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.3189e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.5833e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.3641e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.2556e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.4286e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.8763e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.8600e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  148\n",
      "Epoch:  99 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0258, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.7674e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.6695e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.6333e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.3598e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.8594e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.6591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.8138e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.4004e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.0754e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.6362e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2335e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0625e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.9913e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.5881e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.8973e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.2721e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.5618e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.3578e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.0755e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.0625e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.8385e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.3478e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0382, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "training song  149\n",
      "Epoch:  99 Loss:  tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0265, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0197, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "training song  150\n",
      "Epoch:  99 Loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.2612e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.0621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(9.0966e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.8829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.7020e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.9039e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.5552e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.5859e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.4286e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.0038e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.4267e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.1902e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.3422e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.4569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3834e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.9189e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.5076e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "training song  151\n",
      "Epoch:  99 Loss:  tensor(0.0498, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0230, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.0126e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.7980e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.2417e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.3174e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.7355e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.3739e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.4248e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.5798e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.3039e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.1234e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.5580e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.5501e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.0930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.4578e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.9351e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.6831e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.9902e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.1880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.2014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.6115e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.6277e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.7780e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.2589e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.0610e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(8.9594e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.1009e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.8790e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.5367e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.3131e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.8133e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.7735e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  152\n",
      "Epoch:  99 Loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.8972e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.1504e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.7293e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.9006e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.3941e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.6580e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.0646e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.7705e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.6291e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.5103e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.8698e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.4627e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1784e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.6588e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.0032e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.6671e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.1973e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.1728e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.8441e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.7470e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.9723e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1163e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(7.5510e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(5.6663e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.5043e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.7054e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.1189e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.7975e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.6472e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.8479e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.3378e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.5011e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0324, grad_fn=<MseLossBackward0>)\n",
      "training song  153\n",
      "Epoch:  99 Loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0478, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0330, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.4292e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(8.0675e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.9216e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.0404e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.4116e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.5550e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.7158e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.6722e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.8486e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2802e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.4466e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.5393e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.6190e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.7343e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.8067e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.3806e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  154\n",
      "Epoch:  99 Loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.2595e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.4773e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.6716e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.3670e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.0834e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.9267e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.3142e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.2572e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.1397e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.5526e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.3718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.4532e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.5511e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.3536e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.6343e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.5816e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.2334e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.4207e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.9614e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.4591e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.6928e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.3118e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.7050e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.3935e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.1417e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0934e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  155\n",
      "Epoch:  99 Loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0268, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0283, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.5394e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(8.4621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.7723e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.8090e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.4597e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.6095e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.7306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.8116e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.6620e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.9749e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.7652e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  156\n",
      "Epoch:  99 Loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0375, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0179, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.4752e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(6.9333e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.2493e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.1434e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.1191e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.2917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.1619e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.4016e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.5217e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.6971e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.9818e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.6221e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.5171e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0327, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0161, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  157\n",
      "Epoch:  99 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0254, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(9.1087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.8814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.1130e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.0292e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.6268e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.9391e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.6097e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5396e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.2625e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.1409e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.0771e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.0862e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.5307e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0389, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0274, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "training song  158\n",
      "Epoch:  99 Loss:  tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(8.1186e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(3.4882e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(2.4381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.1280e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.2353e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.5518e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.5720e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.9176e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.5002e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.3423e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.6957e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.0966e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.9147e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.5093e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.5379e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.2227e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.0469e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.1071e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.8896e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.7089e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.9395e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.7337e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.8570e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.1895e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  159\n",
      "Epoch:  99 Loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(8.9665e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(6.6135e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.0696e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.9261e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.7918e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.3565e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.5012e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.3416e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.9676e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.3134e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.8643e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.6905e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.2277e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.6886e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.8373e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0161, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "training song  160\n",
      "Epoch:  99 Loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(8.8537e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(5.9068e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.1294e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.0425e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.2369e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.7976e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(1.4241e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.2277e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.0888e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.1402e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.7689e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.7340e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.5553e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.0381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.4062e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.5630e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.7522e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.3314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.3991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.2721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.0156e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.2305e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.4553e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.9237e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.3869e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.9637e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2402e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  161\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.1916e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(7.5820e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(4.7036e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.3060e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.3947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.7497e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.2761e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.2194e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.5802e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.6518e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.2754e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.7763e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.9497e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.6565e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.0022e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.0488e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.4582e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.0748e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.5436e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.1117e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.9035e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.3476e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.6328e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.2423e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.9243e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.1314e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.7486e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.6348e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.7151e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.9454e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.2976e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.7523e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.3111e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.0771e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.1072e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  162\n",
      "Epoch:  99 Loss:  tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0225, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.5646e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.7114e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.8578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.8875e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.4197e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.6310e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.8331e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.9054e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.3076e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.7525e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.8669e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.1136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.0264e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.0741e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.2887e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.6690e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.0470e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(6.6354e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.7039e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.2387e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.5743e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.9599e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.9974e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.6986e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.7523e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.1786e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3863e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2527e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  163\n",
      "Epoch:  99 Loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.7381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.8462e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.6321e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.7075e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.9934e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.4279e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.4203e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.5954e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.3076e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.0256e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.9155e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.6579e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3720e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2423e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  164\n",
      "Epoch:  99 Loss:  tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(9.6188e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(5.0250e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(4.1404e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(2.8167e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(1.1082e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(7.8165e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(7.4306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.1105e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.8030e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.1499e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.5229e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.9404e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.0921e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.7680e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.1954e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.1454e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.0693e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.6723e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.9430e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.5828e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.4557e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.8491e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0351, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0228, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0138, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "training song  165\n",
      "Epoch:  99 Loss:  tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0316, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.6794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.1601e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.0592e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.2476e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.5464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.0357e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.5113e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.3137e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.9655e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.1837e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.3520e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.0992e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.2909e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.6352e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.1671e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.0342e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.2481e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.6654e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  166\n",
      "Epoch:  99 Loss:  tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(7.8390e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(5.6814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(6.0614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.2179e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.4094e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.5982e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.8355e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.0776e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.2087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.3779e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.9148e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.9179e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.8307e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.4753e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2673e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1270e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.0256e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.4796e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.8551e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.3292e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8302e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.1820e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.4442e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.1925e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.3587e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.6802e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2054e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  167\n",
      "Epoch:  99 Loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(6.4853e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.8686e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.2290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.5133e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.2727e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  168\n",
      "Epoch:  99 Loss:  tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0470, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.9117e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.3094e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.0151e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.2992e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.3864e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.9066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.5078e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.2918e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.7090e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.5932e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.9104e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.9026e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.0073e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.4493e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.8571e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.2295e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.0057e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.2255e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.8835e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.5066e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.5690e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.0371e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.3547e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.6390e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.7392e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  169\n",
      "Epoch:  99 Loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0424, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "training song  170\n",
      "Epoch:  99 Loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.1263e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.4056e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.1080e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.3725e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.1849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0927e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5195e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.2513e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(6.3376e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.4877e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.1881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.6554e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.1054e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.0530e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.3398e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.0095e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.6329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.6212e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2235e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.8455e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.6113e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  171\n",
      "Epoch:  99 Loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0261, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.4147e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.6512e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.8794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.0200e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.4385e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.7313e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.8875e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.2991e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.5833e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.4001e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.6266e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.8258e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.4046e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1218e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.1596e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.5933e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.3590e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.6502e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.4328e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.4667e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.2655e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.2141e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.5611e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.0327e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.5094e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.3936e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.3536e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  172\n",
      "Epoch:  99 Loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.2704e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.9557e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.7666e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.9584e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.7464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.3395e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.8230e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.1465e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.2341e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.8263e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.8011e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.6742e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.1301e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.3620e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.9151e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.7952e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.2575e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.4321e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.5532e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.0806e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.5423e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.0791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.6522e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.3678e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.2819e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.0702e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.8903e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.8537e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0056e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4835e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  173\n",
      "Epoch:  99 Loss:  tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(5.1655e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.7411e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.7744e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0260, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0327, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "training song  174\n",
      "Epoch:  99 Loss:  tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.3044e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.9380e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.0480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.0217e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.3187e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.5273e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.3014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.0368e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.8488e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(6.7452e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.4370e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.0721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.0069e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.7685e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.4697e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.4854e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.3472e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.4559e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.7712e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  175\n",
      "Epoch:  99 Loss:  tensor(0.0675, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.9996e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.8669e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.5517e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.4094e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.6540e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.4139e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.5639e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.2072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.5648e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.2590e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.2809e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.3859e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.8521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.7176e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.1780e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.5677e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.6784e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.1058e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.7007e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.3982e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1809e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0098e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.5398e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.4601e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.2577e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.6476e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.4566e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.8296e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.3678e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  176\n",
      "Epoch:  99 Loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.4066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.5668e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.3698e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.7147e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.5894e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.7209e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.1740e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.3022e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.8794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.9313e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.5302e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2502e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0457e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.7738e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.3081e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.9997e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.8506e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.9593e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.7396e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.8476e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.4435e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.2249e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.3514e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.6633e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.0357e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  177\n",
      "Epoch:  99 Loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.3173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.2844e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.1791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.3881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.7574e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.1464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.8072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.6399e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.4277e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0261, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "training song  178\n",
      "Epoch:  99 Loss:  tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.7262e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.3198e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.7872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.1550e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.2810e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.6931e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.2684e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.8185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.9556e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.4738e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.2539e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.9737e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.3481e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.6324e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.0546e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.4985e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.5399e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.1739e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.5521e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.5799e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.0165e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0501e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.5167e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.7543e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.9457e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.4895e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.1962e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0063e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  179\n",
      "Epoch:  99 Loss:  tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.5556e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.1947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.7080e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.7153e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.3031e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.5001e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.1145e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.9849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.9847e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.2605e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.9723e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.2721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.8244e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  180\n",
      "Epoch:  99 Loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0186, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.9805e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.7361e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.4588e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.8266e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.0669e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  181\n",
      "Epoch:  99 Loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(7.0696e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.0077e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.6850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.0022e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.2304e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.7374e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.4400e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.2612e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.4925e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.5818e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.3053e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.6838e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.9911e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.6044e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.6005e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.3482e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.4036e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.8450e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.6118e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.8632e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0308, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0187, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.9077e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.6291e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  182\n",
      "Epoch:  99 Loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.9875e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.1059e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.1926e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.8891e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.1278e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.5364e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.1891e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.9027e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.4191e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.7746e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4307e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.1698e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.4599e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.4558e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.6762e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.1760e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.8278e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.6700e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.0681e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.8699e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.0466e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.2530e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.9858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.9130e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.0234e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.5357e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  183\n",
      "Epoch:  99 Loss:  tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0258, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0179, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0307, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.0168e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.6521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.1034e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.4864e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.4003e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.0103e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.7342e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.1045e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.2917e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(9.1337e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4847e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.7542e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.5295e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.6596e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.1360e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.9260e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.5064e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.7428e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.2777e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(9.7160e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "training song  184\n",
      "Epoch:  99 Loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0392, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.8106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.5429e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.2293e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.8880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.6379e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.5109e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.1723e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.5075e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.5910e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0353, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "training song  185\n",
      "Epoch:  99 Loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0205, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.6240e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(7.5684e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(5.1947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.7722e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.1346e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.1951e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.7339e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.3143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.1128e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.2685e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.3159e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.5140e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.8480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.3662e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.6987e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.5076e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.4254e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.0097e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.1551e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.9325e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.9511e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.3331e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.3545e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.5726e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9135e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0208e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.2340e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.4371e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.3900e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.6989e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.2090e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.8560e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7729e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4720e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  186\n",
      "Epoch:  99 Loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0429, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.8396e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.5091e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.5490e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.4506e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.5582e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.4655e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.0662e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.3554e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.0325e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.9490e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.5629e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.3307e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1585e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.1129e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.3336e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.5253e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.0455e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.7924e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.6094e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.6629e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0242, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.4811e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.0494e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.5594e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.8171e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.8534e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  187\n",
      "Epoch:  99 Loss:  tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0392, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.0622e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.8580e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.4178e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.3499e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.5760e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.3743e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0594e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4237e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.0425e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.9470e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.2465e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.0662e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.9091e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.9116e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.2721e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.8218e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.4875e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  188\n",
      "Epoch:  99 Loss:  tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0321, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.6701e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.0418e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.8641e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.5447e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.8858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.6755e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.0087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.1889e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.7727e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  189\n",
      "Epoch:  99 Loss:  tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.7603e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.7356e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.2632e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.3700e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.7591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.8678e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.2172e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.2658e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.2822e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.8904e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.2649e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.1452e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.2857e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.4464e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.3164e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.5032e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.8742e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.3710e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.9679e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.5759e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.4182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.2098e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.4231e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.0253e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.2244e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.7233e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.3872e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1473e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.1706e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.1242e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.4505e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  190\n",
      "Epoch:  99 Loss:  tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(7.5096e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(5.7056e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(4.2122e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.9573e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.6049e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.1876e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.3719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.3499e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.0919e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.0734e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.3613e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.9536e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.2563e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.3642e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.7024e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.9477e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.6734e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  191\n",
      "Epoch:  99 Loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0153, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0181, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.1484e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.9855e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.5942e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.9201e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.2776e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.5853e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.5311e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.5105e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.2989e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.3932e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.0083e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.0599e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.2543e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.4680e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.0366e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.4698e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.9557e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.9305e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.1786e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.5998e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.1224e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.8439e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3590e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.4837e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.1915e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.7297e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.9262e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.6744e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.9873e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.0535e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.0850e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  192\n",
      "Epoch:  99 Loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.6601e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.8276e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.9601e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.7724e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.5973e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.8309e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.1182e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.4890e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.3795e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.4741e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.3083e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.8901e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.5286e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.6107e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.9555e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0832e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.8887e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  193\n",
      "Epoch:  99 Loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.3645e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.2282e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.1640e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.6507e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.5920e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.0089e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.6946e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.6596e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.3725e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.5102e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.9608e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.5499e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.1663e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.1702e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.9211e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.4287e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.4754e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.5348e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.5638e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.1571e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.0540e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.1789e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.5682e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1066e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.8632e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.0945e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7097e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0455e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  194\n",
      "Epoch:  99 Loss:  tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.4585e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(7.7548e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(6.8180e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.8908e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.8809e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.2426e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.2800e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.5972e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.1320e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.4897e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.9254e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.7268e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.1214e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.1992e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.3176e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.2221e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.5127e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.1082e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.4065e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.1865e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.8146e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.5523e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.1292e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.5271e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.4382e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.6749e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.1913e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.5128e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.6407e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.2731e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.1828e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2581e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3969e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0289e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  195\n",
      "Epoch:  99 Loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.5881e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.7509e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.3801e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.8919e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.1090e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.2077e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2997e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.8127e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.3682e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.7276e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2992e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(8.2283e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.8130e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.3771e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.4051e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.6953e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.1608e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.7520e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4753e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.4338e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0834e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.1711e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.6609e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  196\n",
      "Epoch:  99 Loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0379, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0210, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0169, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.0187e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.5899e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.9902e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.5565e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.9218e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.3011e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.1944e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.2005e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.4257e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.6338e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.1183e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0030e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.5130e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.7047e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.3127e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "training song  197\n",
      "Epoch:  99 Loss:  tensor(0.0570, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0527, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0251, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.9864e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(7.3338e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.5993e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.5084e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.7964e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.0492e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.1430e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.7578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.5101e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.3302e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.8974e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.3777e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.1698e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.0412e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.4499e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.6582e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.9683e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.3391e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.3651e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.9216e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.7569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.3325e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.7280e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.1598e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.3907e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.1814e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  198\n",
      "Epoch:  99 Loss:  tensor(0.0468, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0237, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(6.2757e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(4.5580e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.6758e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.5715e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.8751e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.3659e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.3097e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.1613e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.1623e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.5523e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "training song  199\n",
      "Epoch:  99 Loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0287, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.7122e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.1987e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.2371e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.3579e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.9407e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.8343e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.2953e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.9369e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.8230e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.0104e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.2534e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.3212e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.6398e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.4158e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.4260e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.7061e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.2396e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.9651e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.6738e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.2774e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.7624e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.3118e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.4314e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.8879e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "training song  200\n",
      "Epoch:  99 Loss:  tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.7143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.8812e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.6987e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.0853e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.6257e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.7827e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.2940e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.6049e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.0616e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.4136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.3122e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.8748e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.3410e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.2636e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.4535e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.8153e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0570, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0204, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "training song  201\n",
      "Epoch:  99 Loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0373, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.2767e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.8651e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.6995e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.6134e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.0707e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.5312e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.8873e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.0259e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.3488e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.1631e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.8696e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9508e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.5912e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2823e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.7083e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.0154e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.5909e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.6015e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.7375e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.7891e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  202\n",
      "Epoch:  99 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0205, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.0708e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.3003e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.4766e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.0595e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(9.0460e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.1968e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.0266e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.2169e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.5730e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.6538e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.9160e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.0902e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1941e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.8346e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.3978e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  203\n",
      "Epoch:  99 Loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0182, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.5180e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.7820e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.6329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.3376e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.5612e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.4911e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.7590e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.8212e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.6222e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.1981e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.8670e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.2495e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.5598e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2839e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0325e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(9.2729e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.0889e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.5292e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4567e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.9914e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.8443e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.8030e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.8052e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.9269e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.4774e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.8515e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.9152e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  204\n",
      "Epoch:  99 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.8614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(6.5145e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.0458e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.0066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.1713e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.4796e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.8910e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.1856e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.8556e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.2601e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.0555e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.6805e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.0111e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.8092e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9262e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.2649e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.0101e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.0697e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.9121e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.3332e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.7848e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.3788e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.6636e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  205\n",
      "Epoch:  99 Loss:  tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0239, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0396, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.1577e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.0491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.1895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.2180e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.6979e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.4959e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.2418e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.0175e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.5755e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0958, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.4144e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.4989e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.0813e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.4303e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.5678e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.9003e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.3658e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.7541e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  206\n",
      "Epoch:  99 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.5988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.5693e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.9452e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.9549e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.2029e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.9041e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.1251e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.3206e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.5686e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.4663e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.5702e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.3906e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.0570e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.0195e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.5411e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.4554e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.0494e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.1364e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.4909e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.0109e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.6408e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.1843e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.6227e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0103e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.9378e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.1218e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.0255e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.7062e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.4592e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2755e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.0754e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.5893e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  207\n",
      "Epoch:  99 Loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0375, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0207, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.0137e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.9514e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.2368e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.3354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.9754e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.4968e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.2149e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3013e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.7731e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.1106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.8970e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.6204e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.9387e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.9267e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.9416e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  208\n",
      "Epoch:  99 Loss:  tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(8.6037e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(5.7646e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.7932e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(2.4714e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(1.6679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.2572e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.9896e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.0455e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.7155e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.6573e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.0242e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.8109e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.5246e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.3867e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.3078e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.2410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.5875e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.7402e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.4095e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.2154e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.5274e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  209\n",
      "Epoch:  99 Loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0258, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0211, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.8634e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.0314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.4098e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.0944e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.2103e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.8867e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.1222e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(6.5689e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.4120e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.0229e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.2130e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.0954e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.7914e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.3406e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.7344e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.7829e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.2673e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.8677e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.6339e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.2418e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.4448e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.9331e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.5826e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.7337e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1684e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.9896e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "training song  210\n",
      "Epoch:  99 Loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0268, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0243, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0229, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0207, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0194, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "training song  211\n",
      "Epoch:  99 Loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(9.7922e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(5.6937e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(3.9832e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.2800e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.1590e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.6050e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.2547e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.0106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.2927e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.1900e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.5722e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5974e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.1395e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.9561e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.2173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.7810e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.5850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.8053e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.0787e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.6028e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.5338e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.4417e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.1396e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.0744e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.1688e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.7857e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.2489e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.6618e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.4025e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.9934e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  212\n",
      "Epoch:  99 Loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.1949e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.5660e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.8969e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.3904e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.1736e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.2214e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.0156e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.8885e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.2817e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.7645e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.5492e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.6849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.0947e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.9421e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.8334e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.6741e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.4543e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.4033e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7468e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4356e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  213\n",
      "Epoch:  99 Loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0423, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.3392e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.2729e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.0341e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.0574e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.1308e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.5187e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.1895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.1525e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.3934e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.4321e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.2000e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.0598e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.1423e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.4926e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.9017e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.0923e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.0477e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2502e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.4403e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.8563e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.0579e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.1158e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.8228e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7372e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  214\n",
      "Epoch:  99 Loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0205, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0205, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0225, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.8948e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.3810e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.5372e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.7788e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.1554e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.8080e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  215\n",
      "Epoch:  99 Loss:  tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0527, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0256, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.9714e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.9959e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.0339e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.3434e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.0656e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.4007e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.9336e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.7241e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.1914e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.8443e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.6076e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.6743e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.2687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.1088e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0214, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0169, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "training song  216\n",
      "Epoch:  99 Loss:  tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0400, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0248, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.0004e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.7152e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(7.7587e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(6.6063e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.0376e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.2435e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.0849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.0940e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.8503e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.3989e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  217\n",
      "Epoch:  99 Loss:  tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(9.3957e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(6.7763e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(5.1386e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.2113e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(5.0555e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(2.4858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.8441e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(1.7721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.3008e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.8900e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.8410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.8996e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.9346e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.9733e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.1905e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.6761e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.0746e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.5959e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  218\n",
      "Epoch:  99 Loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0262, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0210, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(7.0002e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.3247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.8385e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.7633e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.9658e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.9482e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.9686e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.0317e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.6507e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.9637e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5446e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.6575e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.5696e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.1749e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.1705e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.4343e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.6808e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.0486e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.9827e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.0129e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.9943e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.1364e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.5687e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.0053e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.5102e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.6611e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.2859e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.3467e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.8376e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.5057e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.7158e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.9393e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.6112e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0009e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.2340e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  219\n",
      "Epoch:  99 Loss:  tensor(0.0498, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.6784e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.7682e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.4694e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.3425e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.1480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.3923e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.2073e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.4215e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.6721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.7593e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.3148e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.0521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.6679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.4788e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.8849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.1829e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.8552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.7822e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.9789e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.8991e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.7481e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.3784e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.5366e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.2811e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.6590e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.8078e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.8273e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.4952e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.5975e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.4893e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7645e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  220\n",
      "Epoch:  99 Loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.0251e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.0121e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.8118e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.1507e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.6170e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.3511e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.6014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.0996e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.7975e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.1851e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.2227e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.0646e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.4956e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.3577e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.3578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "training song  221\n",
      "Epoch:  99 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0171, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.0879e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(8.6068e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(4.7406e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(4.8587e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(2.4496e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(1.9947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.0654e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.4466e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.6060e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.4653e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.8789e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.7101e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.3967e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.8413e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4875e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.3372e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.5546e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.5179e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.9399e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.1630e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.0039e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.3219e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.8382e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.7276e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.7503e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.8403e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.4843e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0878e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  222\n",
      "Epoch:  99 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.2783e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.4791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.2470e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.4064e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.0385e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.9101e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.4038e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.0610e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.7980e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5878e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.4192e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.3173e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1728e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.8017e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(9.9724e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.6798e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.9221e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.3728e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.8784e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.8860e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1101e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.8587e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.8666e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.4019e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.2787e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.4408e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.8354e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.4039e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.2104e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.7769e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  223\n",
      "Epoch:  99 Loss:  tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.6097e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.2361e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.3978e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(5.6549e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.0491e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0153, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.6022e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.5116e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.5293e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.0063e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.0909e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.4682e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.8867e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.1672e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.9227e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8144e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.4387e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.9449e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.7465e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.6679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.6032e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4766e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  224\n",
      "Epoch:  99 Loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0237, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0241, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.9622e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.1653e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.6684e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.7627e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2744e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.8578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.6656e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.1410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.2557e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.0010e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.3941e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.0371e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.9874e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.2956e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.0458e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.8359e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.4182e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.0722e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.3985e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.8096e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7506e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4451e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  225\n",
      "Epoch:  99 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(9.3313e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.3071e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.9074e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.0795e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.9087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.3233e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.4884e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.9688e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.5819e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.7592e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1733e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.1795e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.7098e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.6939e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.5462e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0348e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.4532e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.2195e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.9072e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  226\n",
      "Epoch:  99 Loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.1204e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.2867e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.2403e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0473, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.2336e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.4294e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.8131e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.7789e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.0323e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.4141e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.8757e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.3515e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.3556e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.8498e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8263e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4473e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.1365e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.0592e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.5802e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3415e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.4553e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  227\n",
      "Epoch:  99 Loss:  tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0153, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0229, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "training song  228\n",
      "Epoch:  99 Loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.3622e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(6.6379e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.7666e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.4387e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.5017e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.9865e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.7508e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0346e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.1912e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.0043e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.2255e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.2603e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.3940e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.6066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.6465e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.0766e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.7834e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.1385e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.9988e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.3119e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.7174e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.7411e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.7704e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.2510e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.7956e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.2030e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(8.2123e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.3762e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.4643e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.3288e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2805e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.0494e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.0577e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  229\n",
      "Epoch:  99 Loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0324, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.8917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.5600e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.5647e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.8463e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.3679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.1507e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.8499e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.3852e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.1679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.3582e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.4847e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.2630e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.3948e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.4751e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.6278e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.3153e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.7127e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.9564e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.2349e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.9514e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(6.7851e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.1664e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.5991e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.4835e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.2555e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  230\n",
      "Epoch:  99 Loss:  tensor(0.0570, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.0290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.7533e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.2363e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.7353e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.2783e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.6817e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.1718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.4261e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.4698e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.8427e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.4116e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.1078e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.5494e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.0769e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.5637e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.3993e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.7689e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.9908e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.3041e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.1898e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.2230e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.9320e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0190, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "training song  231\n",
      "Epoch:  99 Loss:  tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.2070e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.9338e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.2999e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.0390e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.0420e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.3406e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.5055e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.6112e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.0464e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.3055e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1919e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.7489e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.4776e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.0055e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.0087e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.3292e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.8446e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.4791e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1927e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.6416e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.6381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(5.9362e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.4475e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3421e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.4159e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  232\n",
      "Epoch:  99 Loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0237, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.9397e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.9701e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.6960e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.7931e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0344e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.6356e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.1310e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.9773e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.3428e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.2574e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.0544e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.6216e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.2104e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.0212e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.3405e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.2753e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.9770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2995e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.8137e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7225e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  233\n",
      "Epoch:  99 Loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.4959e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.4170e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.9780e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.5914e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.6403e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.3690e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.8687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.7001e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.1895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.7662e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.8349e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.5520e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.0030e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.3223e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.6987e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.3002e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.0958e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.1379e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.5684e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  234\n",
      "Epoch:  99 Loss:  tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0225, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.4614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.4996e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.3790e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.5394e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.0624e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.7794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.3116e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.2411e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.5365e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "training song  235\n",
      "Epoch:  99 Loss:  tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0256, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.8633e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.9149e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.8234e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(9.7791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.3797e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(7.0350e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.7897e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.7679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.7845e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.0518e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.4731e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.0214e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.6765e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.4168e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.2214e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.3307e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.8088e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.7743e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  236\n",
      "Epoch:  99 Loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0258, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.5765e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(6.4445e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.8198e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.8373e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.2524e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.7674e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.3426e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1379e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.2724e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.3589e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.0544e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.0783e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "training song  237\n",
      "Epoch:  99 Loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.6306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.0677e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.4631e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.1216e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.3313e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.5917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.8988e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(8.6037e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.4603e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.9726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.2368e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.5104e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.1609e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3408e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.6090e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.4447e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.2796e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.6023e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.7769e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3559e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.3869e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  238\n",
      "Epoch:  99 Loss:  tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0327, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0274, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.1466e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.6230e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.2423e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.8923e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0898e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.6792e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.5822e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.3512e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.6579e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.0132e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.2243e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.3155e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.3331e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.3323e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.6074e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.6930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.5159e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.9176e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.1336e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0307, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.8791e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  239\n",
      "Epoch:  99 Loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.0406e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.9718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.5395e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.0666e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.9527e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.3509e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.0462e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.7902e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.4532e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.3672e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1986e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.0619e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.0760e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.9574e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(9.3695e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.1128e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0370e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.7621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.4670e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.0639e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.7217e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.1940e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.8758e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.8728e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.4220e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.3176e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.4585e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.7952e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2962e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.9211e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.6300e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  240\n",
      "Epoch:  99 Loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0406, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0169, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.8175e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.5811e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.3355e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.7735e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.9965e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.1050e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.3266e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.4289e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  241\n",
      "Epoch:  99 Loss:  tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.4061e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.2229e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(6.0398e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.2621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.1924e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.9563e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.9324e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.6121e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.0544e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.8893e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.6661e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.7442e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.5560e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.0562e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.0178e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.6730e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.1303e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.2799e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  242\n",
      "Epoch:  99 Loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0254, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0167, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0260, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0142, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "training song  243\n",
      "Epoch:  99 Loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(6.2106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.8060e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(1.2412e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.4850e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.2534e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.1187e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(4.0325e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.1851e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.6982e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.7092e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.6741e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.3382e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.3830e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.5516e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0659e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.9828e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.9892e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0942e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.3894e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.7112e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9813e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.3825e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.1605e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.4386e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.0784e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.1382e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.3338e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.7140e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.5887e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  244\n",
      "Epoch:  99 Loss:  tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(8.5388e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.8547e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.4290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.2769e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.7822e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.6655e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.8824e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.2571e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.3287e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.5591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.4608e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.2019e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.7486e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.8648e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.7619e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.8354e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.3352e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.9563e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.4620e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(9.6880e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "training song  245\n",
      "Epoch:  99 Loss:  tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.9187e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.8027e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.7623e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.5767e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.1726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.6018e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.1794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.1975e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.3724e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.0426e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.4374e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.7779e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.6375e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.4171e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.1795e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.9365e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.4817e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.4418e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0065e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.4175e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.5813e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.2348e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.2258e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.4665e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.1836e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  246\n",
      "Epoch:  99 Loss:  tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0194, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(5.9711e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(6.5290e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(4.2162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.6452e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.4360e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.8592e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.4941e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.2503e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.0833e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.5797e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.9175e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.8306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(7.3718e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(6.5364e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.2206e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.9097e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.0118e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.1453e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.1172e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.4660e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.0021e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.6658e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4496e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1650e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.9708e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.2117e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.5616e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.7635e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.2900e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.8138e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.6827e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.5649e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.1336e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3451e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(7.9600e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  247\n",
      "Epoch:  99 Loss:  tensor(0.0468, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0230, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(8.8799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(4.4907e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(3.6793e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(2.6854e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.0171e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.3596e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.6481e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.4318e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.3875e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.2531e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.3614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.6281e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.4357e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(8.8944e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.7279e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.9120e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.5902e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.1094e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.3181e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4649e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.3897e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.3009e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.5734e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.3391e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.3704e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.0936e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.2659e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.6354e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1549e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.3986e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.0720e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.7502e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.7375e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  248\n",
      "Epoch:  99 Loss:  tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(7.0078e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.8790e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.0293e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.7247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.9986e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.5406e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.2133e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.6794e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.7749e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.0489e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.9725e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.8548e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.0835e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.2509e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.9621e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.5852e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.2705e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.9052e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.1857e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.7356e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.4204e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.1837e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.9818e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.4865e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.2200e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  249\n",
      "Epoch:  99 Loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0194, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0195, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.7098e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  250\n",
      "Epoch:  99 Loss:  tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0498, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0228, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.9131e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.6025e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.0165e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.8063e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.2432e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.4366e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.9827e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.5908e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.7940e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.1770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  251\n",
      "Epoch:  99 Loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0455, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.4394e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.5591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(9.3072e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.4633e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.2767e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.4364e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.8310e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.5589e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.1066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(9.6280e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.4754e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.5782e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.6901e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9047e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.3632e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.9887e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.2717e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.7311e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1188e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.9076e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.6721e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.1315e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.7919e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  252\n",
      "Epoch:  99 Loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0174, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.6794e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.3420e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.8135e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.8381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.1354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.6018e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.1848e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(8.5912e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(6.1502e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.4578e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.5845e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.7407e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.7912e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.8160e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.1978e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.3059e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(8.7983e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.5946e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.3021e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.9444e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.1978e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.5311e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.3635e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  253\n",
      "Epoch:  99 Loss:  tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.2981e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.6287e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(6.8273e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.1172e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.7872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.6800e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.0821e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.6954e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.4219e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.2161e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.0505e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.0643e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.7035e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.3756e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.1200e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.5746e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.2514e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.5158e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.8204e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.6502e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.7552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.2005e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3675e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.8505e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4949e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2327e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0302e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.6888e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.3739e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.2859e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.3764e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.1313e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.0266e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  254\n",
      "Epoch:  99 Loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(9.9008e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(7.3740e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(4.1105e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(2.3583e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(1.7542e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(1.4834e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(1.4311e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(1.0282e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.7048e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.5744e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0317, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.3225e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.2886e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.9809e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.1230e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.3058e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.5349e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.1642e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.2046e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.5511e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.5858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.8107e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.8171e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.2195e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.4531e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.4972e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(8.1704e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.4299e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.3003e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  255\n",
      "Epoch:  99 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0246, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.2438e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(5.0974e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.1837e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.6223e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.0479e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.8848e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.8189e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.9797e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.2054e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.6538e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.1066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.6035e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.2896e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.2321e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(8.9988e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.0943e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.9721e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.6805e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.9066e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.3384e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.6653e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  256\n",
      "Epoch:  99 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(8.2714e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.4645e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.8907e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.3723e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.5749e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.1219e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.2041e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.0325e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.4454e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.2962e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.6028e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.1748e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.5600e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.8335e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.3967e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.1048e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.9713e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(7.4250e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.2335e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.2923e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.5362e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.9213e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.3205e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.4263e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "training song  257\n",
      "Epoch:  99 Loss:  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.9632e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.1532e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.6661e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.4328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.5844e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.1656e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.4833e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.4778e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.3229e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.1665e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  258\n",
      "Epoch:  99 Loss:  tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.8241e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.9844e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.3230e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.5921e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.9859e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.0058e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.6086e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.8935e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.4295e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.1040e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.6136e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.7062e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.1556e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.8869e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.8682e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.0808e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.5020e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0976e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.2473e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.9991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.4051e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.2849e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.0428e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.3643e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.5953e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  259\n",
      "Epoch:  99 Loss:  tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0443, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.3935e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.9592e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.4076e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.5671e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.9004e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.5493e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.2261e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.4936e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.9586e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.0174e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.2559e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0291e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.6215e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.0358e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.2460e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.1738e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.5770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.6891e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(5.9762e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.4515e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.6971e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.8924e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.3078e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "training song  260\n",
      "Epoch:  99 Loss:  tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.8372e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.0929e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.8647e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.1012e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(5.0567e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.8955e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.1629e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.4935e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(8.9726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.0746e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.1048e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.3904e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.6924e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0184e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.9571e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.6543e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.5157e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.6040e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.5181e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.1328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.5176e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.9158e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  261\n",
      "Epoch:  99 Loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.6711e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(7.6385e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.7017e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.1898e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(4.5923e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.3443e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(5.0717e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.4162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0356e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.3798e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.1652e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.8602e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.8179e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.3880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.1262e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.3852e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.9145e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.7015e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.7052e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.2858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(6.6625e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.4818e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.4874e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8372e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.7924e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.1440e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.1109e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.1667e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
      "training song  262\n",
      "Epoch:  99 Loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0307, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.7187e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.9871e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.9426e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.0450e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.7143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.2818e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.6552e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.1183e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.6381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.1363e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1277e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.9861e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.2423e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  263\n",
      "Epoch:  99 Loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0262, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.9915e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.3996e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.8513e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.2963e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.8127e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.8718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.6679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.5199e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.8917e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.4589e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.1362e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.8719e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(6.8771e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.2374e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.9126e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.5621e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.8364e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.3973e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.4484e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.7890e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.3292e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.0083e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.9891e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.0040e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.5150e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "training song  264\n",
      "Epoch:  99 Loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0657, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(9.1249e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.7588e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.5839e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.6559e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.8745e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.9361e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.9814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.9382e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.3499e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.3543e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.8857e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.5158e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.4980e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.6601e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.8951e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.6507e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.0039e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.3777e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  265\n",
      "Epoch:  99 Loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.5720e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.9413e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.8231e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.3330e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.4351e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.8194e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.6931e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.1120e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.4451e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.5119e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.7919e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0214, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(8.1737e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "training song  266\n",
      "Epoch:  99 Loss:  tensor(0.0478, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.7521e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(7.7124e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(5.7979e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.7699e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.7810e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.7106e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.0086e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.4859e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.0618e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.5034e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.1858e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(9.8921e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.3744e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.2644e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.8967e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.3847e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.9867e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.7541e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.9574e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(5.8646e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.9099e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.9087e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.1680e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.4676e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.8381e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  267\n",
      "Epoch:  99 Loss:  tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.0646e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.4179e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.4130e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.9939e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.2814e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.5216e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.0358e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.1108e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.0848e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.5861e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.6567e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0765e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(5.2492e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(3.7391e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.1945e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.7886e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.1736e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1797e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.9085e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(6.4609e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(4.0301e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.3736e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.5634e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.8754e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.2062e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.7086e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.6464e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.7405e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.9774e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0980e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  268\n",
      "Epoch:  99 Loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0214, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0285, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.4851e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.1447e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.8670e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.4012e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.0845e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.4565e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.0582e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.1674e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.0821e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.6687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.8089e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.2855e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.2229e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.2108e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.8000e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.9745e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.9771e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.2359e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.2359e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.9248e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.7734e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.9077e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  269\n",
      "Epoch:  99 Loss:  tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0454, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0190, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0214, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0287, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0230, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.4467e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.0256e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.9174e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.1228e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.1272e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.7824e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  270\n",
      "Epoch:  99 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0248, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(8.3278e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.5085e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.7202e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.0581e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.0686e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.1782e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.8480e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.1275e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.6918e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.0754e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7747e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  271\n",
      "Epoch:  99 Loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0424, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.6863e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(7.7862e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.6129e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0219, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.6348e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.7080e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.1228e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.5080e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.4299e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.4889e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.7197e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.1098e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.9310e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.4826e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.3983e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.0651e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.0293e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.0538e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  272\n",
      "Epoch:  99 Loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0211, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.4889e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.1247e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.0799e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.4206e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.9006e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.5739e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.3719e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.6883e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.1556e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4757e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0548e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.7010e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(8.0267e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.7190e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.6266e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(5.1215e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.6464e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  273\n",
      "Epoch:  99 Loss:  tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0392, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.5183e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.4463e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(9.3225e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(8.1177e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.1643e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.6757e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.3043e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.3553e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(6.5718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.5907e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.2737e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.7172e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.3168e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.1758e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.8531e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2822e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.1550e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.8635e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.9880e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.6096e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.9048e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1702e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.0000e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.6624e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.1540e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.4466e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  274\n",
      "Epoch:  99 Loss:  tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0167, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(9.9395e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0271, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(6.5995e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.3733e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.9107e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.9136e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.3177e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.2744e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.0062e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.4579e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.5039e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.6408e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.7853e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.0647e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.2682e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.7151e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.5825e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.5605e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.7438e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.0991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.3432e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.3359e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.9357e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.9131e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.8322e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.2112e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  275\n",
      "Epoch:  99 Loss:  tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(9.1031e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.9297e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(5.4785e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.4929e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.7340e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.7487e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0610e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.6706e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.1962e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.8378e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.2347e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.2840e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.9402e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.5592e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.6737e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(5.7581e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.8896e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.5174e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.0789e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.0087e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.5153e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.2955e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.7196e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.9479e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.2194e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.6753e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.0859e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(7.9202e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.1708e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  276\n",
      "Epoch:  99 Loss:  tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0285, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0235, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.9421e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.5626e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.4498e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.2228e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.3957e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.1584e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.3215e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.9323e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.6329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.2285e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.3061e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.8025e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.5831e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.4528e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.3640e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.3687e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.4544e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(7.2915e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.4355e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0725e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  277\n",
      "Epoch:  99 Loss:  tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(8.9354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.9255e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.2070e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.7920e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.2464e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.5872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.5262e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.3217e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.8214e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  278\n",
      "Epoch:  99 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.0581e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.4884e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.2075e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.1613e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.4775e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.9164e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.5690e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.2055e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.6503e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(7.6676e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.4468e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.3957e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.6650e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(7.1914e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.3329e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.7117e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(4.5942e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.0672e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.4825e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.4791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.0650e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.6330e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.5209e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.6663e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.0012e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(8.6315e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7685e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  279\n",
      "Epoch:  99 Loss:  tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.7969e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.6341e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.1543e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.5148e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.9328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.6063e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.3612e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.9349e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.2396e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.6506e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.6225e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.4135e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(9.8243e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.4557e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.7690e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.1292e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.8189e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.9058e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.7811e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  280\n",
      "Epoch:  99 Loss:  tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.6293e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.9910e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.1425e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.8187e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.0499e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.7649e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.2343e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.6505e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(9.9866e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.2053e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(4.4456e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.8477e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.8524e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.8091e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.2111e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.0727e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.5456e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.6749e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.6081e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.8835e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.3469e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.7900e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.6167e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.1852e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.2096e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.9622e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(8.4101e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.1314e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.2291e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.2805e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.4475e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.0064e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  281\n",
      "Epoch:  99 Loss:  tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(9.8152e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(9.8600e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(4.4818e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.2576e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.8774e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.2680e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.2009e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.1213e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.1471e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.9208e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.0865e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.5234e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.1302e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2731e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(6.5036e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.9619e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.8444e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.9182e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.2988e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.7538e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.4721e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.1313e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.6237e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.2473e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.0649e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.8407e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.3610e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  282\n",
      "Epoch:  99 Loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0236, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(8.4465e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.5934e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.3129e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.3982e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.2333e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.6338e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.5038e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.9591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.5783e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.9657e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(4.8575e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.2608e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.1706e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.3477e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.7995e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.3872e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.0692e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.4526e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.5913e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(9.9196e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(7.0259e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0368e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(3.4855e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.7991e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.0931e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(7.3015e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.5699e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.9684e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.2844e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.3474e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.2668e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  283\n",
      "Epoch:  99 Loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(8.9932e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(7.2052e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.1089e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.2357e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.6673e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.5157e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.6216e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.8186e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.4742e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(1.8402e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3878e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(9.8532e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.4552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(5.5967e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.1552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.0768e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.2861e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.7108e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.2933e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(9.9039e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.1938e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(6.2276e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.9648e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.7535e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.4353e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.2250e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.9671e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(5.5822e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.0694e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(3.0711e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  284\n",
      "Epoch:  99 Loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.8954e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.9248e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.5264e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(2.7446e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(2.2269e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.8537e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(6.8693e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.4970e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.2359e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.0390e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.6552e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.4637e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.2718e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.5819e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.4047e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.5447e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.9188e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(4.8643e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.3497e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.3444e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.5940e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4357e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.5301e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.1276e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.8691e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.1088e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.6184e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2855e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.0482e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(8.7156e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  285\n",
      "Epoch:  99 Loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0396, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0186, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.9218e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.8905e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(5.4718e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(5.0657e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.4907e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.2662e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(3.2964e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.2008e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.9691e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.2056e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.1865e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.4044e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.2844e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.1531e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.5796e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.3145e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.3678e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(9.9935e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.8586e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.6283e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(7.0179e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(8.4057e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(6.8205e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.4162e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.5143e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  286\n",
      "Epoch:  99 Loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(8.6535e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(6.5284e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(5.0122e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.8769e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.0101e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(2.3371e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.8129e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.0654e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.8870e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.3591e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.0628e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.5924e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.0600e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(5.8362e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.8221e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.4957e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(9.8825e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.2363e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.8023e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.9247e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.2984e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.8349e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.4882e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.2263e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0256e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(8.6834e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.4226e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.6207e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.7615e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.5425e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.1922e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.5272e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  287\n",
      "Epoch:  99 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(8.7886e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(6.0730e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.9847e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.5183e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.6872e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(4.3840e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.4214e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.7296e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(5.2459e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.4096e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.5306e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.5532e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.6703e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(2.9574e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.8145e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.2303e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.9088e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.7664e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(5.3324e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.3300e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4705e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.7532e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(3.0353e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.6021e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(3.3644e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.6853e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(4.4929e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.1858e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.9132e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(2.5770e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.9181e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.0038e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(4.4024e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.7040e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  288\n",
      "Epoch:  99 Loss:  tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0169, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.6512e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(4.5961e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(3.7107e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(6.8013e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.8237e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.8498e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.5717e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.2930e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.1219, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.2333e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.0411e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.4577e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.9941e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.6478e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.6319e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2782e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.9014e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(6.8578e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  289\n",
      "Epoch:  99 Loss:  tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0317, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(8.9904e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.6093e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.3355e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.0895e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.6185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.0210e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.3636e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.3774e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.5717e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(5.0246e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.4512e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.6143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.2113e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(9.5790e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.7453e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.4158e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(9.5817e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(4.4234e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.9252e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.9496e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.9586e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.0458e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  290\n",
      "Epoch:  99 Loss:  tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0947, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0321, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(7.9952e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.4085e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.9802e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.0866e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.5936e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(4.0825e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.9453e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.8057e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(2.5076e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.7925e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(1.2724e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(4.2198e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.3614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(2.1352e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(9.4037e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.0642e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.7316e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.4970e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0081e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.1086e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(3.0275e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(6.8657e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.2955e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.2116e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.2192e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.2972e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(1.4432e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  291\n",
      "Epoch:  99 Loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0389, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.9791e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.1066e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.8430e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.9114e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(3.4384e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(4.0443e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.2496e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.6567e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.2744e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(9.8408e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(7.6250e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(6.0038e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(4.8178e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(6.2162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(6.9679e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.6381e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(3.4593e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(2.6731e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.0967e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.6558e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.3095e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.0325e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(8.0949e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.3100e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.9035e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.0422e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.4506e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(9.0531e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.1594e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.4004e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  292\n",
      "Epoch:  99 Loss:  tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0262, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(8.3108e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(7.9413e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(4.6855e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(3.7130e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(2.8277e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.2734e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.8692e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(1.5495e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.2812e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.1126e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(8.7594e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(7.5070e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(6.2464e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(2.0842e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.6897e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(3.0700e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.2410e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0386, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.7228e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.8458e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.2306e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.6156e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(7.3328e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.9921e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.4132e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.6913e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.2410e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(9.3173e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  293\n",
      "Epoch:  99 Loss:  tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0389, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(7.9748e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0396, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(9.4875e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(9.9595e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(5.3212e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.0694e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.7773e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.2970e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(3.6509e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.1773e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(1.5261e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.1380e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(9.0990e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.5639e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(6.4179e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(5.5045e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.7449e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.3818e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(3.6510e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.1220e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.7578e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.1349e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  294\n",
      "Epoch:  99 Loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0265, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.0577e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(6.7650e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.5675e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.4424e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.6143e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(3.7991e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.7452e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.0150e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4162e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.7647e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.5458e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.0010e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.1565e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.5539e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.0931e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  295\n",
      "Epoch:  99 Loss:  tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(7.0747e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0231, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.7161e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(9.4185e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(6.2929e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.6386e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(4.1682e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(3.3398e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.7427e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.2730e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(1.9102e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.7869e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(1.4506e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.5746e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.3770e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1483e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.4889e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(5.2794e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  296\n",
      "Epoch:  99 Loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0423, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0237, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(7.3186e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(3.2207e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.2981e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.5078e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.7074e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.2354e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(1.0665e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(6.9417e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(8.5691e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(4.0678e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.2892e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.8931e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(7.7957e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.8301e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.5122e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.7001e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.5726e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(8.1328e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(9.7855e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.0071e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(4.2510e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(3.0564e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.3114e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  297\n",
      "Epoch:  99 Loss:  tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(9.4908e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(6.7841e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(4.6864e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(3.7360e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(2.6952e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(1.7011e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.2518e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(9.8194e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(1.2614e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(1.2648e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(1.6942e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.5462e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(3.3011e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(8.6283e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(5.4811e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(3.4817e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.1641e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.2861e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(7.5839e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.7818e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.7153e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.4738e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(4.8930e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(1.5892e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(2.3465e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.1662e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(9.8184e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.7133e-05, grad_fn=<MseLossBackward0>)\n",
      "training song  298\n",
      "Epoch:  99 Loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(9.4362e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(6.3630e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.0330e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(3.6140e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.7493e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.7698e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(1.9574e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(1.4042e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(9.9727e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(7.0470e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.0355e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.7487e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(4.2305e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.7836e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(2.0378e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.5788e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.2622e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(1.0277e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(8.4700e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(7.0449e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(5.9165e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.4359e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(2.7585e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(1.3707e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(8.4604e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.9253e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(4.4828e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(3.5661e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(2.9328e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.4661e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  299\n",
      "Epoch:  99 Loss:  tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0367, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0228, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "training song  300\n",
      "Epoch:  99 Loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(8.1825e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(8.6258e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.1418e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(4.5315e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(3.4329e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(2.5893e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(3.7707e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(2.4396e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(1.7589e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(1.3117e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(1.0069e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(8.1002e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.2125e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(7.7262e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(4.5076e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.5522e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.3441e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.9333e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(2.3345e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "training song  301\n",
      "Epoch:  99 Loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(8.9740e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(7.2444e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(6.0209e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(5.1018e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(4.3057e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(3.3815e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.7100e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.3413e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(4.5323e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.5493e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.8592e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(2.1887e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.7249e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.3324e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(1.0657e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "training song  302\n",
      "Epoch:  99 Loss:  tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.9793e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(4.2941e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(3.2230e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(7.1318e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(2.6609e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(8.9789e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(7.8949e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(3.8569e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.9646e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(4.3679e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(2.2283e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.3780e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.3008e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(2.3148e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(2.7150e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(7.6205e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(6.2333e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.8046e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.3699e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(6.2205e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.3100e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(6.5738e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(4.1871e-06, grad_fn=<MseLossBackward0>)\n",
      "training song  303\n",
      "Epoch:  99 Loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(9.1251e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(5.5386e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(3.5208e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(2.2402e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(5.9879e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(1.2381e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(9.5983e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.4371e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(6.2481e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(4.9533e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(2.6528e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(8.9304e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(5.4342e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(3.7879e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(2.7776e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(2.0908e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(1.6033e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.2513e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(9.9488e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.4572e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(7.3148e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(6.0442e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(1.5954e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(7.9948e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(5.7131e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(4.5352e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(3.7847e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(3.2457e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(7.0330e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(2.8358e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.4365e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.5344e-07, grad_fn=<MseLossBackward0>)\n",
      "training song  304\n",
      "Epoch:  99 Loss:  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  199 Loss:  tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  299 Loss:  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  399 Loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  499 Loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  599 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  699 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  799 Loss:  tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  899 Loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  999 Loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1099 Loss:  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1199 Loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1299 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1399 Loss:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1499 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1599 Loss:  tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1699 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1799 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1899 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  1999 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2099 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2199 Loss:  tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2299 Loss:  tensor(5.1519e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2399 Loss:  tensor(2.7167e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2499 Loss:  tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2599 Loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2699 Loss:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2799 Loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2899 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  2999 Loss:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3099 Loss:  tensor(7.6253e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3199 Loss:  tensor(6.1300e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3299 Loss:  tensor(3.7275e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3399 Loss:  tensor(2.6298e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3499 Loss:  tensor(1.9854e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3599 Loss:  tensor(1.3947e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3699 Loss:  tensor(1.0438e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3799 Loss:  tensor(8.0430e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3899 Loss:  tensor(5.8872e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  3999 Loss:  tensor(1.5481e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4099 Loss:  tensor(6.5776e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4199 Loss:  tensor(4.6901e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4299 Loss:  tensor(3.6331e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4399 Loss:  tensor(2.9154e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4499 Loss:  tensor(2.3915e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4599 Loss:  tensor(5.6755e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4699 Loss:  tensor(1.8160e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4799 Loss:  tensor(1.5109e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4899 Loss:  tensor(1.3374e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch:  4999 Loss:  tensor(2.0184e-06, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "epoch = 5000\n",
    "index = 0\n",
    "for song in train:\n",
    "    index = index + 1\n",
    "    print(\"training song \", index)\n",
    "    song = train[0]\n",
    "    scaler_melody = MinMaxScaler()\n",
    "    scaler_harmonies = MinMaxScaler()\n",
    "    melody = torch.tensor(scaler_melody.fit_transform(song.iloc[0].values.reshape(-1,1)), dtype=torch.float32).unsqueeze(0).reshape(1,song.shape[1],1)\n",
    "    harmonies = torch.tensor(scaler_harmonies.fit_transform(song.iloc[1:].values.T), dtype=torch.float32).unsqueeze(0)\n",
    "    model = Model(1, harmonies.shape[2])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    train_model(model, melody, harmonies, optimizer, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234bbe1-0257-4c91-a95f-263342853d92",
   "metadata": {},
   "source": [
    "# Hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1d90b-1d91-4fcc-a156-6edcb721d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = [0.01,0.001]\n",
    "n_layers= [1,2,3,4,5]\n",
    "epochs= [100,1000,5000]\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "\n",
    "for LR in learning:\n",
    "    for n_layer in n_layers:\n",
    "        for epoch in epochs:\n",
    "            print(f\"Training with LR={LR} and n_layers={n_layer} and epochs={epoch}\")\n",
    "            model = Model(input_size=1, output_size=harmonies.shape[2], n_layers=n_layer)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "            train_model(model, melody, harmonies, optimizer, criterion, epoch)\n",
    "            with torch.no_grad():\n",
    "                output = model(melody)\n",
    "                loss = criterion(output, harmonies)\n",
    "                print(f\"Final Loss: {loss.item()}\")        \n",
    "            # Keep track of the best model (with lowest loss)\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_params = {'learning': LR, 'n_layers': n_layer, 'epochs': epoch}\n",
    "print(\"BEST: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny = pd.DataFrame([[67,62,59,43], [68,62,59,43]]).transpose()\n",
    "# melody = torch.tensor(tiny.iloc[0], dtype=torch.float32).unsqueeze(0).reshape(1,2,1)\n",
    "# harmonies = torch.transpose(torch.tensor(tiny.iloc[1:].values, dtype=torch.float32),0,1).unsqueeze(0)\n",
    "# model = Model(1, harmonies.shape[2])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# train_model(model, melody, harmonies, optimizer, criterion, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f9e5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71.34917  73.88738  74.9473   75.08467  75.20721  74.8395   74.52746\n",
      "  74.30749  74.209465 74.171165 74.14145  74.12845  74.111664 74.09347\n",
      "  74.073105 74.05653  74.06194  74.04737  74.04065  74.034515 74.041275\n",
      "  74.03543  74.03374  74.03118  74.051315 74.04441  74.04413  74.04183\n",
      "  74.041595 74.04014  74.039566 74.03845  74.01663  74.02082  74.019295\n",
      "  74.019356 74.00791  74.00979  74.00823  74.0079   73.9879   73.991714\n",
      "  73.9899   73.98987  74.01642  74.00899  74.00841  74.00624  74.026566\n",
      "  74.0203   74.02011  74.0181   74.0178   74.01648  74.01586  74.01478\n",
      "  74.0362   74.03015  74.02983  74.02799  74.02759  74.026306 74.02557\n",
      "  74.02446  74.00159  74.00571  74.004295 74.00416  73.98237  73.986534\n",
      "  73.98489  73.98483  73.973724 73.975334 73.97384  73.9733   73.97188\n",
      "  73.97095  73.96966  73.96856  73.9767   73.973175 73.97199  73.970276\n",
      "  73.96907  73.967545 73.98591  73.97941  73.97797  73.975365 73.973885\n",
      "  73.971695 73.96988  73.967766 73.965775 73.9636   73.93302  73.9387\n",
      "  73.936646 73.93619  73.916885 73.92039  73.91847  73.91792  73.90033\n",
      "  73.90355  73.90184  73.901375 73.89958  73.89845  73.89679  73.89535\n",
      "  73.908966 73.90272  73.90058  73.8975   73.89531  73.892525 73.890015\n",
      "  73.90351  73.89545  73.8919   73.88729  73.88361  73.879234 73.87514\n",
      "  73.870674 73.86627  73.8699   73.862045 73.85654  73.85034  73.836426\n",
      "  73.83341  73.827896 73.82268  73.8016   73.80217  73.797386 73.79377\n",
      "  73.8455   73.816124 73.80466  73.78958  73.73775  73.7443   73.738464\n",
      "  73.73501  73.73651  73.72867  73.72232  73.71633  73.72657  73.71343\n",
      "  73.706535 73.69877  73.69327  73.6873   73.68286  73.67848  73.659065\n",
      "  73.66368  73.662834 73.6638   73.65538  73.65976  73.66017  73.66234\n",
      "  73.64801  73.65653  73.65817  73.66237  73.68639  73.67937  73.679695\n",
      "  73.67843  73.696686 73.68918  73.690384 73.68846  73.690735 73.69055\n",
      "  73.693146 73.69411  73.7154   73.70906  73.711685 73.71058  73.714\n",
      "  73.71436  73.71782  73.71913 ]\n",
      " [67.8323   63.801918 62.277946 59.77161  58.866703 57.940475 57.078083\n",
      "  56.700325 56.614845 56.549973 56.447983 56.358704 56.271378 56.198174\n",
      "  56.136078 56.089134 56.025505 55.998512 55.98002  55.967457 55.944363\n",
      "  55.937286 55.932873 55.929764 55.897034 55.89348  55.89206  55.89116\n",
      "  55.891792 55.89187  55.892715 55.893257 55.925125 55.927917 55.929554\n",
      "  55.9309   55.9458   55.947655 55.948303 55.94925  55.97716  55.97962\n",
      "  55.980434 55.98139  55.938812 55.936317 55.935326 55.934727 55.90536\n",
      "  55.903404 55.904114 55.903946 55.90591  55.906586 55.908356 55.909447\n",
      "  55.878822 55.877827 55.879417 55.87993  55.88272  55.88403  55.886524\n",
      "  55.888237 55.922943 55.927322 55.92944  55.932587 55.963943 55.96871\n",
      "  55.969894 55.973015 55.987785 55.991127 55.99186  55.994125 55.994606\n",
      "  55.996284 55.997086 55.99849  55.984943 55.98492  55.986076 55.98666\n",
      "  55.988377 55.989372 55.96006  55.9582   55.9599   55.959583 55.962227\n",
      "  55.96289  55.965057 55.96622  55.96811  55.969505 56.018097 56.024826\n",
      "  56.026382 56.030846 56.059456 56.065697 56.066414 56.070408 56.09712\n",
      "  56.102905 56.1038   56.107555 56.107925 56.11055  56.111504 56.113556\n",
      "  56.087917 56.086258 56.08754  56.08732  56.08959  56.09021  56.092117\n",
      "  56.063103 56.06052  56.06143  56.06059  56.062603 56.062706 56.064274\n",
      "  56.06489  56.06622  56.05021  56.04875  56.049255 56.048904 56.067505\n",
      "  56.07054  56.072296 56.07454  56.109177 56.116486 56.11939  56.124283\n",
      "  55.99324  55.97508  55.97112  55.96382  56.07032  56.084076 56.0934\n",
      "  56.10196  56.087242 56.092476 56.09753  56.1046   56.069794 56.07231\n",
      "  56.078888 56.0868   56.097397 56.10784  56.119987 56.132366 56.18979\n",
      "  56.208046 56.224766 56.239838 56.27528  56.292145 56.307938 56.322884\n",
      "  56.376923 56.394875 56.411304 56.42553  56.379013 56.38803  56.398872\n",
      "  56.413254 56.382835 56.394344 56.406464 56.42188  56.4364   56.45163\n",
      "  56.46591  56.48059  56.446297 56.458023 56.46905  56.48425  56.49739\n",
      "  56.511803 56.524498 56.53799 ]\n",
      " [59.01913  53.4749   52.72446  49.421104 48.647438 46.38196  44.28161\n",
      "  42.48624  41.639496 41.041477 40.762173 40.58915  40.55168  40.5267\n",
      "  40.528675 40.518475 40.48555  40.490913 40.467747 40.463215 40.431168\n",
      "  40.432884 40.412006 40.408134 40.36254  40.37749  40.35142  40.355286\n",
      "  40.33994  40.338963 40.328625 40.325706 40.349266 40.32401  40.333893\n",
      "  40.319893 40.336975 40.318283 40.32546  40.315086 40.344334 40.31868\n",
      "  40.33223  40.318897 40.28237  40.30498  40.286243 40.296574 40.257843\n",
      "  40.285923 40.26703  40.281315 40.274048 40.28246  40.280655 40.28673\n",
      "  40.256554 40.28843  40.276432 40.295647 40.295116 40.309258 40.3143\n",
      "  40.32679  40.366245 40.350834 40.377174 40.376144 40.42301  40.40314\n",
      "  40.432983 40.429947 40.463303 40.45623  40.47923  40.482693 40.500214\n",
      "  40.50911  40.52493  40.53702  40.53962  40.566727 40.576836 40.59969\n",
      "  40.615284 40.637043 40.62931  40.68034  40.689697 40.729156 40.7504\n",
      "  40.78585  40.813484 40.848473 40.8807   40.917114 40.989403 40.981804\n",
      "  41.036266 41.050575 41.115704 41.11415  41.163406 41.180233 41.24079\n",
      "  41.24237  41.288795 41.306576 41.345383 41.37193  41.40883  41.440685\n",
      "  41.458855 41.520546 41.55236  41.60708  41.648865 41.701782 41.74951\n",
      "  41.78465  41.868416 41.919582 41.9969   42.0602   42.13664  42.207775\n",
      "  42.286453 42.364162 42.43857  42.54104  42.627815 42.730785 42.835346\n",
      "  42.92197  43.026962 43.12373  43.244812 43.312767 43.42417  43.509117\n",
      "  43.5727   43.822716 43.943024 44.16328  44.343487 44.42489  44.581173\n",
      "  44.68272  44.820854 44.95915  45.099693 45.240204 45.37135  45.558777\n",
      "  45.701653 45.87402  46.019062 46.177933 46.319424 46.46694  46.611897\n",
      "  46.701946 46.828938 46.923622 47.04483  47.12023  47.229073 47.310287\n",
      "  47.420685 47.464542 47.55947  47.61512  47.68774  47.811504 47.892597\n",
      "  48.000862 48.069553 48.208363 48.280743 48.39479  48.46427  48.559155\n",
      "  48.62431  48.705753 48.755833 48.875175 48.928947 49.025795 49.077095\n",
      "  49.156326 49.204304 49.271404]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv202147\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv202147_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv202147\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,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\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv202147_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv202147_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/foodrunner/CS370/PolyphAI/Code/output.xml')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melody = train[0].iloc[0]\n",
    "result = model(torch.tensor(melody, dtype=torch.float32).unsqueeze(0).reshape(1, train[0].shape[1], 1))\n",
    "result_numpy = result.detach().numpy()\n",
    "inversed = scaler_harmonies.inverse_transform(np.squeeze(result_numpy)).T\n",
    "print(inversed)\n",
    "\n",
    "score = stream.Score()\n",
    "melody_part = stream.Part()\n",
    "alto_part = stream.Part()\n",
    "tenor_part = stream.Part()\n",
    "bass_part = stream.Part()\n",
    "\n",
    "for pitch in melody:\n",
    "    melody_note = note.Note(int(pitch))\n",
    "    melody_part.append(melody_note)\n",
    "\n",
    "alto_notes = inversed[0]\n",
    "tenor_notes = inversed[1]\n",
    "bass_notes = inversed[2]  \n",
    "\n",
    "for pitch in alto_notes:\n",
    "    alto_note = note.Note(int(pitch.item()))\n",
    "    alto_part.append(alto_note)\n",
    "for pitch in tenor_notes:\n",
    "     tenor_note = note.Note(int(pitch.item()))\n",
    "     tenor_part.append(tenor_note)\n",
    "for pitch in bass_notes:\n",
    "    bass_note = note.Note(int(pitch.item()))\n",
    "    bass_part.append(bass_note)\n",
    "\n",
    "score.append(melody_part)\n",
    "score.append(alto_part)\n",
    "score.append(tenor_part)\n",
    "score.append(bass_part)\n",
    "score.show('midi')\n",
    "score.write('musicxml', 'output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75011af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune (hyperparameters, move around test data (refer to notes), etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new data + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make any other changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheet music + audio (musicAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new models if time permits (follow steps 3 - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front end ** if time permits\n",
    "# - Interactive sheet music\n",
    "# - musescore front end??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121de9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
